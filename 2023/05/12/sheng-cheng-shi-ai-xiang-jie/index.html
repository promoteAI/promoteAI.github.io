<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><title>生成式AI详解 | promoteAI</title><meta name="keywords" content="人工智能, AI"><meta name="author" content="陈汉江"><meta name="copyright" content="陈汉江"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#f7f9fe"><meta name="mobile-web-app-capable" content="yes"><meta name="referrer" content="no-referrer"><meta name="apple-touch-fullscreen" content="yes"><meta name="apple-mobile-web-app-title" content="生成式AI详解"><meta name="application-name" content="生成式AI详解"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="#f7f9fe"><meta property="og:type" content="article"><meta property="og:title" content="生成式AI详解"><meta property="og:url" content="http://promoteai.com/2023/05/12/sheng-cheng-shi-ai-xiang-jie/index.html"><meta property="og:site_name" content="promoteAI"><meta property="og:description" content="Chapter 5. Paint到目前为止，我们已经探索了各种训练模型生成新样本的方法，只需要给定我们想要模仿的训练数据集。我们将其应用于几个数据集，并看到在每种情况下，VAEs和GANs如何能够学习潜在空间和原始像素空间之间的映射。通过从潜空间的分布中采样，我们可以使用生成模型将该向量映射到像素空"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg"><meta property="article:author" content="陈汉江"><meta property="article:tag" content="人工智能, AI"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg"><meta name="description" content="Chapter 5. Paint到目前为止，我们已经探索了各种训练模型生成新样本的方法，只需要给定我们想要模仿的训练数据集。我们将其应用于几个数据集，并看到在每种情况下，VAEs和GANs如何能够学习潜在空间和原始像素空间之间的映射。通过从潜空间的分布中采样，我们可以使用生成模型将该向量映射到像素空"><link rel="shortcut icon" href="/favicon.ico"><link rel="canonical" href="http://promoteai.com/2023/05/12/sheng-cheng-shi-ai-xiang-jie/"><link rel="preconnect" href="//cdn.cbd.int"/><meta name="google-site-verification" content="xxx"/><meta name="baidu-site-verification" content="code-xxx"/><meta name="msvalidate.01" content="xxx"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  linkPageTop: undefined,
  peoplecanvas: {"enable":true,"img":"https://upload-bbs.miyoushe.com/upload/2024/07/27/125766904/ba62475f396df9de3316a08ed9e65d86_5680958632268053399..png"},
  postHeadAiDescription: {"enable":true,"gptName":"AnZhiYu","mode":"local","switchBtn":false,"btnLink":"https://afdian.net/item/886a79d4db6711eda42a52540025c377","randomNum":3,"basicWordCount":1000,"key":"xxxx","Referer":"https://xx.xx/"},
  diytitle: {"enable":true,"leaveTitle":"w(ﾟДﾟ)w 不要走！再看看嘛！","backTitle":"♪(^∇^*)欢迎肥来！"},
  LA51: undefined,
  greetingBox: undefined,
  twikooEnvId: '',
  commentBarrageConfig:undefined,
  music_page_default: "nav_music",
  root: '/',
  preloader: {"source":3},
  friends_vue_info: undefined,
  navMusic: true,
  mainTone: {"mode":"api","api":"https://img2color-go.vercel.app/api?img=","cover_change":true},
  authorStatus: {"skills":["🤖️ 数码科技爱好者","🔍 分享与热心帮助","🏠 智能家居小能手","🔨 设计开发一条龙","🤝 专修交互与设计","🏃 脚踏实地行动派","🧱 团队小组发动机","💢 壮汉人狠话不多"]},
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简","rightMenuMsgToTraditionalChinese":"转为繁体","rightMenuMsgToSimplifiedChinese":"转为简体"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":330},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    simplehomepage: true,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"copy":true,"copyrightEbable":false,"limitCount":50,"languages":{"author":"作者: 陈汉江","link":"链接: ","source":"来源: promoteAI","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","copySuccess":"复制成功，复制和转载请标注本文地址"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#425AEF","bgDark":"#1f1f1f","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.min.js',
      css: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: true,
  shortcutKey: undefined,
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  configTitle: 'promoteAI',
  title: '生成式AI详解',
  postAI: '',
  pageFillDescription: 'Chapter 5. Paint, CycleGAN, Data, Overview, The Generators (U-Net), CONCATENATE LAYER, INSTANCE NORMALIZATION LAYER, core code, The Discriminators, core code, Compiling the CycleGAN, Training the CycleGAN, Analysis of the CycleGAN, Creating a CycleGAN to Paint Like Monet, The Generators (ResNet), Analysis of the CycleGAN, Neural Style Transfer, Content Loss, Style Loss, Total Variance Loss, Running the Neural Style Transfer, Analysis of the Neural Style Transfer Model, Summary, Chapter 6. Write, Long Short-Term Memory Networks, Your First LSTM Network, Tokenization, Building the Dataset, The LSTM Architecture, The Embedding Layer, The LSTM Layer, The LSTM Cell, Generating New Text, RNN Extensions, Stacked Recurrent Networks, Gated Recurrent Units, Bidirectional Cells, Encoder–Decoder Models, A Question and Answer Generator, A Question-Answer Dataset, Model Architecture, GLOVE WORD VECTORS, Inference, Model Results, Summary, Chapter 7. Compose, Preliminaries, Musical Notation, Your First Music-Generating RNN, Attention, Building an Attention Mechanism in Keras, Analysis of the RNN with Attention, Attention in Encoder–Decoder Networks, Generating Polyphonic Music, The Musical Organ, Chapter 8. Play, Reinforcement Learning, OpenAI Gym, World Model Architecture, The Variational Autoencoder, The MDN-RNN, The Controller, Setup到目前为止我们已经探索了各种训练模型生成新样本的方法只需要给定我们想要模仿的训练数据集我们将其应用于几个数据集并看到在每种情况下和如何能够学习潜在空间和原始像素空间之间的映射通过从潜空间的分布中采样我们可以使用生成模型将该向量映射到像素空间中的新图像请注意到目前为止我们看到的所有示例都是从头开始生成新的观察结果的也就是说除了从潜在空间中采样的随机潜在向量用于生成图像外没有其他输入生成模型的另一个应用是在风格迁移领域本文的目标是建立一个模型可以转换输入的基图像以便给人一种它与给定的一组风格图像来自同一集合的印象这种技术有明显的商业应用现在被用于计算机图形软件计算机游戏设计和移动电话应用程序图展示了其中的一些例子在本章中你将学习如何构建两种不同类型的风格迁移模型和并将这些技术应用于你自己的照片和艺术品我们将从参观一家水果和蔬菜店开始那里的一切并不像看上去那样风格迁移常用的模型循环一致对抗网络或原始论文代表了风格迁移领域的重要一步因为它展示了如何训练一个模型在没有成对样本的训练集的情况下将风格从参考图像集复制到不同的图像上之前的风格迁移模型如要求训练集中的每个图像都存在于源域和目标域虽然对于某些风格的问题设置例如黑白到彩色照片映射到卫星图像可以制造这种数据集但对于其他问题这是不可能的例如我们没有莫奈画睡莲系列的池塘的原始照片也没有毕加索的帝国大厦画作将马和斑马站在相同位置的照片进行整理也需要花费巨大的精力论文在论文发布几个月后发布展示了如何训练一个模型来解决源域和目标域没有图像对的问题图分别展示了和的成对数据集和未成对数据集之间的差异虽然只能在一个方向上工作从源到目标但同时在两个方向上训练模型以便模型学习将图像从目标到源以及源到目标这是模型架构的结果因此您可以免费获得相反的方向现在让我们看看如何在中构建模型首先我们将使用前面的苹果和橘子的例子来遍历的每个部分并对该架构进行实验然后我们将应用相同的技术来建立一个模型可以将给定艺术家的风格应用到您选择的照片中使用数据集实际上由四个模型两个发生器和两个鉴别器组成第一个生成器将图像从域转换到域第二个生成器将图像从域转换到域由于我们没有成对的图像来训练我们的生成器我们还需要训练两个判别器来确定生成器生成的图像是否令人信服第一个鉴别器被训练成能够识别来自域的真实图像和由生成器产生的假图像之间的差异相反鉴别器被训练成能够识别来自域的真实图像和由生成器产生的假图像之间的差异四种型号的对应关系如图所示让我们首先看一下生成器的架构通常个生成器采用两种形式之一或残差网络在他们早期的论文中作者使用了架构但他们为切换到了架构本章将从开始构建这两种架构与变分自编码器类似由两部分组成下采样的一半输入图像在空间上进行压缩但在通道上进行扩展上采样的一半在空间上扩展表示同时减少通道数量然而与不同的是在网络的上采样和下采样部分中形状相同的层之间也存在跳跃连接是线性的数据通过网络从输入流到输出一层接一层是不同的因为它包含跳过连接允许信息通过网络的部分快捷方式传递到后面的层这里的直觉是随着网络的下采样部分的每一层的后续模型越来越多地捕获图像的内容并丢失关于哪里的信息在的顶点特征图将学习对图像中内容的上下文理解而对其位置的理解很少对于预测分类模型这就是我们所需要的因此我们可以将其连接到最终的密集层以输出图像中特定类存在的概率然而对于原始的应用程序图像分割和风格迁移至关重要的是当我们上采样回原始图像大小时我们将下采样期间丢失的空间信息传递回每一层这正是我们需要连接的原因它们允许网络将下采样过程中捕获的高级抽象信息即图像风格与从网络中的前一层反馈回来的特定空间信息即图像内容相融合为了构建跳跃连接我们需要引入一种新类型的层层只是沿着特定的轴默认是最后一个轴将一组层连接在一起例如在中我们可以将前面的层和层连接在一起如下所示在中我们使用级联层将上采样层连接到网络的下采样部分中同等大小的层这些层沿着通道维度连接在一起因此通道的数量从增加到而空间维度的数量保持不变注意级联层中不需要学习权重它们只是用来粘合前几层生成器还包含另一个新的层类型这个的生成器使用实例规范化层而不是批规范化层这在风格迁移问题中可以导致更令人满意的结果实例规范化层单独规范化每个观测值而不是作为一个批处理进行规范化与层不同它不需要在训练期间将和参数作为运行平均值进行计算因为在测试时该层可以像在训练时一样对每个实例进行归一化用于归一化每层的均值和标准偏差计算每个通道和每个观测此外对于该网络中的实例归一化层没有权重需要学习因为我们没有使用缩放或移位参数图展示了批量归一化和实例归一化以及其他两种归一化方法层归一化和组归一化之间的区别这里是批处理轴是通道轴表示空间轴因此立方体表示归一化层的输入张量蓝色像素使用相同的均值和方差根据这些像素的值计算进行归一化版本核心部分代码生成器网络下采样上采样到目前为止我们看到的鉴别器只输出一个数字输入图像为实的预测概率我们将构建的中的鉴别器输出单通道张量而不是单个数字这样做的原因是继承了模型的鉴别器架构在模型中鉴别器将图像划分为重叠的方形补丁并猜测每个补丁是真的还是假的而不是对整个图像进行预测因此鉴别器的输出是一个张量其中包含每个的预测概率而不仅仅是一个数字请注意当我们通过网络传递图像时这些补丁是同时预测的我们不会手动分割图像然后逐个通过网络传递每个补丁由于鉴别器的卷积结构将图像划分为小块是很自然的使用鉴别器的好处是损失函数可以衡量鉴别器根据风格而不是内容区分图像的能力由于鉴别器预测的每个单独元素仅基于图像的一个小正方形因此它必须使用补丁的样式而不是其内容来做出决定这正是我们所需要的我们宁愿我们的鉴别器擅长于识别两张图片在风格上的不同而不是内容上的不同鉴别器的代码判别器鉴别器是一系列卷积层所有层都具有实例规范化第一层除外最后一层是卷积层只有一个滤波器没有激活总结一下我们的目标是建立一组模型可以将域例如苹果的图像转换为域例如橘子的图像反之亦然因此我们需要编译四个不同的模型两个生成器和两个鉴别器如下所示学习将图像从域转换到域学习将图像从域转换到域学习来自域的真实图像和生成的假图像之间的差异学习来自域的真实图像和生成的假图像之间的差异我们可以直接编译这两个鉴别器因为我们有输入来自每个域的图像和输出二进制响应表示图像来自该域表示它是生成的假图像编译判别器但是我们不能直接编译生成器因为我们的数据集中没有成对的图像相反我们根据三个标准同时判断生成器有效性每个生成器产生的图像是否欺骗了相关的鉴别器例如输出是否来自输出是否来自重建如果我们依次应用两个生成器在两个方向上我们会返回原始图像吗得名于这个循环重构准则身份如果我们将每个生成器应用于其自己的目标域的图像图像是否保持不变下面展示了如何编译一个模型来满足这三个条件代码中的数字标记对应前面的列表编译生成器组合模型接受来自每个域的一批图像作为输入并为每个域提供三个输出以匹配三个标准因此总共有六个输出请注意我们如何冻结判别器中的权重这是典型的以便组合模型只训练生成器权重即使判别器涉及到模型中总损失是每个准则损失的加权和均方误差用于有效性标准根据真实或虚假响应检查鉴别器的输出平均绝对误差用于基于图像到图像的标准重建和一致性在我们的判别器和组合模型编译后我们现在可以训练我们的模型了这遵循了标准的实践即交替训练鉴别器和训练生成器通过组合模型我们对真实图像使用响应对生成图像使用响应注意每个有一个响应因为我们使用的是鉴别器为了训练鉴别器我们首先使用各自的生成器创建一批假图像然后我们在这个假集和一批真实图像上训练每个鉴别器通常对于批处理大小为单个图像通过前面编译的组合模型在一个步骤中一起训练生成器看看这六个输出如何与前面在编译期间定义的六个损失函数相匹配让我们看看在简单数据集上的表现并观察改变损失函数中的权重参数会如何对结果产生巨大影响现在您已经熟悉了架构您可能会意识到这个图像代表了判断组合模型的三个标准有效性重建和身份让我们用代码库中的适当函数重新标记这个图像以便更清楚地看到这一点如图所示我们可以看到网络的训练是成功的因为每个生成器都明显地改变了输入图像使其看起来更像来自相反域的有效图像此外当一个接一个地应用生成器时输入图像和重建图像之间的差异最小最后当每个生成器应用于自己输入域的图像时图像不会发生显著变化在最初的论文中除了必要的重构损失和有效性损失外身份损失是可选的为了证明身份项在损失函数中的重要性让我们通过在损失函数中设置身份损失权重参数为零来看看如果我们去掉身份项会发生什么图仍然成功地将橘子翻译成苹果但装橘子的托盘的颜色已经从黑色变成了白色因为现在没有身份损失术语来防止这种背景颜色的变化身份项有助于调节生成器以确保它只调整完成转换所需的图像部分而不是更多这突出了确保三个损失函数的权重很好地平衡的重要性身份损失过少会出现颜色偏移问题身份丢失太多没有足够的动力来改变输入使其看起来像来自相反域的图像现在我们已经探索了的基本结构我们可以将注意力转向更有趣和令人印象深刻的技术应用在最初的论文中一个突出的成就是模型能够学习如何将给定的照片转换成特定艺术家风格的绘画由于这是一个该模型也能够以另一种方式转换将艺术家的绘画转换为逼真的照片要下载从到照片的数据集在这个例子中我们将介绍一种新型的生成器架构残差网络简称架构类似于因为它允许来自网络中先前层的信息提前跳过一层或多层然而不是通过将网络的下采样部分连接到相应的上采样层来创建形而是由相互堆叠的残差块构建其中每个块包含一个跳跃连接在将其传递到下一层之前对块的输入和输出进行求和单个残差块如图所示在我们的中图中的权重层是卷积的具有实例规范化的层在中残差块的编码方法如例所示在残差块的两侧我们的生成器还包含下采样和上采样层的总体架构如图所示研究表明架构可以被训练到数百甚至数千层的深度并且不会受到梯度消失问题的影响其中早期层的梯度很小因此训练非常缓慢这是因为误差梯度可以通过作为残差块的一部分的跳跃连接在网络中自由反向传播此外人们相信添加额外的层绝不会导致模型精度的下降因为跳跃连接确保了如果不能提取进一步的信息特征则总是可以通过前一层的身份映射在原始的论文中该模型被训练了个以实现艺术家到照片风格迁移的最先进结果在图中我们显示了每个生成器在早期训练过程不同阶段的输出以显示模型开始学习如何将莫奈的绘画转换为照片和反之亦然时的进展在上面一行中我们可以看到莫奈所使用的独特的颜色和笔触逐渐转化为照片中所期望的更自然的颜色和平滑的边缘类似地下面一行的情况正好相反因为生成器学会了如何将一张照片转换成莫奈可能自己画的场景图是原始论文中该模型经过次训练后得到的部分结果到目前为止我们已经看到了如何在两个域之间转换图像其中训练集中的图像不一定是成对的现在我们来看看风格转移的另一种应用我们根本没有训练集而是希望将一张图像的风格转移到另一张图像上如图所示这被称为神经风格转移这个想法的前提是我们要最小化一个损失函数该函数是三个不同部分的加权和内容损失我们希望组合图像包含与基础图像相同的内容风格损失我们希望组合图像与风格图像具有相同的总体风格总方差损失我们希望组合后的图像看起来平滑而不是像素化我们通过梯度下降来最小化这种损失也就是说在多次迭代中我们按与损失函数的负梯度成比例的数量更新每个像素值这样损失随着每次迭代逐渐减少我们最终得到一个图像该图像将一个图像的内容与另一个图像的风格相融合通过梯度下降优化生成的输出与我们迄今为止解决生成模型问题的方式不同之前我们通过在整个网络中反向传播误差来训练一个深度神经网络如或以从训练数据集中学习并将学到的信息泛化以生成新图像在这里我们不能采用这种方法因为我们只有两个图像要处理基础图像和样式图像然而正如我们将看到的我们仍然可以使用预训练的深度神经网络来提供损失函数中每个图像的重要信息我们将从定义三个单独的损失函数开始因为它们是神经风格迁移引擎的核心内容损失衡量了两幅图像在主题和内容的整体位置方面的差异两幅包含相似场景的图像例如一排建筑物的照片和另一幅从不同光线不同角度拍摄的相同建筑物的照片的损失应该小于两幅包含完全不同场景的图像简单地比较两幅图像的像素值是不行的因为即使是在同一场景的两幅不同的图像中我们也不会期望单个像素值相似我们真的不希望内容损失关心单个像素的值我们更希望它根据高层特征如建筑物天空或河流的存在和大致位置对图像进行评分我们以前见过这个概念这是深度学习的整个前提训练用于识别图像内容的神经网络通过结合前一层的简单特征自然地在网络的更深层学习更高级别的特征因此我们需要的是一个已经成功训练过识别图像内容的深度神经网络这样我们就可以利用网络的深层来提取给定输入图像的高层特征如果我们测量基础图像的输出和当前组合图像之间的均方误差我们就有了内容损失函数我们将使用的预训练网络称为这是一个层的卷积神经网络经过训练可以将数据集中的多万张图像分类为个对象类别组网图如图所示注意库包含可以导入的预训练的模型我们定义两个变量来保存基础图像和样式图像以及一个占位符它将包含生成的组合图像模型的输入张量是三个图像的连接这里我们创建了模型的一个实例指定了输入张量和我们想要预加载的权重参数指定我们不需要为最终用于图像分类的网络密集层加载权重这是因为我们只对前面的卷积层感兴趣这些层捕获输入图像的高级特征而不是原始模型被训练为输出的实际概率我们用来计算内容损失的层是第五个块的第二层卷积层选择网络中较浅或较深点的层会影响损失函数如何定义内容因此会改变生成的组合图像的属性在这里我们从输入张量中提取基础图像特征和组合图像特征这些特征已经通过网络输入内容损失是两个图像的选定层的输出之间的平方和乘以一个加权参数风格损失更难量化我们如何衡量两个图像之间的风格相似性神经风格迁移论文中给出的解决方案是基于这样的想法在给定层中风格相似的图像通常具有相同的特征图之间的相关性模式通过一个例子我们可以更清楚地看到这一点假设在网络中我们有一些层其中一个通道已经学会识别图像的绿色部分另一个通道已经学会识别尖峰另一个已经学会识别图像的棕色部分来自这些通道的三个输入的输出特征图如图所示我们可以看到和在样式上很相似都是绿草我们可以把特征图展平然后计算点积如果结果值很高则特征图高度相关如果该值较低则特征图不相关我们可以定义一个矩阵其中包含层中所有可能特征对之间的点积这被称为矩阵图展示了每个图像的三个特征的矩阵很明显样式相似的图像和在这一层具有相似的矩阵即使它们的内容可能非常不同但矩阵度量层中所有特征对之间的相关性是相似的因此为了计算风格损失我们需要做的就是为整个网络中的一组层计算基图像和组合图像的矩阵并使用误差平方和比较它们的相似性从代数上讲对于大小为高度宽度有个通道的给定层基础图像和生成图像之间的风格损失可以写成注意它是如何根据通道数量和层大小进行缩放的这是因为我们将整体风格损失计算为几个层的加权和所有这些层都有不同的大小总风格损失计算如下样式损失是在五个层上计算的模型的五个块中的每一个的第一个卷积层在这里我们从通过网络提供的输入张量中提取风格图像特征和组合图像特征样式损失由加权参数和计算的层数来缩放总方差损失只是组合图像中噪声的度量为了判断图像的噪声程度我们可以将其向右移动一个像素然后计算平移后图像与原始图像之间的平方和为了平衡我们也可以执行相同的过程但将图像向下移动一个像素这两项的和就是总方差损失总损失图像和同一图像之间的平方差将向下移动一个像素图像和同一图像之间的平方差向右移动了一个像素总方差损失由一个加权参数缩放总体损失是内容风格和总方差损失的总和该过程以基图像作为起始合并图像进行初始化在每次迭代时我们将当前合并的图像变平传递给中的优化函数优化包根据算法执行一个梯度下降步骤在这里是一个对象包含计算前面描述的总体损失以及相对于输入图像的损失梯度的方法图显示了学习过程中三个不同阶段的神经风格迁移过程输出参数如下我们可以看到随着每个训练步骤的进行该算法在风格上越来越接近风格图像并失去了基础图像的细节同时保留了整体内容结构有许多方法可以试验这种架构你可以尝试改变损失函数或用于确定内容相似度的层中的权重参数看看这是如何影响组合输出图像和训练速度的您还可以尝试衰减风格损失函数中赋予每个层的权重以使模型偏向于迁移更精细或更粗糙的风格特征在本章中我们探索了两种不同的生成新艺术品的方法和神经风格迁移方法允许我们训练一个模型来学习艺术家的一般风格并将其转移到照片上以生成看起来就像艺术家画了照片中的场景一样的输出该模型还免费为我们提供了反向过程将绘画转换为逼真的照片至关重要的是来自每个域的配对图像不需要工作使其成为一种极其强大和灵活的技术神经风格迁移技术允许我们将单个图像的风格迁移到基图像上使用巧妙选择的损失函数惩罚模型偏离基础图像的内容和风格图像的艺术风格太远同时保持输出的平滑程度这项技术已经被许多知名的应用程序商业化可以将用户的照片与一组特定风格的绘画融合在一起在下一章中我们将从基于图像的生成建模转移到一个新的挑战领域基于文本的生成建模在本章中我们将探讨在文本数据上构建生成模型的方法文本数据和图像数据之间有几个关键的区别这意味着许多适用于图像数据的方法并不那么容易适用于文本数据特别是文本数据由离散的块字符或单词组成而图像中的像素是连续光谱中的点我们可以很容易地让绿色像素更蓝色但如何让单词更像单词就不是很明显了这意味着我们可以轻松地将反向传播应用于图像数据因为我们可以计算损失函数相对于单个像素的梯度以确定像素颜色应改变的方向以最小化损失对于离散的文本数据我们不能像通常那样应用反向传播因此我们需要找到一种解决这个问题的方法文本数据有时间维度但没有空间维度而图像数据有两个空间维度但没有时间维度单词的顺序在文本数据中非常重要单词倒过来是没有意义的而图像通常可以翻转而不影响内容此外模型需要捕获单词之间的长期顺序依赖关系例如问题的答案或延续代词的上下文对于图像数据可以同时处理所有像素文本数据对单个单位单词或字符的微小变化非常敏感图像数据通常对单个像素单位的变化不太敏感即使改变了一些像素一幅房子的图像仍然可以被识别为一所房子然而对于文本数据即使改变几个单词也可能彻底改变文章的含义或使其变得毫无意义这使得训练一个模型来生成连贯的文本变得非常困难因为每个单词对文章的整体含义都至关重要文本数据具有基于规则的语法结构而图像数据不遵循关于如何分配像素值的规则例如在任何内容中写都没有语法意义还有一些语义规则很难建模说是没有意义的尽管从语法上讲这句话没有任何错误文本建模已经取得了良好的进展但上述问题的解决方案仍然是正在进行的研究领域我们将从最常用和已建立的用于生成序列数据如文本的模型之一开始递归神经网络特别是长短期记忆层本章还将探索一些在问题答案对生成领域取得有希望成果的新技术网络是一种特殊类型的循环神经网络包含一个循环层或单元格它能够通过使自己在特定时间步的输出成为下一个时间步输入的一部分来处理顺序数据以便过去的信息可以影响当前时间步的预测我们说网络是指具有递归层的神经网络当首次引入时递归层非常简单仅由算子组成该算子确保时间步之间传递的信息在和之间缩放然而这被证明会受到梯度消失问题的影响并且不能很好地扩展到长序列数据年和在一篇论文中首次介绍了单元在这篇论文中作者描述了如何没有普通所经历的梯度消失问题并且可以在数百个时间步长的序列上进行训练从那时起架构已经被调整和改进像门控循环单元这样的变体现在被广泛使用并作为中的层让我们首先看看如何使用构建一个非常简单的网络它可以生成伊索寓言风格的文本数据集第一步是对文本进行清理和标记化分词是将文本分割为单个单元如单词或字符的过程如何标记文本将取决于您试图使用文本生成模型实现的目标同时使用单词标记和字符标记各有利弊您的选择将影响您在建模和模型输出之前需要如何清理文本如果使用单词标记所有文本都可以转换为小写以确保句子开头大写的单词与出现在句子中间的相同单词的标记化方式相同但在某些情况下这可能不是我们想要的例如一些专有名词如名称或地点可以保持大写以便独立标记它们文本词汇表训练集中不同的单词的集合可能非常大其中一些单词出现得非常稀疏或者可能只出现一次将稀疏单词替换为未知单词的标记可能是明智的而不是将它们作为单独的标记包括在内以减少神经网络需要学习的权重数量单词可以被词根化这意味着它们被简化为最简单的形式这样一个动词的不同时态保持在一起例如浏览浏览浏览和浏览都可以用眉毛来表示您需要将标点符号标记化或者完全删除它使用单词标记化意味着模型将永远无法预测训练词汇表之外的单词如果你使用字符标记模型可能会在训练词汇表之外生成形成新单词的字符序列这在某些上下文中可能是可取的但在另一些上下文中则不是大写字母可以转换为对应的小写字母也可以保留为单独的标记使用字符分词时词汇表通常要小得多这有利于模型训练速度因为最终输出层需要学习的权重更少在本例中我们将使用小写单词分词而不使用单词词干提取我们还将标记标点符号例如我们希望模型能够预测何时应该结束句子或开始结束演讲标记最后我们将用一个新的故事角色块替换故事之间的多个换行符这样当我们使用模型生成文本时我们可以用这个字符块填充模型这样模型就知道要从头开始一个新的故事我们的网络将被训练为预测序列中的下一个单词给定此点之前的单词序列例如我们可以给模型输入贪心的猫和和的标记并期望模型输出一个合适的下一个单词例如而不是整个模型的架构如图所示模型的输入是一个整数标记序列输出是词汇表中每个单词在下一个序列中出现的概率为了详细理解它是如何工作的我们需要引入两种新的层类型嵌入和嵌入层本质上是一个查找表它将每个标记转换为长度为的向量图因此这一层学习到的权重数量等于词汇表的大小乘以输入层将形状为的整数序列张量传递到嵌入层嵌入层输出形状为的张量然后将其传递给层图我们将每个整数标记嵌入到一个连续向量中因为它使模型能够学习每个单词的表示并能够通过反向传播进行更新我们也可以对每个输入标记进行独热编码但首选使用嵌入层因为它使嵌入本身可训练从而使模型在决定如何嵌入每个标记以提高模型性能方面具有更大的灵活性要理解层我们必须首先了解一般的循环层是如何工作的循环层具有能够处理顺序输入数据的特殊属性它由一个单元格组成当序列的每个元素通过它时它更新其隐藏状态每次一个时间步隐藏状态是一个长度等于单元格中单元数量的向量它可以被认为是单元格当前对序列的理解在时间步单元格使用隐藏状态的前一个值和当前时间步的数据来生成更新的隐藏状态向量这个循环过程一直持续到序列的末尾一旦序列完成该层输出单元的最终隐藏状态然后传递到网络的下一层这个过程如图所示为了更详细地解释这一点让我们展开这个过程以便我们可以确切地看到单个序列是如何穿过该层的图在这里我们通过在每个时间步长绘制单元的副本来表示循环过程并显示隐藏状态在流过单元时如何不断更新我们可以清楚地看到前一个隐藏状态是如何与当前序列数据点即当前嵌入的词向量混合以产生下一个隐藏状态的在输入序列中的每个单词被处理后该层的输出是单元的最终隐藏状态重要的是要记住此图中的所有单元格共享相同的权重因为它们实际上是同一个单元格这个图和图没有什么区别这只是绘制循环层机制的另一种方式现在我们已经看到了通用循环层的工作原理让我们看看单个单元的内部单元的工作是输出一个新的隐藏状态给定它之前的隐藏状态和当前的词嵌入总结一下的长度等于中的单元数这是一个在定义层时设置的参数与序列的长度无关确保你没有混淆术语和层中有一个单元由它包含的单元数量定义就像我们之前故事中的囚犯单元包含许多囚犯一样我们经常将循环层绘制为展开的单元链因为它有助于可视化隐藏状态在每个时间步是如何更新的一个细胞维护一个细胞状态这可以被认为是细胞关于序列当前状态的内部信念这与隐藏状态不同在最后一个时间步长之后最终由单元输出单元状态的长度与隐藏状态相同单元中的单元数让我们更仔细地看一下单个单元格以及隐藏状态是如何更新的如图所示现在我们已经编译和训练了网络我们可以通过应用以下过程开始使用它来生成长字符串文本向网络提供现有的单词序列并要求它预测接下来的单词将这个单词添加到现有序列中并重复网络将输出我们可以从中采样的每个单词的概率集合因此我们可以使文本的生成是随机的而不是确定性的此外我们可以在采样过程中引入一个温度参数以表明我们希望该过程具有多大的确定性关于这两篇文章有几点需要注意首先两者在风格上类似于原始训练集中的寓言它们都以熟悉的故事中人物的陈述开始通常带有言语标记的文本更像对话使用人称代词由所说的单词的出现而准备其次与温度时生成的文本相比在温度时生成的文本不那么冒险但在选择单词方面更连贯因为较低的温度值导致更确定性的采样最后很明显这两种方法都不能很好地跨越多个句子因为网络无法掌握它生成的单词的语义为了生成语义合理的可能性更大的段落我们可以构建一个人工辅助的文本生成器其中模型输出概率最高的个单词然后最终由人类从这个列表中选择下一个单词这类似于手机上的预测文本你可以从已经输入的单词中选择几个单词为了证明这一点图显示了具有最高概率的前个单词遵循各种序列不是来自训练集该模型能够在一系列上下文中为下一个最有可能的单词生成合适的分布例如即使模型从未被告知词性如名词动词形容词和介词但它通常能够将单词分成这些类别并以语法正确的方式使用它们它还可以猜测以鹰为故事开头的文章更可能是而不是图中的标点符号示例表明模型对输入序列的细微变化也很敏感在第一段中狮子说模型以的可能性猜测语音标记紧随其后因此从句在口语对话之前然而如果我们输入下一个单词作为它能够理解现在不太可能有词性标记因为从句更有可能取代对话句子更有可能继续作为描述性散文上一节中的网络是一个简单的例子展示了如何训练网络以学习如何生成给定样式的文本在本节中我们将探讨对这一思想的几种扩展我们刚刚看到的网络包含单个层但我们也可以训练具有堆叠层的网络这样就可以从文本中学习到更深的特征为了实现这一点我们将第一个层中的参数设置为这使得层从每个时间步输出隐藏状态而不仅仅是最后一个时间步然后第二个层可以使用第一层的隐藏状态作为其输入数据如图所示整体模型架构如图所示另一种常用的层是门控循环单元与单元的关键区别如下遗忘门和输入门被重置门和更新门取代没有单元状态或输出门只有从单元输出的隐藏状态隐藏状态分步更新如图所示前一个时间步的隐藏状态和当前的词嵌入状态被连接起来用于创建重置门该门是一个密集层具有权重矩阵和激活函数结果向量的长度等于单元格中单元的数量并存储在到之间的值这些值决定了前一个隐藏状态的多少信息应该被带入计算单元格的新信念重置门应用于隐藏状态并与当前的词嵌入连接然后该向量被馈送到具有权重矩阵和激活函数的密集层以生成一个向量该向量存储细胞的新信念它的长度等于单元格中的单元数并存储在到之间的值前一个时间步的隐藏状态和当前词嵌入的连接也用于创建更新门该门是一个密集层具有权重矩阵和激活所得向量的长度等于单元格中单元的数目存储在和之间的值用于确定有多少新的深信融合到当前的隐藏状态中细胞的新信念与当前隐藏状态按更新门确定的比例混合以产生更新后的隐藏状态该状态由细胞输出对于在推理时模型可以获得整个文本的预测问题没有理由只向前处理序列它可以向后处理双向层通过存储两组隐藏状态来利用这一点一组是在通常的正向处理序列时产生的另一组是在向后处理序列时产生的这样该层可以从之前的和给定时间步后的信息中学习在中这是作为循环层的包装实现的如下所示结果层中的隐藏状态是长度等于被包装单元中单元数量的两倍的向量连接前向和后向隐藏状态因此在这个例子中层的隐藏状态是长度为的向量到目前为止我们已经研究了使用网络来生成现有文本序列的延续我们已经看到单个层如何顺序处理数据以更新表示该层当前对序列的理解的隐藏状态通过将最终的隐藏状态连接到密集层网络可以输出下一个单词的概率分布对于某些任务目标不是预测现有序列中的单个下一个单词相反我们希望预测一个完全不同的单词序列该序列在某种程度上与输入序列相关这种类型的任务的一些例子有语言翻译网络接收源语言的文本字符串目标是输出翻译成目标语言的文本问题生成网络接收一段文本目标是生成一个可以针对文本提出的可行问题文本摘要网络接收一段很长的文本目标是对该文本进行简短的摘要对于这类问题我们可以使用一种称为编码器解码器的网络我们已经在图像生成的背景下看到了一种编码器解码器网络变分自动编码器对于顺序数据编码器解码器过程如下所示编码器将原始输入序列汇总为单个向量这个向量用于初始化解码器解码器在每个时间步的隐藏状态连接到一个密集层该层输出单词词汇表的概率分布这样解码器可以生成一个新的文本序列它已经用编码器产生的输入数据的表示进行了初始化这个过程如图所示以英语和德语之间的翻译为例编码器的最终隐藏状态可以被认为是整个输入文档的表示然后解码器将这种表示转换为顺序输出例如将文本翻译成另一种语言或与文档有关的问题在训练过程中将解码器在每个时间步产生的输出分布与真实的下一个单词进行比较以计算损失在训练过程中解码器不需要从这些分布中采样来生成单词因为后续的单元被提供的是下一个单词的真实值而不是从前一个输出分布中采样的单词这种训练编码器解码器网络的方式被称为教师强迫我们可以想象网络是一个学生有时会做出错误的分布预测但无论网络在每个时间步长输出什么老师都提供正确的响应作为网络尝试下一个单词的输入现在我们将把所有内容放在一起构建一个可以从文本块中生成问题和答案对的模型这个项目的灵感来自的代码库和和提出的模型该模型由两部分组成从文本块中识别候选答案编码器解码器网络在突出显示的候选答案之一的情况下生成合适的问题例如考虑以下关于足球比赛的文本段落的开头我们希望我们的第一个网络能够识别潜在的答案例如我们的第二个网络应该能够在给定每个答案的情况下生成一个问题例如让我们首先看一下我们将更详细地使用的数据集我们将使用数据集你可以按照上的说明下载它得到的和文件应该放在图书仓库的文件夹中这些文件都具有相同的列结构如下所示故事的唯一标识符例如制胜球是由岁的前锋乔布洛格斯在比赛中打进的例如前锋花了多少钱表示答案在故事文本中的标记位置例如如果答案在文章中出现多次可能会有多个区间用逗号分隔这些原始数据经过处理并标记化以便能够将其用作我们模型的输入经过这种转换后训练集中的每个观察值由以下五个特征组成分词后的故事文本例如用零剪裁填充长度为一个参数分词后的问题例如填充长度为另一个参数分词后的问题偏移一个时间步长例如用零填充长度为二进制掩码矩阵形状为如果问题的答案的第个单词位于文档的第个单词则矩阵的值为否则为长度为的二进制向量例如如果文档中的第个单词被认为是答案的一部分则向量的第个元素为否则为现在让我们看一下能够从给定的文本块中生成问题答案对的模型架构图展示了我们将要构建的整体模型架构如果这看起来很吓人不要担心它只是由我们已经见过的元素构建而成我们将在本节中一步一步地介绍这个架构让我们首先看一下构建图顶部模型部分的代码它预测文档中的每个单词是否是答案的一部分注意文档令牌作为模型的输入这里我们使用变量来描述输入的大小但该变量实际上被设置为这是因为模型的架构不依赖于输入序列的长度层中的单元格数量将自适应等于输入序列的长度因此我们不需要显式指定它嵌入层用词向量初始化在下面的侧边栏中解释循环层是一个双向的它在每个时间步返回隐藏状态输出密集层在每个时间步与隐藏状态连接只有两个单元具有激活表示每个单词是答案一部分或不是答案一部分的概率嵌入层是用一组预训练的词嵌入初始化的而不是我们之前看到的随机向量这些词向量是全局向量项目的一部分该项目使用无监督学习来获取大量单词的代表向量这些向量具有许多有益的特性如连接词之间的向量相似度例如单词和的嵌入向量与单词和之间的向量大致相同这就好像单词的性别被编码到单词向量存在的潜在空间中用初始化嵌入层通常比从头开始训练要好因为捕获单词表示的大量艰苦工作已经通过训练过程实现了然后您的算法可以调整单词嵌入以适应您的机器学习问题的特定上下文为了在这个项目中使用词向量请从项目网站下载文件亿个单词每个单词的嵌入长度为然后从图书库中运行以下脚本修剪该文件使其只包含训练语料库中存在的单词模型的第二部分是编码器解码器网络它接受给定的答案并尝试制定匹配的问题图的底部部分注意答案掩码作为输入传递给模型这允许我们将隐藏状态从单个答案范围传递到编码器解码器这是通过层实现的编码器是一个层它将给定答案范围的隐藏状态作为输入数据解码器的输入数据是与给定答案范围匹配的问题问题词标记通过与答案识别模型相同的嵌入层传递解码器是一个层使用编码器的最终隐藏状态进行初始化解码器的隐藏状态通过一个密集层传递以生成序列中下一个单词的整个词汇表的分布这就完成了我们的问题答案对生成网络为了训练网络我们分批传递文档文本问题文本和答案掩码作为输入数据并最小化答案位置预测和问题词生成的交叉熵损失平均加权为了在一个它从未见过的输入文档序列上测试模型我们需要运行以下过程将文档字符串提供给答案生成器以生成文档中答案的示例位置选择这些答案块中的一个将其传递到编码器解码器问题生成器即创建适当的答案掩码将文档和答案掩码提供给编码器以生成解码器的初始状态用这个初始状态初始化解码器并将来生成问题的第一个单词继续这个过程一个一个输入生成的单词直到由模型预测如前所述在训练过程中模型使用教师强迫将基本真实单词而不是预测的下一个单词输入到解码器单元然而在推理过程中模型必须自己生成一个问题因此我们希望能够将预测的单词反馈给解码器单元同时保留其隐藏状态实现这一目标的一种方法是定义一个额外的模型该模型接受当前单词标记和当前解码器隐藏状态作为输入并输出预测的下一个单词分布和更新的解码器隐藏状态然后我们可以在循环中使用该模型逐字生成输出问题模型的示例结果如图所示根据模型右边的图表显示了文档中每个单词构成答案一部分的概率然后将这些答案短语提供给问题生成器该模型的输出显示在图的左侧预测问题首先请注意答案生成器如何能够准确识别文档中哪些单词最有可能包含在答案中这已经相当令人印象深刻了因为它以前从未见过此文本也可能从未见过文档中包含在答案中的某些单词例如它能够从上下文中理解这很可能是一个人的姓氏因此很可能构成答案的一部分编码器从每个可能的答案中提取上下文以便解码器能够生成合适的问题值得注意的是编码器能够捕捉到第一个答案中提到的人岁的前锋乔布洛格斯可能有一个与他的进球能力相关的匹配问题并能够将此上下文传递给解码器从而生成问题谁得分了而不是例如谁是总统解码器已经用标签完成了这个问题但不是因为它不知道接下来要做什么它预测接下来的单词很可能来自核心词汇表之外我们不应该惊讶于模型诉诸于使用标签在这种情况下由于原始语料库中的许多小众词将被标记为这种方式我们可以看到在每种情况下解码器都根据答案的类型选择了正确的问题类型谁多少钱什么时候问但是仍然有一些问题例如问他损失了多少钱而不是为这名前锋支付了多少钱这是可以理解的因为解码器只有最终的编码器状态而不能引用原始文档以获取额外的信息有一些对编码器解码器网络的扩展可以提高模型的准确性和生成能力其中使用最广泛的两种是和使模型能够指向输入文本中的特定单词以包括在生成的问题中而不仅仅依赖于已知词汇表中的单词这有助于解决前面提到的问题我们将在下一章中详细探讨注意力机制在本章中我们看到了如何应用循环神经网络来生成模仿特定写作风格的文本序列以及如何从给定的文档中生成合理的问题答案对我们探索了两种不同类型的循环层长短期记忆和并了解了这些单元如何堆叠或双向形成更复杂的网络架构本章介绍的编码器解码器架构是一个重要的生成工具因为它允许序列数据被压缩为单个向量然后可以解码为另一个序列这适用于除问答对生成外的一系列问题如翻译和文本摘要在这两种情况下我们都看到了如何将非结构化的文本数据转换为可与循环神经网络层一起使用的结构化格式的重要性很好地理解张量的形状在数据流经网络时如何变化也是构建成功网络的关键而递归层中需要特别注意顺序数据的时间维度因为转换过程增加了额外的复杂性在下一章中我们将看到有多少关于的相同思想可以应用于另一种类型的序列数据音乐除了视觉艺术和创意写作音乐创作是我们认为人类独有的另一种核心创意行为要让机器创作出令我们愉悦的音乐它必须克服前一章中涉及文本时遇到的许多相同的技术挑战特别是我们的模型必须能够学习并重新创建音乐的顺序结构还必须能够从后续音符的离散可能性中进行选择然而音乐生成提出了文本生成所不需要的额外挑战即音高和节奏音乐通常是复音的也就是说有几个音符流同时在不同的乐器上演奏它们结合起来创造出不和谐冲突或和谐和谐的和谐文本生成只需要我们处理单个文本流而不是音乐中存在的并行和弦流此外文本生成可以一次处理一个单词我们必须仔细考虑这是否是处理音乐数据的合适方法因为听音乐的大部分兴趣是在整个乐团不同节奏之间的相互作用例如吉他手可能会演奏一连串更快的音符而钢琴家则演奏较长的持续和弦因此按音符生成音乐是复杂的因为我们通常不希望所有的乐器同时变换音符本章将从简化问题开始专注于单声道音乐的音乐生成我们将看到前一章中关于文本生成的许多技术也可以用于音乐生成因为这两个任务有许多共同的主题本章还将介绍注意力机制它允许我们构建能够选择关注之前的音符从而预测接下来会出现哪些音符最后我们将解决复调音乐生成的任务并探索如何部署基于的架构来为多个声音创建音乐任何处理音乐生成任务的人都必须首先对音乐理论有一个基本的了解在本节中我们将介绍阅读音乐所需的基本符号以及如何将其表示为数字以便将音乐转换为训练生成模型所需的输入数据我们将学习本书存储库中的另一个入门使用生成音乐的优秀资源是的博客文章和附带的存储库我们将使用的原始数据集是一组巴赫的大提琴组曲的文件你可以使用任何你想使用的数据集但如果你想使用这个数据集你可以在中找到下载文件的说明要查看和收听模型生成的音乐您需要一些可以生成乐谱的软件是一个很好的工具可以免费下载我们将使用库将文件加载到中进行处理我们使用方法将所有同时演奏的音符压缩为一个声部中的和弦而不是将它们拆分为多个声部由于这首曲子是由一种乐器大提琴演奏的所以我们这样做是合理的尽管有时我们可能希望将这些部分分开来产生本质上是复调的音乐这带来了更多的挑战我们将在本章后面讨论代码逻辑循环遍历乐谱并将乐曲中每个音符和休止符的音高和时值提取到两个列表中和弦中的单个音符被一个点分隔开这样整个和弦就可以被存储为一个单独的弦每个音符名称后面的数字表示音符所在的八度因为音符名称到重复所以需要这个八度来唯一标识音符的音高例如是低于的一个倍频程这个过程的输出如表所示结果数据集现在看起来更像我们之前处理过的文本数据单词就是音高我们应该尝试建立一个模型在给定之前的音高序列的情况下预测下一个音高同样的想法也可以应用于持续时间列表使我们能够灵活地构建一个可以同时处理音高和持续时间预测的模型为了创建用于训练模型的数据集我们首先需要给每个基音和持续时间一个整数值图就像我们之前对文本语料库中的每个单词所做的那样这些值是什么并不重要因为我们将使用嵌入层将整数查找值转换为向量然后我们通过将数据分割为个音符的小块并具有序列中下一个音符的响应变量独热编码用于音高和持续时间来创建训练集我们将要构建的模型是一个带有注意力机制的堆叠网络在上一章中我们看到了如何通过将前一层的隐藏状态作为输入传递到下一层层来堆叠层以这种方式堆叠层可以让模型自由地从数据中学习更复杂的特征在本节中我们将介绍注意力机制它现在是最复杂的序列生成模型的组成部分它最终产生了一种完全基于注意力的模型甚至不需要递归或卷积层第章会详细介绍的架构现在让我们专注于将注意力合并到堆叠的网络中以尝试在给定之前的音符序列的情况下预测下一个音符注意力机制最初应用于文本翻译问题特别是将英语句子翻译成法语在上一章中我们看到了编码器解码器网络如何解决这种问题首先将输入序列通过编码器来生成上下文向量然后将这个向量通过解码器网络来输出翻译后的文本这种方法的一个问题是上下文向量可能成为瓶颈来自源句子开头的信息在到达上下文向量时可能会被稀释特别是对于长句子因此这种类型的编码器解码器网络有时很难保留所有所需的信息以使解码器准确地翻译源例如假设我们希望模型将以下句子翻译成德语显然如果将单词替换为整个句子的意思就会发生变化然而编码器的最终隐藏状态可能无法充分保留这些信息因为得分的单词出现在句子的早期这个句子的正确翻译是如果我们看一下正确的德语翻译我们可以看到得分这个词实际上出现在句子的末尾因此这个模型不仅要保留这样一个事实即在编码器中判罚得分而不是失分而且还要一直保留在解码器中同样的原理也适用于音乐要了解某一特定的音乐段落可能会出现哪些音符或音符序列使用序列中较早的信息可能是至关重要的而不仅仅是最新的信息以巴赫第一大提琴组曲的前奏曲为例图你认为下一个音符是什么即使你没有受过音乐训练你也可以猜出来如果你说的是与曲子的第一个音符相同那么你是正确的你怎么知道的你可能已经看到每一小节和半小节都以相同的音符开始并利用这些信息来指导你的决定我们希望我们的模型能够执行同样的技巧特别是我们希望它不仅关心网络现在的隐藏状态而且还要特别注意网络在个音符前的隐藏状态当前一个低被记录时为了解决这一问题提出了注意机制而不是只使用编码器的最终隐藏状态作为上下文向量注意机制允许模型创建上下文向量作为编码器在每个前一个时间步的隐藏状态的加权和注意机制是将之前的编码器隐藏状态和当前的解码器隐藏状态转换为生成上下文向量的加权总和的一组层如果这听起来令人困惑不要担心首先我们将看到如何在一个简单的循环层之后应用注意力机制即解决预测巴赫大提琴组曲第一的下一个音符的问题然后我们将看到它如何扩展到编码器解码器网络我们想要预测后续音符的整个序列而不仅仅是一个首先让我们提醒自己如何使用标准循环层来预测给定前一个音符序列的下一个音符图显示了输入序列如何一步一步地馈送到层不断更新层的隐藏状态输入序列可以是注释嵌入也可以是来自前一个循环层的隐藏状态序列循环层的输出是最终隐藏状态一个与单元数相同长度的向量然后可以将其馈送到具有输出的层以预测序列中下一个音符的分布图显示了相同的网络但这一次将注意力机制应用于循环层的隐藏状态音符持续时间拼接我们将从从头开始生成一些音乐开始通过仅用令牌即我们告诉模型假设它是从片段的开头开始的然后我们可以使用我们在第章中用于生成文本序列的相同迭代技术来生成一个音乐段落如下所示给定当前序列音符名称和音符持续时间该模型预测下一个音符名称和持续时间的两个分布我们从这两个分布中采样使用温度参数来控制采样过程中的变化程度所选音符被存储其名称和持续时间被附加到相应的序列中如果序列的长度现在大于模型被训练的序列长度我们从序列的开始处删除一个元素这个过程用新的序列重复依此类推我们希望生成多少音符就生成多少音符图显示了该模型在训练过程的各个阶段从头生成的音乐示例我们在本节中的大部分分析将集中在音调预测上而不是节奏上因为巴赫的大提琴组曲的和声复杂性更难捕捉因此更值得研究但是您也可以将相同的分析应用于模型的节奏预测这可能与您可以用于训练该模型的其他风格的音乐例如鼓音轨特别相关关于图中生成的通道有几点需要注意首先看看随着训练的进行音乐是如何变得越来越复杂的首先该模型通过坚持使用同一组音符和节奏来确保安全到第阶段这个模型已经开始产生小的音符到第阶段它开始产生有趣的节奏并牢固地建立在一个固定的键降大调上每个时间步的预测分布作为热图图中为例热图如图所示这里值得注意的有趣的一点是模型已经清楚地了解了哪些音符属于特定的键因为在不属于该键的音符的分布中存在空白例如有一个灰色的间隙沿行注释对应于这个音在降大调的乐曲中是极不可能出现的在生成过程的早期图的左边关键还没有牢固地建立起来因此在如何选择下一个音符方面有更多的不确定性随着作品的发展模型固定在一个键上某些音符几乎肯定不会出现值得注意的是这个模型并没有在一开始就明确地决定将音乐设置在某个键上而是在它的过程中不断地创造它试图选择最适合它之前选择的音符值得指出的是该模型已经学习了巴赫的典型风格即在大提琴上下降到一个低音来结束一个乐句然后再弹回来开始下一个乐句看看在音符左右乐句以低调结束这在巴赫大提琴组曲中很常见然后在下一个乐句开始时回到更高更铿锵的乐器音域这正是模型所预测的在低音高和下一个音符之间有一个很大的灰色间隙这个音符预计在音高左右而不是继续在乐器的深处隆隆作响最后我们应该检查我们的注意力机制是否像预期的那样工作图所示为网络在生成序列中各点计算出的向量元素值横轴表示生成的音符序列纵轴显示了当沿水平轴即向量预测每个音符时网络的注意力集中在哪里正方形越暗对序列中与此点对应的隐藏状态的关注就越大我们可以看到对于作品的第二个音符降网络选择将几乎所有的注意力放在作品的第一个音符也是的事实上这是有道理的如果你知道第一个音符是降你可能会用这个信息来决定下一个音符当我们在接下来的几个音符中移动时神经网络将它的注意力大致均匀地分散在之前的音符上然而它很少把注意力放在超过六个音符之前的音符上再说一次这是有道理的在前六个隐藏状态中可能包含足够的信息来理解这个短语应该如何继续也有网络选择忽略附近某个音符的例子因为它不会在理解短语时添加任何额外的信息例如看一下图表中心的白色方框注意中间有一条方框它切断了回顾前面四到六个音符的通常模式为什么网络在决定如何继续这个短语时愿意选择忽略这个注释呢如果你看一下它对应的是哪个音符你会发现它是三个音符中的第一个模型选择忽略这一点因为在此之前的音符也是降低一个八度此时网络的隐藏状态将为模型提供足够的信息来理解降是这段话中的一个重要音符因此模型不需要注意后续的更高的降因为它没有添加任何额外的信息更多的证据表明该模型已经开始理解八度的概念可以在下方和右侧的绿色框中看到在这里模型选择忽略低因为在此之前的音符也是高一个八度记住我们并没有告诉模型哪些音符是通过八度相关联的它只是通过研究巴赫的音乐自己解决了这个问题这很了不起注意机制是一个强大的工具可以帮助网络决定循环层的哪些先前状态对于预测序列的延续是重要的到目前为止我们已经看到了提前一个音符的预测然而我们也可能希望将注意力构建到编码器解码器网络中在那里我们通过使用解码器来预测未来音符的序列而不是一次构建一个音符序列回顾一下图显示了一个标准的编码器解码器模型在没有注意的情况下的样子我们在第章中介绍的那种图显示了相同的网络但在编码器和解码器之间增加了注意机制注意机制的工作方式与我们之前看到的完全相同只是有一点改变解码器的隐藏状态也被纳入到机制中这样模型不仅可以通过之前的编码器隐藏状态还可以从当前的解码器隐藏状态决定将注意力集中在哪里图显示了编码器解码器框架中注意模块的内部工作原理虽然在编码器解码器网络中存在许多注意机制的副本但它们都共享相同的权值因此在需要学习的参数数量上没有额外的开销唯一的变化是现在解码器隐藏状态被滚动到注意力计算中图中的红线这稍微改变了方程加入了一个额外的索引来指定解码器的步长还要注意在图中我们如何使用编码器的最终状态来初始化解码器的隐藏状态在有注意的编码器解码器中我们使用循环层的内置标准初始化器来初始化解码器上下文向量与传入数据连接起来形成一个扩展的数据向量进入解码器的每个单元因此我们将上下文向量视为输入解码器的附加数据我们在本节中探索的带有注意机制框架的在单线单音音乐中效果很好但它能适应多线复音音乐吗框架当然足够灵活可以通过循环机制同时生成多条音乐线但就目前而言我们目前的数据集并没有很好地为此设置因为我们将和弦存储为单个实体而不是由多个单独的音符组成的部分例如我们当前的无法知道大调和弦和实际上与小调和弦和非常接近只有一个音符需要改变即变为相反它将两者视为两个不同的元素可以独立预测理想情况下我们希望设计一个网络可以接受多个渠道的音乐作为单独的流并学习这些流应该如何相互作用以产生好听的音乐而不是不和谐的噪音这听起来是不是有点像生成图像对于图像生成我们有三个通道红绿蓝我们希望网络学习如何组合这些通道来生成漂亮的图像而不是随机的像素化噪声事实上正如我们将在下一节看到的我们可以将音乐生成直接视为图像生成问题这意味着我们可以将同样的基于卷积的技术应用于音乐图像生成问题而不是使用循环网络特别是在我们探索这个新建筑之前我们只有足够的时间去参观音乐厅那里的演出即将开始指挥家在指挥台上敲了两下指挥棒演出就要开始了在他面前坐着一支管弦乐队然而这支乐团并不打算演奏贝多芬的交响乐或柴可夫斯基的序曲这个管弦乐队在演出期间现场创作原创音乐完全由一组演奏者向舞台中央的一个巨大的管风琴简称发出指令它将这些指令转化为美妙的音乐为观众带来愉悦管弦乐队可以通过训练来演奏特定风格的音乐而且没有两次演出是相同的管弦乐队的名演奏者被分成个平均的小组每组名演奏者每个部分都向提供指示并在管弦乐队中负有明确的责任风格组负责制作演出的整体音乐风格在许多方面它是所有部分中最简单的工作因为每个演奏者只需要在音乐会开始时生成一个指令然后在整个演出过程中不断地向博物馆提供信息凹槽部分有类似的工作但每个播放器产生几个指令一个为每个不同的音乐轨道由输出例如在一场音乐会中每个部分的成员制作了五个指令分别对应人声钢琴弦乐贝斯和鼓的音轨因此他们的工作是为每一个独立的器乐声音提供槽然后在整个演出中保持不变风格和凹槽部分在整个作品中没有改变它们的指示表演的动态元素是由最后两个部分提供的这确保了音乐随着每个小节的变化而不断变化小节或小节是一个小的音乐单位包含固定的少量的节拍例如如果你能跟着一段音乐数那么每小节就有两拍你可能在听进行曲如果你能数那么每小节有三拍你可能正在听华尔兹和弦部分的演奏者在每小节开始时改变他们的指示这样做的效果是给每个小节一个独特的音乐特征例如通过一个和弦的变化和弦部分的演奏者每小节只产生一个指令然后应用于每个器乐轨道旋律部分的乐手是最累人的工作因为他们在整首曲子的每个小节开始时对每个器乐音轨给出不同的指示这些玩家对音乐有最精细的控制因此这可以被认为是提供旋律兴趣的部分这就完成了对管弦乐队的描述我们可以将各部分的职责总结如表所示根据当前的条指令每个播放器一条指令由生成下一个音乐小节训练做到这一点并不容易最初乐器只会产生可怕的噪音因为它无法理解如何解释指令来产生与真正的音乐没有区别的小节这就是指挥员的作用当音乐与真实音乐明显不同时指挥会告诉然后调整其内部线路以便下次更有可能骗过指挥指挥和博物馆使用的过程与我们在第章看到的完全相同和一起工作不断改进拍摄的动物照片的演奏者在世界各地巡回演出在有足够的现有音乐来训练的地方举办任何风格的音乐会在下一节中我们将看到如何使用构建以学习如何生成逼真的复调音乐年月和发表了他们的世界模型论文这篇论文展示了如何训练一个模型该模型可以通过在自身生成的幻觉梦境中进行实验而不是在环境本身中学习如何执行特定任务当与强化学习等其他机器学习技术一起应用时这是一个很好的例子说明了生成式建模如何用于解决实际问题该架构的一个关键组件是生成模型在给定当前状态和动作的情况下该模型可以构建下一个可能状态的概率分布在通过随机运动建立了对环境基本物理的理解之后该模型能够完全在其自身对环境的内部表示中从头开始训练自己的新任务这种方法在两项测试中都获得了世界上最好的分数在本章中我们将详细探索该模型并展示如何创建这种惊人的尖端技术的自己版本基于原始论文我们将构建一个强化学习算法学习如何以尽可能快的速度在赛道上驾驶汽车虽然我们将使用计算机模拟作为我们的环境但同样的技术也可以应用于现实世界的场景在真实环境中测试策略是昂贵的或不可实现的然而在我们开始构建模型之前我们需要仔细了解一下强化学习的概念和平台强化学习可以定义如下强化学习是机器学习的一个领域旨在训练智能体在给定环境中针对特定目标进行最佳表现判别建模和生成建模的目标都是最小化观测数据集上的损失函数而强化学习的目标是最大化智能体在给定环境中的长期奖励它通常被描述为机器学习的三大分支之一与监督学习使用标记数据进行预测和无监督学习从无标记数据中学习结构并列让我们首先介绍一些与强化学习相关的关键术语智能体运行的世界它定义了一组规则根据智能体的先前动作和当前游戏状态控制游戏状态更新过程和奖励分配例如如果我们正在教强化学习算法下国际象棋环境将由控制给定行动例如移动如何影响下一个游戏状态棋盘上棋子的新位置的规则组成还将指定如何评估给定位置是否被将死并在获胜棋手获胜后分配的奖励在环境中执行操作的智能体代表代理可能的特定情况的数据遭遇也称为状态例如一个特定的棋盘配置附带的游戏信息如哪个玩家会采取下一步行动一个智能体可以采取的可行行动奖励在环境中运行一次智能体这也被称为对于离散事件环境所有状态动作和奖励都被标下标以显示它们在时间步的值这些定义之间的关系如图所示首先用当前游戏状态初始化环境在时间步智能体接收当前游戏状态并使用它来决定下一个最佳动作然后执行该动作给定这个动作环境然后计算下一个状态并奖励并将这些传递回智能体以便再次开始循环循环继续直到满足事件的结束标准例如经过给定数量的时间步长或代理赢输我们如何设计一个智能体来最大化给定环境中的奖励总和我们可以构建一个智能体它包含一套如何响应任何给定游戏状态的规则然而随着环境变得更加复杂这很快就变得不可行并且永远不允许我们在特定任务中构建具有超人能力的智能体因为我们正在硬编码规则强化学习包括创建一个智能体该智能体可以通过反复游戏在复杂环境中学习最优策略本章将使用它来构建我们的智能体现在我将介绍赛车环境之家我们将使用它来模拟汽车在赛道上行驶是一个用于开发强化学习算法的工具包可以作为库使用该库中包含几个经典的强化学习环境如和以及提出更复杂挑战的环境如训练智能体在不平坦的地形上行走或赢得游戏所有的环境都提供了方法你可以通过它提交给定的操作环境将返回下一个状态和奖励通过使用智能体选择的操作反复调用方法您可以在环境中播放一个片段除了每个环境的抽象机制外还提供了允许您观看智能体在给定环境中执行的图形这对于调试和查找智能体可以改进的地方很有用我们将利用中的赛车环境让我们看看如何为这个环境定义游戏状态动作奖励和情节图以图形形式展示了这些概念请注意汽车从它的视角看不到轨道但我们应该想象一个漂浮在轨道上方的智能体从鸟瞰图控制汽车在探索构建每个组件所需的详细步骤之前我们现在将介绍我们将用于构建通过强化学习进行学习的代理的整个架构的高级概述该解决方案由三个不同的部分组成分别进行训练如图所示当你在开车时做决定时你不会主动分析视图中的每个像素而是将视觉信息压缩为少量的潜在实体例如道路的直线度即将到来的转弯以及你相对于道路的位置以通知你的下一步行动我们在第章中看到如何通过最小化重构误差和散度将一个高维输入图像压缩为一个近似服从标准多元正态分布的潜在随机变量这确保了潜空间是连续的并且我们能够轻松地从中采样以生成有意义的新观测在赛车示例中将输入图像压缩为维正态分布随机变量参数为和这里是分布方差的对数我们可以从这个分布中采样以产生表示当前状态的潜在向量这被传递到网络的下一部分当你开车时随后的每一个观察对你来说都不是一个完全的惊喜如果当前的观察显示前方道路左转而你把方向盘转到左边你期望下一次观察显示你仍然与道路保持一致如果你没有这种能力你的驾驶可能会在整个道路上蜿蜒因为你无法看到稍微偏离中心会在下一个时间步变得更糟除非你现在就做些什么这种前瞻性思维是的工作这是一个试图根据前一个潜在状态和前一个动作预测下一个潜在状态分布的网络具体来说是一个具有个隐藏单元的层后面是一个混合密度网络输出层它允许下一个潜在状态实际上可以从几个正态分布中的任何一个中提取世界模型论文的作者之一将同样的技术应用于手写生成任务如图所示以描述下一个笔尖可能落在任何一个明显的红色区域的事实在赛车的例子中我们允许从五个正态分布中的任意一个提取下一个观察到的潜在状态的每个元素到目前为止我们还没有提到任何关于选择动作的内容这个责任在于控制者控制器是一个密集连接的神经网络其中输入是从编码的分布中采样的当前潜状态和的隐藏状态的连接三个输出神经元对应于三个动作转弯加速刹车并被缩放到适当的范围内下降我们需要使用强化学习来训练控制器因为没有训练数据集可以告诉我们某个动作是好的另一个动作是坏的相反智能体将需要通过反复实验自己发现这一点正如我们将在本章后面看到的世界模型论文的关键是它演示了如何在智能体自己的环境生成模型中进行这种强化学习而不是在环境中换句话说它发生在智能体对环境行为的幻觉版本中而不是真实的东西为了理解这三个组件的不同角色以及它们如何一起工作我们可以想象它们之间的对话查看最新的观察这看起来像一条笔直的道路有一个轻微的左转弯接近汽车面向道路的方向根据描述和控制器在最后一个时间步动作选择加速的事实我将更新我的隐藏状态以便预测下一个观察仍然是直线道路但在视图中有更多的左转控制器基于的描述和的当前隐藏状态我的神经网络输出作为下一个动作然后控制器的动作被传递给环境环境返回一个更新后的观测值然后循环再次开始有关该模型的更多信息也有一个优秀的在线交互式解释',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-09-16 19:21:13',
  postMainColor: '',
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#18171d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#f7f9fe')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(e => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.0.0-rc1">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head><body data-type="anzhiyu"><div id="web_bg"></div><div id="an_music_bg"></div><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><img class="loading-img nolazyload" alt="加载头像" src="https://npm.elemecdn.com/anzhiyu-blog-static@1.0.4/img/avatar.jpg"/><div class="loading-image-dot"></div></div></div><script>const preloader = {
  endLoading: () => {
    document.getElementById('loading-box').classList.add("loaded");
  },
  initLoading: () => {
    document.getElementById('loading-box').classList.remove("loaded")
  }
}
window.addEventListener('load',()=> { preloader.endLoading() })
setTimeout(function(){preloader.endLoading();},10000)

if (true) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.10/progress_bar/progress_bar.css"/><script async="async" src="https://cdn.cbd.int/pace-js@1.2.4/pace.min.js" data-pace-options="{ &quot;restartOnRequestAfter&quot;:false,&quot;eventLag&quot;:false}"></script><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><div id="nav-group"><span id="blog_name"><div class="back-home-button"><i class="anzhiyufont anzhiyu-icon-grip-vertical"></i><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">项目</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://gitee.com/promoteAI/mkdown-images/tree/master/imgs" title="图床"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://image.anheyu.com/favicon.ico" alt="图床"/><span class="back-menu-item-text">图床</span></a></div></div><div class="back-menu-list-group"><div class="back-menu-list-title">项目</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://image.anheyu.com/" title="安知鱼图床"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://image.anheyu.com/favicon.ico" alt="安知鱼图床"/><span class="back-menu-item-text">安知鱼图床</span></a></div></div></div></div><a id="site-name" href="/" accesskey="h"><div class="title">promoteAI</div><i class="anzhiyufont anzhiyu-icon-house-chimney"></i></a><div id="he-plugin-simple"></div><script>var WIDGET = {
  "CONFIG": {
    "modules": "0124",
    "background": "2",
    "tmpColor": "FFFFFF",
    "tmpSize": "16",
    "cityColor": "FFFFFF",
    "citySize": "16",
    "aqiColor": "E8D87B",
    "aqiSize": "16",
    "weatherIconSize": "24",
    "alertIconSize": "18",
    "padding": "10px 10px 10px 10px",
    "shadow": "0",
    "language": "auto",
    "borderRadius": "20",
    "fixed": "true",
    "vertical": "top",
    "horizontal": "left",
    "left": "20",
    "top": "7.1",
    "key": "df245676fb434a0691ead1c63341cd94"
  }
}
</script><link rel="stylesheet" href="https://widget.qweather.net/simple/static/css/he-simple.css?v=1.4.0"/><script src="https://widget.qweather.net/simple/static/js/he-simple.js?v=1.4.0"></script></span><div class="mask-name-container"><div id="name-container"><a id="page-name" href="javascript:anzhiyu.scrollToDest(0, 500)">PAGE_NAME</a></div></div><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 文章</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><i class="anzhiyufont anzhiyu-icon-box-archive faa-tada" style="font-size: 0.9em;"></i><span> 隧道</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><i class="anzhiyufont anzhiyu-icon-shapes faa-tada" style="font-size: 0.9em;"></i><span> 分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags faa-tada" style="font-size: 0.9em;"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 友链</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/link/"><i class="anzhiyufont anzhiyu-icon-link faa-tada" style="font-size: 0.9em;"></i><span> 友人帐</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/fcircle/"><i class="anzhiyufont anzhiyu-icon-artstation faa-tada" style="font-size: 0.9em;"></i><span> 朋友圈</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/comments/"><i class="anzhiyufont anzhiyu-icon-envelope faa-tada" style="font-size: 0.9em;"></i><span> 留言板</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 我的</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/music/"><i class="anzhiyufont anzhiyu-icon-music faa-tada" style="font-size: 0.9em;"></i><span> 音乐馆</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/bangumis/"><i class="anzhiyufont anzhiyu-icon-bilibili faa-tada" style="font-size: 0.9em;"></i><span> 追番页</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/album/"><i class="anzhiyufont anzhiyu-icon-images faa-tada" style="font-size: 0.9em;"></i><span> 相册集</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/air-conditioner/"><i class="anzhiyufont anzhiyu-icon-fan faa-tada" style="font-size: 0.9em;"></i><span> 小空调</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 关于</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/about/"><i class="anzhiyufont anzhiyu-icon-paper-plane faa-tada" style="font-size: 0.9em;"></i><span> 关于本人</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/essay/"><i class="anzhiyufont anzhiyu-icon-lightbulb faa-tada" style="font-size: 0.9em;"></i><span> 闲言碎语</span></a></li><li><a class="site-page child faa-parent animated-hover" href="javascript:toRandomPost()"><i class="anzhiyufont anzhiyu-icon-shoe-prints1 faa-tada" style="font-size: 0.9em;"></i><span> 随便逛逛</span></a></li></ul></div></div></div><div id="nav-right"><div class="nav-button only-home" id="travellings_button" title="随机前往一个开往项目网站"><a class="site-page" onclick="anzhiyu.totraveling()" title="随机前往一个开往项目网站" href="javascript:void(0);" rel="external nofollow" data-pjax-state="external"><i class="anzhiyufont anzhiyu-icon-train"></i></a></div><div class="nav-button" id="randomPost_button"><a class="site-page" onclick="toRandomPost()" title="随机前往一个文章" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-dice"></i></a></div><div class="nav-button" id="search-button"><a class="site-page social-icon search" href="javascript:void(0);" title="搜索🔍" accesskey="s"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span> 搜索</span></a></div><input id="center-console" type="checkbox"/><label class="widget" for="center-console" title="中控台" onclick="anzhiyu.switchConsole();"><i class="left"></i><i class="widget center"></i><i class="widget right"></i></label><div id="console"><div class="console-card-group-reward"><ul class="reward-all console-card"><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" target="_blank"><img class="post-qr-code-img" alt="微信" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" target="_blank"><img class="post-qr-code-img" alt="支付宝" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div><div class="console-card-group"><div class="console-card-group-left"><div class="console-card" id="card-newest-comments"><div class="card-content"><div class="author-content-item-tips">互动</div><span class="author-content-item-title"> 最新评论</span></div><div class="aside-list"><span>正在加载中...</span></div></div></div><div class="console-card-group-right"><div class="console-card tags"><div class="card-content"><div class="author-content-item-tips">兴趣点</div><span class="author-content-item-title">寻找你感兴趣的领域</span></div></div><div class="console-card history"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-box-archiv"></i><span>文章</span></div><div class="card-archives"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-archive"></i><span>归档</span></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/05/"><span class="card-archive-list-date">五月 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li></ul></div><hr/></div></div></div><div class="button-group"><div class="console-btn-item"><a class="darkmode_switchbutton" title="显示模式切换" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-moon"></i></a></div><div class="console-btn-item" id="consoleHideAside" onclick="anzhiyu.hideAsideBtn()" title="边栏显示控制"><a class="asideSwitch"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></a></div><div class="console-btn-item" id="consoleMusic" onclick="anzhiyu.musicToggle()" title="音乐开关"><a class="music-switch"><i class="anzhiyufont anzhiyu-icon-music"></i></a></div></div><div class="console-mask" onclick="anzhiyu.hideConsole()" href="javascript:void(0);"></div></div><div class="nav-button" id="nav-totop"><a class="totopbtn" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i><span id="percent" onclick="anzhiyu.scrollToDest(0,500)">0</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" title="切换"><i class="anzhiyufont anzhiyu-icon-bars"></i></a></div></div></div></nav></header><main id="blog-container"><div class="layout" id="content-inner"><div id="post"><div id="post-info"><div id="post-firstinfo"><div class="meta-firstline"><a class="post-meta-original">原创</a><span class="post-meta-categories"><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-inbox post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url">人工智能</a></span><span class="article-meta tags"></span></div></div><h1 class="post-title" itemprop="name headline">生成式AI详解<a class="post-edit-link" href="undefined_posts/生成式AI详解.md" title="在 GitHub 上编辑 - 生成式AI详解" target="_blank"><i class="anzhiyufont anzhiyu-icon-pencil"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" itemprop="dateCreated datePublished" datetime="2023-05-12T04:30:27.816Z" title="发表于 2023-05-12 12:30:27">2023-05-12</time><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-history post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" itemprop="dateCreated datePublished" datetime="2025-09-16T11:21:13.360Z" title="更新于 2025-09-16 19:21:13">2025-09-16</time></span></div><div class="meta-secondline"><span class="post-meta-separator"></span><span class="post-meta-wordcount"><i class="anzhiyufont anzhiyu-icon-file-word post-meta-icon" title="文章字数"></i><span class="post-meta-label" title="文章字数">字数总计:</span><span class="word-count" title="文章字数">26.4k</span><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-clock post-meta-icon" title="阅读时长"></i><span class="post-meta-label" title="阅读时长">阅读时长:</span><span>85分钟</span></span><span class="post-meta-separator">       </span><span class="post-meta-position" title="作者IP属地为北京"><i class="anzhiyufont anzhiyu-icon-location-dot"></i>北京</span></div></div></div><article class="post-content" id="article-container" itemscope itemtype="http://promoteai.com/2023/05/12/sheng-cheng-shi-ai-xiang-jie/"><header><a class="post-meta-categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url">人工智能</a><h1 id="CrawlerTitle" itemprop="name headline">生成式AI详解</h1><span itemprop="author" itemscope itemtype="http://schema.org/Person">陈汉江</span><time itemprop="dateCreated datePublished" datetime="2023-05-12T04:30:27.816Z" title="发表于 2023-05-12 12:30:27">2023-05-12</time><time itemprop="dateCreated datePublished" datetime="2025-09-16T11:21:13.360Z" title="更新于 2025-09-16 19:21:13">2025-09-16</time></header><h2 id="Chapter-5-Paint"><a href="#Chapter-5-Paint" class="headerlink" title="Chapter 5. Paint"></a>Chapter 5. Paint</h2><p>到目前为止，我们已经探索了各种训练模型生成新样本的方法，只需要给定我们想要模仿的训练数据集。我们将其应用于几个数据集，并看到在每种情况下，VAEs和GANs如何能够学习潜在空间和原始像素空间之间的映射。通过从潜空间的分布中采样，我们可以使用生成模型将该向量映射到像素空间中的新图像。</p>
<p>请注意，到目前为止，我们看到的所有示例都是从头开始生成新的观察结果的，也就是说，除了从潜在空间中采样的随机潜在向量用于生成图像外，没有其他输入。生成模型的另一个应用是在风格迁移领域。本文的目标是建立一个模型，可以转换输入的基图像，以便给人一种它与给定的一组风格图像来自同一集合的印象。这种技术有明显的商业应用，现在被用于计算机图形软件、计算机游戏设计和移动电话应用程序。图5-1展示了其中的一些例子。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230418193555408.png" alt="image-20230418193555408"></p>
<p>在本章中，你将学习如何构建两种不同类型的风格迁移模型(CycleGAN和Neural style transfer)，并将这些技术应用于你自己的照片和艺术品。我们将从参观一家水果和蔬菜店开始，那里的一切并不像看上去那样……</p>
<h3 id="CycleGAN"><a href="#CycleGAN" class="headerlink" title="CycleGAN"></a>CycleGAN</h3><p>风格迁移常用的模型:循环一致对抗网络，或CycleGAN。原始论文代表了风格迁移领域的重要一步，因为它展示了如何训练一个模型，在没有成对样本的训练集的情况下，将风格从参考图像集复制到不同的图像上。之前的风格迁移模型，如pix2pix，要求训练集中的每个图像都存在于源域和目标域。虽然对于某些风格的问题设置(例如，黑白到彩色照片，映射到卫星图像)，可以制造这种数据集，但对于其他问题，这是不可能的。例如，我们没有莫奈画《睡莲》系列的池塘的原始照片，也没有毕加索的帝国大厦画作。将马和斑马站在相同位置的照片进行整理也需要花费巨大的精力。</p>
<p>CycleGAN论文在pix2pix论文发布几个月后发布，展示了如何训练一个模型来解决源域和目标域没有图像对的问题。图5-4分别展示了pix2pix和CycleGAN的成对数据集和未成对数据集之间的差异。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230418194309402.png" alt="image-20230418194309402"></p>
<p>虽然pix2pix只能在一个方向上工作(从源到目标)，但CycleGAN同时在两个方向上训练模型，以便模型学习将图像从目标到源以及源到目标。这是模型架构的结果，因此您可以免费获得相反的方向。</p>
<p>现在让我们看看如何在Keras中构建CycleGAN模型。首先，我们将使用前面的苹果和橘子的例子来遍历CycleGAN的每个部分，并对该架构进行实验。然后，我们将应用相同的技术来建立一个模型，可以将给定艺术家的风格应用到您选择的照片中。</p>
<h4 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h4><p>使用apple2orange数据集</p>
<p><a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/apple2orange.zip">https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/apple2orange.zip</a></p>
<h4 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h4><p>CycleGAN实际上由四个模型、两个发生器和两个鉴别器组成。第一个生成器G_AB将图像从域A转换到域B。第二个生成器G_BA将图像从域B转换到域A。</p>
<p>由于我们没有成对的图像来训练我们的生成器，我们还需要训练两个判别器来确定生成器生成的图像是否令人信服。第一个鉴别器d_A被训练成能够识别来自域A的真实图像和由生成器G_BA产生的假图像之间的差异。相反，鉴别器d_B被训练成能够识别来自域B的真实图像和由生成器G_AB产生的假图像之间的差异。四种型号的对应关系如图5-5所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230419170224638.png" alt="image-20230419170224638"></p>
<p>让我们首先看一下生成器的架构。通常，4个CycleGAN生成器采用两种形式之一:U-Net或ResNet(残差网络)。在他们早期的pix2pix论文中，作者使用了U-Net架构，但他们为CycleGAN切换到了ResNet架构。本章将从U-Net开始构建这两种架构</p>
<h4 id="The-Generators-U-Net"><a href="#The-Generators-U-Net" class="headerlink" title="The Generators (U-Net)"></a>The Generators (U-Net)</h4><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230419170753927.png" alt="image-20230419170753927"></p>
<p>与变分自编码器类似，U-Net由两部分组成:下采样的一半，输入图像在空间上进行压缩，但在通道上进行扩展，上采样的一半，在空间上扩展表示，同时减少通道数量。</p>
<p>然而，与VAE不同的是，在网络的上采样和下采样部分中，形状相同的层之间也存在跳跃连接。VAE是线性的;数据通过网络从输入流到输出，一层接一层。U-Net是不同的，因为它包含跳过连接，允许信息通过网络的部分快捷方式传递到后面的层。这里的直觉是，随着网络的下采样部分的每一层的后续，模型越来越多地捕获图像的内容，并丢失关于哪里的信息。在U的顶点，特征图将学习对图像中内容的上下文理解，而对其位置的理解很少。对于预测分类模型，这就是我们所需要的，因此我们可以将其连接到最终的密集层，以输出图像中特定类存在的概率。然而，对于原始的U-Net应用程序(图像分割)和风格迁移，至关重要的是，当我们上采样回原始图像大小时，我们将下采样期间丢失的空间信息传递回每一层。这正是我们需要skip连接的原因。它们允许网络将下采样过程中捕获的高级抽象信息(即图像风格)与从网络中的前一层反馈回来的特定空间信息(即图像内容)相融合。</p>
<p> 为了构建跳跃连接，我们需要引入一种新类型的层:Concatenate。</p>
<h5 id="CONCATENATE-LAYER"><a href="#CONCATENATE-LAYER" class="headerlink" title="CONCATENATE LAYER"></a>CONCATENATE LAYER</h5><p>Concatenate层只是沿着特定的轴(默认是最后一个轴)将一组层连接在一起。例如，在Keras中，我们可以将前面的x层和y层连接在一起，如下所示:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Concatenate()([x,y])</span><br></pre></td></tr></tbody></table></figure>

<p>在U-Net中，我们使用级联层将上采样层连接到网络的下采样部分中同等大小的层。这些层沿着通道维度连接在一起，因此通道的数量从k增加到2k，而空间维度的数量保持不变。注意，级联层中不需要学习权重;它们只是用来“粘合”前几层。</p>
<p> 生成器还包含另一个新的层类型，</p>
<p>InstanceNormalization。</p>
<h5 id="INSTANCE-NORMALIZATION-LAYER"><a href="#INSTANCE-NORMALIZATION-LAYER" class="headerlink" title="INSTANCE NORMALIZATION LAYER"></a>INSTANCE NORMALIZATION LAYER</h5><p>这个CycleGAN的生成器使用实例规范化层而不是批规范化层，这在风格迁移问题中可以导致更令人满意的结果。</p>
<p>实例规范化层(instancnormalization layer)单独规范化每个观测值，而不是作为一个批处理进行规范化。与BatchNormalization层不同，它不需要在训练期间将mu和sigma参数作为运行平均值进行计算，因为在测试时该层可以像在训练时一样对每个实例进行归一化。用于归一化每层的均值和标准偏差计算每个通道和每个观测。</p>
<p>此外，对于该网络中的实例归一化层，没有权重需要学习，因为我们没有使用缩放(gamma)或移位(beta)参数。</p>
<p>图5-7展示了批量归一化和实例归一化以及其他两种归一化方法(层归一化和组归一化)之间的区别。</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230419172836165.png" alt="image-20230419172836165"></p>
<p>这里，N是批处理轴，C是通道轴，(H, W)表示空间轴。因此，立方体表示归一化层的输入张量。蓝色像素使用相同的均值和方差(根据这些像素的值计算)进行归一化。</p>
<h5 id="core-code"><a href="#core-code" class="headerlink" title="core code"></a>core code</h5><p>keras版本</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 核心部分代码</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">unet_generator</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">'''Unet生成器网络'''</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">downsample</span>(<span class="params">layer_input, filters, kernel_size=<span class="number">4</span></span>):</span><br><span class="line">            <span class="string">'''下采样'''</span></span><br><span class="line">            d = Conv2D(filters, kernel_size, strides=<span class="number">2</span>, padding=<span class="string">'same'</span>)(layer_input)</span><br><span class="line">            d = InstanceNormalization(axis=-<span class="number">1</span>, center=<span class="literal">False</span>, scale=<span class="literal">False</span>)(d)</span><br><span class="line">            d = ReLU()(d)</span><br><span class="line">            <span class="keyword">return</span> d</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">upsample</span>(<span class="params">layer_input, skip_input, filters, kernel_size=<span class="number">4</span>, droput_rate=<span class="number">0</span></span>):</span><br><span class="line">            <span class="string">'''上采样'''</span></span><br><span class="line">            u = UpSampling2D(size=<span class="number">2</span>)(layer_input)</span><br><span class="line">            u = Conv2D(filters, kernel_size=kernel_size, strides=<span class="number">1</span>, padding=<span class="string">'same'</span>)(u)</span><br><span class="line">            u = InstanceNormalization(axis=-<span class="number">1</span>, center=<span class="literal">False</span>, scale=<span class="literal">False</span>)(u)</span><br><span class="line">            u = ReLU()(u)</span><br><span class="line">            <span class="keyword">if</span> droput_rate:</span><br><span class="line">                u = Dropout(droput_rate)(u)</span><br><span class="line">            u = Concatenate()([u, skip_input])</span><br><span class="line">            <span class="keyword">return</span> u</span><br><span class="line"></span><br><span class="line">        <span class="comment"># image input</span></span><br><span class="line">        img = Input(shape=self.img_shape)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Downsampling</span></span><br><span class="line">        d1 = downsample(img, self.gen_n_filters)</span><br><span class="line">        d2 = downsample(d1, self.gen_n_filters * <span class="number">2</span>)</span><br><span class="line">        d3 = downsample(d2, self.gen_n_filters * <span class="number">4</span>)</span><br><span class="line">        d4 = downsample(d3, self.gen_n_filters * <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Upsampling</span></span><br><span class="line">        u1 = upsample(d4, d3, self.gen_n_filters * <span class="number">4</span>)</span><br><span class="line">        u2 = upsample(u1, d2, self.gen_n_filters * <span class="number">2</span>)</span><br><span class="line">        u3 = upsample(u2, d1, self.gen_n_filters)</span><br><span class="line">        u4 = UpSampling2D(size=<span class="number">2</span>)(u3)</span><br><span class="line"></span><br><span class="line">        output = Conv2D(self.channels, kernel_size=<span class="number">4</span>, strides=<span class="number">1</span>, padding=<span class="string">'same'</span>, activation=<span class="string">'tanh'</span>)(u4)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> Model(img, output)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>

<h4 id="The-Discriminators"><a href="#The-Discriminators" class="headerlink" title="The Discriminators"></a>The Discriminators</h4><p>到目前为止，我们看到的鉴别器只输出一个数字:输入图像为“实”的预测概率。我们将构建的CycleGAN中的鉴别器输出8 × 8单通道张量，而不是单个数字。</p>
<p>这样做的原因是CycleGAN继承了PatchGAN模型的鉴别器架构，在PatchGAN模型中，鉴别器将图像划分为重叠的方形“补丁”，并猜测每个补丁是真的还是假的，而不是对整个图像进行预测。因此鉴别器的输出是一个张量，其中包含每个patch的预测概率，而不仅仅是一个数字。</p>
<p>请注意，当我们通过网络传递图像时，这些补丁是同时预测的——我们不会手动分割图像，然后逐个通过网络传递每个补丁。由于鉴别器的卷积结构，将图像划分为小块是很自然的。</p>
<p>使用PatchGAN鉴别器的好处是，损失函数可以衡量鉴别器根据风格而不是内容区分图像的能力。由于鉴别器预测的每个单独元素仅基于图像的一个小正方形，因此它必须使用补丁的样式而不是其内容来做出决定。这正是我们所需要的;我们宁愿我们的鉴别器擅长于识别两张图片在风格上的不同，而不是内容上的不同。</p>
<h5 id="core-code-1"><a href="#core-code-1" class="headerlink" title="core code"></a>core code</h5><p>鉴别器的Keras代码</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">discriminator</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="string">'''判别器'''</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">conv4</span>(<span class="params">layer_input, filters, stride=<span class="number">2</span>, norm=<span class="literal">True</span></span>):</span><br><span class="line">        y = Conv2D(filters, kernel_size=<span class="number">4</span>, strides=stride, padding=<span class="string">'same'</span>)(layer_input)</span><br><span class="line">        <span class="keyword">if</span> norm:</span><br><span class="line">            y = InstanceNormalization(axis=-<span class="number">1</span>, center=<span class="literal">False</span>, scale=<span class="literal">False</span>)(y)</span><br><span class="line">        y = LeakyReLU(<span class="number">0.2</span>)(y)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">    img = Input(shape=self.img_shape)</span><br><span class="line"></span><br><span class="line">    y = conv4(img, self.disc_n_filters, stride=<span class="number">2</span>, norm=<span class="literal">False</span>)</span><br><span class="line">    y = conv4(y, self.disc_n_filters * <span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">    y = conv4(y, self.disc_n_filters * <span class="number">4</span>, stride=<span class="number">2</span>)</span><br><span class="line">    y = conv4(y, self.disc_n_filters * <span class="number">8</span>, stride=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    output = Conv2D(<span class="number">1</span>, kernel_size=<span class="number">4</span>, strides=<span class="number">1</span>, padding=<span class="string">'same'</span>)(y)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Model(img, output)</span><br></pre></td></tr></tbody></table></figure>

<p>CycleGAN鉴别器是一系列卷积层，所有层都具有实例规范化(第一层除外)。</p>
<p>最后一层是卷积层，只有一个滤波器，没有激活。</p>
<h4 id="Compiling-the-CycleGAN"><a href="#Compiling-the-CycleGAN" class="headerlink" title="Compiling the CycleGAN"></a>Compiling the CycleGAN</h4><p>总结一下，我们的目标是建立一组模型，可以将域a(例如苹果的图像)转换为域B(例如橘子的图像)，反之亦然。因此，我们需要编译四个不同的模型，两个生成器和两个鉴别器，如下所示:</p>
<p>g_AB学习将图像从域A转换到域B。</p>
<p>g_BA学习将图像从域B转换到域A。</p>
<p>d_A学习来自域A的真实图像和g_BA生成的假图像之间的差异。</p>
<p>d_B学习来自域B的真实图像和g_AB生成的假图像之间的差异。</p>
<p>我们可以直接编译这两个鉴别器，因为我们有输入(来自每个域的图像)和输出(二进制响应:1表示图像来自该域，0表示它是生成的假图像)。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 编译判别器</span></span><br><span class="line">self.d_A = self.discriminator()</span><br><span class="line">self.d_B = self.discriminator()</span><br><span class="line">self.d_A.<span class="built_in">compile</span>(loss=<span class="string">'mse'</span>,</span><br><span class="line">				 optimizer=Adam(self.learning_rate, <span class="number">0.5</span>),</span><br><span class="line">       			 metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">self.d_B.<span class="built_in">compile</span>(loss=<span class="string">'mse'</span>,</span><br><span class="line">                 optimizer=Adam(self.learning_rate, <span class="number">0.5</span>),</span><br><span class="line">                 metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></tbody></table></figure>

<p> 但是，我们不能直接编译生成器，因为我们的数据集中没有成对的图像。相反，我们根据三个标准同时判断生成器:</p>
<ul>
<li><p>1.有效性。每个生成器产生的图像是否欺骗了相关的鉴别器?(例如，输出是否来自g_BA fool d_A，输出是否来自g_AB fool d_B?)</p>
</li>
<li><p>2.重建。如果我们依次应用两个生成器(在两个方向上)，我们会返回原始图像吗?CycleGAN得名于这个循环重构准则。</p>
</li>
<li><p>3.身份。如果我们将每个生成器应用于其自己的目标域的图像，图像是否保持不变?</p>
</li>
</ul>
<p>下面展示了如何编译一个模型来满足这三个条件(代码中的数字标记对应前面的列表)。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 编译生成器</span></span><br><span class="line">self.g_AB = self.unet_generator()</span><br><span class="line">self.g_BA = self.unet_generator()</span><br><span class="line"><span class="comment"># For the combined model we will only train the generators</span></span><br><span class="line">self.d_A.trainable = <span class="literal">False</span></span><br><span class="line">self.d_B.trainable = <span class="literal">False</span></span><br><span class="line">img_A = Input(shape=self.img_shape)</span><br><span class="line">img_B = Input(shape=self.img_shape)</span><br><span class="line">fake_A = self.g_BA(img_B)</span><br><span class="line">fake_B = self.g_AB(img_A)</span><br><span class="line"></span><br><span class="line">valid_A = self.d_A(fake_A)</span><br><span class="line">valid_B = self.d_B(fake_B)</span><br><span class="line"></span><br><span class="line">reconstr_A = self.g_BA(fake_B)</span><br><span class="line">reconstr_B = self.g_AB(fake_A)</span><br><span class="line"></span><br><span class="line">img_A_id = self.g_BA(img_A)</span><br><span class="line">img_B_id = self.g_AB(img_B)</span><br><span class="line"></span><br><span class="line">self.combined = Model(inputs=[img_A, img_B],</span><br><span class="line">                      output=[valid_A, valid_B,</span><br><span class="line">                              reconstr_A, reconstr_B,</span><br><span class="line">                              img_A_id, img_B_id])</span><br><span class="line">self.combined.<span class="built_in">compile</span>(loss=[<span class="string">'mse'</span>, <span class="string">'mse'</span>,</span><br><span class="line">                            <span class="string">'mse'</span>, <span class="string">'mse'</span>,</span><br><span class="line">                            <span class="string">'mse'</span>, <span class="string">'mse'</span></span><br><span class="line">                            ],</span><br><span class="line">                      loss_weights=[</span><br><span class="line">                          self.lambda_validation,</span><br><span class="line">                          self.lambda_validation,</span><br><span class="line">                          self.lambda_reconstr,</span><br><span class="line">                          self.lambda_reconstr,</span><br><span class="line">                          self.lambda_id,</span><br><span class="line">                          self.lambda_id,</span><br><span class="line">                      ],</span><br><span class="line">                      optimizer=self.optimizer</span><br><span class="line">                      )</span><br><span class="line">self.d_A.trainable = <span class="literal">True</span></span><br><span class="line">self.d_B.trainable = <span class="literal">True</span></span><br></pre></td></tr></tbody></table></figure>

<p> 组合模型接受来自每个域的一批图像作为输入，并为每个域提供三个输出(以匹配三个标准)——因此，总共有六个输出。请注意我们如何冻结判别器中的权重，这是典型的GANs，以便组合模型只训练生成器权重，即使判别器涉及到模型中。</p>
<p>总损失是每个准则损失的加权和。均方误差用于有效性标准——根据真实(1)或虚假(0)响应检查鉴别器的输出——平均绝对误差用于基于图像到图像的标准(重建和一致性)。</p>
<h4 id="Training-the-CycleGAN"><a href="#Training-the-CycleGAN" class="headerlink" title="Training the CycleGAN"></a>Training the CycleGAN</h4><p> 在我们的判别器和组合模型编译后，我们现在可以训练我们的模型了。这遵循了标准的GAN实践，即交替训练鉴别器和训练生成器(通过组合模型)。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_discriminators</span>(<span class="params">self, imgs_A, imgs_B, valid, fake</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Translate images to opposite domain</span></span><br><span class="line">    fake_B = self.g_AB.predict(imgs_A)</span><br><span class="line">    fake_A = self.g_BA.predict(imgs_B)</span><br><span class="line"></span><br><span class="line">    self.buffer_B.append(fake_B)</span><br><span class="line">    self.buffer_A.append(fake_A)</span><br><span class="line"></span><br><span class="line">    fake_A_rnd = random.sample(self.buffer_A, <span class="built_in">min</span>(<span class="built_in">len</span>(self.buffer_A), <span class="built_in">len</span>(imgs_A)))</span><br><span class="line">    fake_B_rnd = random.sample(self.buffer_B, <span class="built_in">min</span>(<span class="built_in">len</span>(self.buffer_B), <span class="built_in">len</span>(imgs_B)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Train the discriminators (original images = real / translated = Fake)</span></span><br><span class="line">    dA_loss_real = self.d_A.train_on_batch(imgs_A, valid)</span><br><span class="line">    dA_loss_fake = self.d_A.train_on_batch(fake_A_rnd, fake)</span><br><span class="line">    dA_loss = <span class="number">0.5</span> * np.add(dA_loss_real, dA_loss_fake)</span><br><span class="line"></span><br><span class="line">    dB_loss_real = self.d_B.train_on_batch(imgs_B, valid)</span><br><span class="line">    dB_loss_fake = self.d_B.train_on_batch(fake_B_rnd, fake)</span><br><span class="line">    dB_loss = <span class="number">0.5</span> * np.add(dB_loss_real, dB_loss_fake)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Total disciminator loss</span></span><br><span class="line">    d_loss_total = <span class="number">0.5</span> * np.add(dA_loss, dB_loss)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        d_loss_total[<span class="number">0</span>]</span><br><span class="line">        , dA_loss[<span class="number">0</span>], dA_loss_real[<span class="number">0</span>], dA_loss_fake[<span class="number">0</span>]</span><br><span class="line">        , dB_loss[<span class="number">0</span>], dB_loss_real[<span class="number">0</span>], dB_loss_fake[<span class="number">0</span>]</span><br><span class="line">        , d_loss_total[<span class="number">1</span>]</span><br><span class="line">        , dA_loss[<span class="number">1</span>], dA_loss_real[<span class="number">1</span>], dA_loss_fake[<span class="number">1</span>]</span><br><span class="line">        , dB_loss[<span class="number">1</span>], dB_loss_real[<span class="number">1</span>], dB_loss_fake[<span class="number">1</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_generators</span>(<span class="params">self, imgs_A, imgs_B, valid</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Train the generators</span></span><br><span class="line">    <span class="keyword">return</span> self.combined.train_on_batch([imgs_A, imgs_B],</span><br><span class="line">                                        [valid, valid,</span><br><span class="line">                                         imgs_A, imgs_B,</span><br><span class="line">                                         imgs_A, imgs_B])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, data_loader, run_folder, epochs, test_A_file, test_B_file, batch_size=<span class="number">1</span>, sample_interval=<span class="number">50</span></span>):</span><br><span class="line"></span><br><span class="line">    start_time = datetime.datetime.now()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Adversarial loss ground truths</span></span><br><span class="line">    valid = np.ones((batch_size,) + self.disc_patch)</span><br><span class="line">    fake = np.zeros((batch_size,) + self.disc_patch)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(self.epoch, epochs):</span><br><span class="line">        <span class="keyword">for</span> batch_i, (imgs_A, imgs_B) <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader.load_batch()):</span><br><span class="line"></span><br><span class="line">            d_loss = self.train_discriminators(imgs_A, imgs_B, valid, fake)</span><br><span class="line">            g_loss = self.train_generators(imgs_A, imgs_B, valid)</span><br><span class="line"></span><br><span class="line">            elapsed_time = datetime.datetime.now() - start_time</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Plot the progress</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %05f, adv: %05f, recon: %05f, id: %05f] time: %s "</span> \</span><br><span class="line">                  % (self.epoch, epochs,</span><br><span class="line">                     batch_i, data_loader.n_batches,</span><br><span class="line">                     d_loss[<span class="number">0</span>], <span class="number">100</span> * d_loss[<span class="number">7</span>],</span><br><span class="line">                     g_loss[<span class="number">0</span>],</span><br><span class="line">                     np.<span class="built_in">sum</span>(g_loss[<span class="number">1</span>:<span class="number">3</span>]),</span><br><span class="line">                     np.<span class="built_in">sum</span>(g_loss[<span class="number">3</span>:<span class="number">5</span>]),</span><br><span class="line">                     np.<span class="built_in">sum</span>(g_loss[<span class="number">5</span>:<span class="number">7</span>]),</span><br><span class="line">                     elapsed_time))</span><br><span class="line"></span><br><span class="line">            self.d_losses.append(d_loss)</span><br><span class="line">            self.g_losses.append(g_loss)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># If at save interval =&gt; save generated image samples</span></span><br><span class="line">            <span class="keyword">if</span> batch_i % sample_interval == <span class="number">0</span>:</span><br><span class="line">                self.sample_images(data_loader, batch_i, run_folder, test_A_file, test_B_file)</span><br><span class="line">                self.combined.save_weights(os.path.join(run_folder, <span class="string">'weights/weights-%d.h5'</span> % (self.epoch)))</span><br><span class="line">                self.combined.save_weights(os.path.join(run_folder, <span class="string">'weights/weights.h5'</span>))</span><br><span class="line">                self.save_model(run_folder)</span><br><span class="line"></span><br><span class="line">        self.epoch += <span class="number">1</span></span><br></pre></td></tr></tbody></table></figure>

<p>1.我们对真实图像使用1响应，对生成图像使用0响应。注意每个patch有一个响应，因为我们使用的是PatchGAN鉴别器。</p>
<p>2.为了训练鉴别器，我们首先使用各自的生成器创建一批假图像，然后我们在这个假集和一批真实图像上训练每个鉴别器。通常，对于CycleGAN，批处理大小为1(单个图像)。</p>
<p>3.通过前面编译的组合模型，在一个步骤中一起训练生成器。看看这六个输出如何与前面在编译期间定义的六个损失函数相匹配。</p>
<h4 id="Analysis-of-the-CycleGAN"><a href="#Analysis-of-the-CycleGAN" class="headerlink" title="Analysis of the CycleGAN"></a>Analysis of the CycleGAN</h4><p>让我们看看CycleGAN在简单数据集apples and oranges上的表现，并观察改变损失函数中的权重参数会如何对结果产生巨大影响。</p>
<p>现在您已经熟悉了CycleGAN架构，您可能会意识到这个图像代表了判断组合模型的三个标准:有效性、重建和身份。</p>
<p>让我们用代码库中的适当函数重新标记这个图像，以便更清楚地看到这一点(如图5-8所示)。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230423153642240.png" alt="image-20230423153642240"></p>
<p>我们可以看到网络的训练是成功的，因为每个生成器都明显地改变了输入图像，使其看起来更像来自相反域的有效图像。此外，当一个接一个地应用生成器时，输入图像和重建图像之间的差异最小。最后，当每个生成器应用于自己输入域的图像时，图像不会发生显著变化。</p>
<p>在最初的CycleGAN论文中，除了必要的重构损失和有效性损失外，身份损失是可选的。为了证明身份项在损失函数中的重要性，让我们通过在损失函数中设置身份损失权重参数为零，来看看如果我们去掉身份项会发生什么(图5-9)。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230423154002808.png" alt="image-20230423154002808"></p>
<p>CycleGAN仍然成功地将橘子翻译成苹果，但装橘子的托盘的颜色已经从黑色变成了白色，因为现在没有身份损失术语来防止这种背景颜色的变化。身份项有助于调节生成器，以确保它只调整完成转换所需的图像部分，而不是更多。</p>
<p>这突出了确保三个损失函数的权重很好地平衡的重要性——身份损失过少，会出现颜色偏移问题;身份丢失太多，CycleGAN没有足够的动力来改变输入，使其看起来像来自相反域的图像。</p>
<h4 id="Creating-a-CycleGAN-to-Paint-Like-Monet"><a href="#Creating-a-CycleGAN-to-Paint-Like-Monet" class="headerlink" title="Creating a CycleGAN to Paint Like Monet"></a>Creating a CycleGAN to Paint Like Monet</h4><p>现在我们已经探索了CycleGAN的基本结构，我们可以将注意力转向更有趣和令人印象深刻的技术应用。</p>
<p>在最初的CycleGAN论文中，一个突出的成就是模型能够学习如何将给定的照片转换成特定艺术家风格的绘画。由于这是一个CycleGAN，该模型也能够以另一种方式转换，将艺术家的绘画转换为逼真的照片。</p>
<p>要下载从monet到照片的数据集：</p>
<p><a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/monet2photo.zip">https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/monet2photo.zip</a></p>
<h4 id="The-Generators-ResNet"><a href="#The-Generators-ResNet" class="headerlink" title="The Generators (ResNet)"></a>The Generators (ResNet)</h4><p>在这个例子中，我们将介绍一种新型的生成器架构:残差网络(residual network，简称ResNet)。ResNet架构类似于U-Net，因为它允许来自网络中先前层的信息提前跳过一层或多层。然而，ResNet不是通过将网络的下采样部分连接到相应的上采样层来创建U形，而是由相互堆叠的残差块构建，其中每个块包含一个跳跃连接，在将其传递到下一层之前，对块的输入和输出进行求和。单个残差块如图5-10所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230423155141774.png" alt="image-20230423155141774"></p>
<p>在我们的CycleGAN中，图中的“权重层”是卷积的具有实例规范化的层。在Keras中，残差块的编码方法如例5-8所示。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.layers.merge <span class="keyword">import</span> add</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">residual</span>(<span class="params">layer_input, filters</span>):</span><br><span class="line">    shortcut = layer_input</span><br><span class="line">    y = Conv2D(filters, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), strides=<span class="number">1</span>, padding=<span class="string">'same'</span>)(layer_input)</span><br><span class="line">    y = InstanceNormalization(axis = -<span class="number">1</span>, center = <span class="literal">False</span>, scale = <span class="literal">False</span>)(y)</span><br><span class="line">    y = Activation(<span class="string">'relu'</span>)(y)</span><br><span class="line">    y = Conv2D(filters, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), strides=<span class="number">1</span>, padding=<span class="string">'same'</span>)(y)</span><br><span class="line">    y = InstanceNormalization(axis = -<span class="number">1</span>, center = <span class="literal">False</span>, scale = <span class="literal">False</span>)(y)</span><br><span class="line">    <span class="keyword">return</span> add([shortcut, y])</span><br></pre></td></tr></tbody></table></figure>

<p>在残差块的两侧，我们的ResNet生成器还包含下采样和上采样层。ResNet的总体架构如图5-11所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230423155831146.png" alt="image-20230423155831146"></p>
<p>研究表明，ResNet架构可以被训练到数百甚至数千层的深度，并且不会受到梯度消失问题的影响，其中早期层的梯度很小，因此训练非常缓慢。这是因为误差梯度可以通过作为残差块的一部分的跳跃连接在网络中自由反向传播。此外，人们相信，添加额外的层绝不会导致模型精度的下降，因为跳跃连接确保了，如果不能提取进一步的信息特征，则总是可以通过前一层的身份映射。</p>
<h4 id="Analysis-of-the-CycleGAN-1"><a href="#Analysis-of-the-CycleGAN-1" class="headerlink" title="Analysis of the CycleGAN"></a>Analysis of the CycleGAN</h4><p>在原始的CycleGAN论文中，该模型被训练了200个epoch，以实现艺术家到照片风格迁移的最先进结果。在图5-12中，我们显示了每个生成器在早期训练过程不同阶段的输出，以显示模型开始学习如何将莫奈的绘画转换为照片和反之亦然时的进展。</p>
<p>在上面一行中，我们可以看到，莫奈所使用的独特的颜色和笔触逐渐转化为照片中所期望的更自然的颜色和平滑的边缘。类似地，下面一行的情况正好相反，因为生成器学会了如何将一张照片转换成莫奈可能自己画的场景。</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="C:\Users\cheris\AppData\Roaming\Typora\typora-user-images\image-20230423160148468.png" alt="image-20230423160148468"></p>
<p>图5-13是原始论文中该模型经过200次epoch训练后得到的部分结果。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230423160303136.png" alt="image-20230423160303136"></p>
<h3 id="Neural-Style-Transfer"><a href="#Neural-Style-Transfer" class="headerlink" title="Neural Style Transfer"></a>Neural Style Transfer</h3><p>到目前为止，我们已经看到了CycleGAN如何在两个域之间转换图像，其中训练集中的图像不一定是成对的。现在我们来看看风格转移的另一种应用，我们根本没有训练集，而是希望将一张图像的风格转移到另一张图像上，如图5-14所示。这被称为神经风格转移。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230423162133259.png" alt="image-20230423162133259"></p>
<p>这个想法的前提是我们要最小化一个损失函数，该函数是三个不同部分的加权和:</p>
<p>内容损失：我们希望组合图像包含与基础图像相同的内容。</p>
<p>风格损失：我们希望组合图像与风格图像具有相同的总体风格。</p>
<p>总方差损失：我们希望组合后的图像看起来平滑而不是像素化。</p>
<p>我们通过梯度下降来最小化这种损失，也就是说，在多次迭代中，我们按与损失函数的负梯度成比例的数量更新每个像素值。这样，损失随着每次迭代逐渐减少，我们最终得到一个图像，该图像将一个图像的内容与另一个图像的风格相融合。</p>
<p> 通过梯度下降优化生成的输出与我们迄今为止解决生成模型问题的方式不同。之前，我们通过在整个网络中反向传播误差来训练一个深度神经网络，如VAE或GAN，以从训练数据集中学习，并将学到的信息泛化以生成新图像。在这里，我们不能采用这种方法，因为我们只有两个图像要处理，基础图像和样式图像。然而，正如我们将看到的，我们仍然可以使用预训练的深度神经网络来提供损失函数中每个图像的重要信息。我们将从定义三个单独的损失函数开始，因为它们是神经风格迁移引擎的核心。</p>
<h5 id="Content-Loss"><a href="#Content-Loss" class="headerlink" title="Content Loss"></a>Content Loss</h5><p> 内容损失衡量了两幅图像在主题和内容的整体位置方面的差异。两幅包含相似场景的图像(例如，一排建筑物的照片和另一幅从不同光线、不同角度拍摄的相同建筑物的照片)的损失应该小于两幅包含完全不同场景的图像。简单地比较两幅图像的像素值是不行的，因为即使是在同一场景的两幅不同的图像中，我们也不会期望单个像素值相似。我们真的不希望内容损失关心单个像素的值;我们更希望它根据高层特征(如建筑物、天空或河流)的存在和大致位置对图像进行评分。</p>
<p>我们以前见过这个概念。这是深度学习的整个前提——训练用于识别图像内容的神经网络，通过结合前一层的简单特征，自然地在网络的更深层学习更高级别的特征。因此，我们需要的是一个已经成功训练过识别图像内容的深度神经网络，这样我们就可以利用网络的深层来提取给定输入图像的高层特征。如果我们测量基础图像的输出和当前组合图像之间的均方误差，我们就有了内容损失函数!</p>
<p>我们将使用的预训练网络称为VGG19。这是一个19层的卷积神经网络，经过训练可以将ImageNet数据集中的100多万张图像分类为1000个对象类别。组网图如图5-15所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230423162835600.png" alt="image-20230423162835600"></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.applications <span class="keyword">import</span> vgg19</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br><span class="line"></span><br><span class="line">base_image_path = <span class="string">'/path/base_image.jpg'</span></span><br><span class="line">style_reference_image_path = <span class="string">'/path/style_image.jpg'</span></span><br><span class="line"></span><br><span class="line">content_weight = <span class="number">0.01</span></span><br><span class="line">base_image = K.variable(preprocess_image(base_image_path)) </span><br><span class="line">style_reference_image = K.variable(preprocess_image(style_reference_image_path))</span><br><span class="line">combination_image = K.placeholder((<span class="number">1</span>, img_nrows, img_ncols, <span class="number">3</span>))</span><br><span class="line">input_tensor = K.concatenate([base_image,</span><br><span class="line">                              style_reference_image,</span><br><span class="line">                              combination_image], axis=<span class="number">0</span>) </span><br><span class="line">model = vgg19.VGG19(input_tensor=input_tensor,</span><br><span class="line">                    weights=<span class="string">'imagenet'</span>, include_top=<span class="literal">False</span>) </span><br><span class="line">outputs_dict = <span class="built_in">dict</span>([(layer.name, layer.output) <span class="keyword">for</span> layer <span class="keyword">in</span> model.layers])</span><br><span class="line">layer_features = outputs_dict[<span class="string">'block5_conv2'</span>] </span><br><span class="line">base_image_features = layer_features[<span class="number">0</span>, :, :, :]</span><br><span class="line">combination_features = layer_features[<span class="number">2</span>, :, :, :] </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">content_loss</span>(<span class="params">content, gen</span>):</span><br><span class="line">    <span class="keyword">return</span> K.<span class="built_in">sum</span>(K.square(gen - content))</span><br><span class="line">content_loss = content_weight * content_loss(base_image_features</span><br><span class="line">                                           , combination_features)</span><br></pre></td></tr></tbody></table></figure>

<p>注意：</p>
<p>Keras库包含可以导入的预训练的VGG19模型。</p>
<p>我们定义两个Keras变量来保存基础图像和样式图像，以及一个占位符，它将包含生成的组合图像。</p>
<p>VGG19模型的输入张量是三个图像的连接。</p>
<p>这里，我们创建了VGG19模型的一个实例，指定了输入张量和我们想要预加载的权重。include_top = False参数指定我们不需要为最终用于图像分类的网络密集层加载权重。这是因为我们只对前面的卷积层感兴趣，这些层捕获输入图像的高级特征，而不是原始模型被训练为输出的实际概率。</p>
<p>我们用来计算内容损失的层是第五个块的第二层卷积层。选择网络中较浅或较深点的层会影响损失函数如何定义“内容”，因此会改变生成的组合图像的属性。在这里，我们从输入张量中提取基础图像特征和组合图像特征，这些特征已经通过VGG19网络输入。</p>
<p>内容损失是两个图像的选定层的输出之间的平方和乘以一个加权参数。</p>
<h5 id="Style-Loss"><a href="#Style-Loss" class="headerlink" title="Style Loss"></a>Style Loss</h5><p>风格损失更难量化——我们如何衡量两个图像之间的风格相似性?神经风格迁移论文中给出的解决方案是基于这样的想法:<strong>在给定层中，风格相似的图像通常具有相同的特征图之间的相关性模式</strong>。通过一个例子我们可以更清楚地看到这一点。假设在VGG19网络中，我们有一些层，其中一个通道已经学会识别图像的绿色部分，另一个通道已经学会识别尖峰，另一个已经学会识别图像的棕色部分。来自这些通道的三个输入的输出(特征图)如图5-16所示。</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230423170631065.png" alt="image-20230423170631065"></p>
<p>我们可以看到A和B在样式上很相似——都是绿草。我们可以把特征图展平，然后<strong>计算点积。如果结果值很高，则特征图高度相关;如果该值较低，则特征图不相关。</strong>我们可以定义一个矩阵，其中包含层中所有可能特征对之间的点积。这被称为Gram矩阵。图5-17展示了每个图像的三个特征的Gram矩阵。</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230423172710390.png" alt="image-20230423172710390"></p>
<p>很明显，样式相似的图像A和B在这一层具有相似的Gram矩阵。即使它们的内容可能非常不同，但Gram矩阵(度量层中所有特征对之间的相关性)是相似的。因此，为了计算风格损失，我们需要做的就是为整个网络中的一组层计算基图像和组合图像的Gram矩阵(GM)，并使用误差平方和比较它们的相似性。从代数上讲，对于大小为M(高度x宽度)、有N个通道的给定层(l)，基础图像(S)和生成图像(G)之间的风格损失可以写成:<br>$$<br>{\cal L}<em>{G M}\left(S,G,l\right)=\frac{1}{4N</em>{l}^{2}M_{l}^{2}}\sum_{i j}\left(G M\left<a href="S">l\right</a><em>{i j}-G M\left<a href="G">l\right</a></em>{i j}\right)^{2}<br>$$<br>注意它是如何根据通道数量(N)和层大小(M)进行缩放的。这是因为我们将整体风格损失计算为几个层的加权和，所有这些层都有不同的大小。总风格损失计算如下:<br>$$<br>{\cal L}<em>{s t y l e}\left(S,{\cal G}\right)=\sum</em>{l=0}^{L}w_{l}{\cal L}_{G M}\left(S,{\cal G},l\right)<br>$$</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">style_loss = <span class="number">0.0</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gram_matrix</span>(<span class="params">x</span>):</span><br><span class="line">    features = K.batch_flatten(K.permute_dimensions(x, (<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)))</span><br><span class="line">    gram = K.dot(features, K.transpose(features))</span><br><span class="line">    <span class="keyword">return</span> gram</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">style_loss</span>(<span class="params">style, combination</span>):</span><br><span class="line">    S = gram_matrix(style)</span><br><span class="line">    C = gram_matrix(combination)</span><br><span class="line">    channels = <span class="number">3</span></span><br><span class="line">    size = img_nrows * img_ncols</span><br><span class="line">    <span class="keyword">return</span> K.<span class="built_in">sum</span>(K.square(S - C)) / (<span class="number">4.0</span> * (channels ** <span class="number">2</span>) * (size ** <span class="number">2</span>))</span><br><span class="line">feature_layers = [<span class="string">'block1_conv1'</span>, <span class="string">'block2_conv1'</span>,</span><br><span class="line">                  <span class="string">'block3_conv1'</span>, <span class="string">'block4_conv1'</span>,</span><br><span class="line">                  <span class="string">'block5_conv1'</span>] </span><br><span class="line"><span class="keyword">for</span> layer_name <span class="keyword">in</span> feature_layers:</span><br><span class="line">    layer_features = outputs_dict[layer_name]</span><br><span class="line">    style_reference_features = layer_features[<span class="number">1</span>, :, :, :] </span><br><span class="line">    combination_features = layer_features[<span class="number">2</span>, :, :, :]</span><br><span class="line">    sl = style_loss(style_reference_features, combination_features)</span><br><span class="line">    style_loss += (style_weight / <span class="built_in">len</span>(feature_layers)) * sl</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 样式损失是在五个层上计算的——VGG19模型的五个块中的每一个的第一个卷积层。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在这里，我们从通过VGG19网络提供的输入张量中提取风格图像特征和组合图像特征。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 样式损失由加权参数和计算的层数来缩放</span></span><br></pre></td></tr></tbody></table></figure>



<h5 id="Total-Variance-Loss"><a href="#Total-Variance-Loss" class="headerlink" title="Total Variance Loss"></a>Total Variance Loss</h5><p>总方差损失只是组合图像中噪声的度量。为了判断图像的噪声程度，我们可以将其向右移动一个像素，然后计算平移后图像与原始图像之间的平方和。为了平衡，我们也可以执行相同的过程，但将图像向下移动一个像素。这两项的和就是总方差损失。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">total_variation_loss</span>(<span class="params">x</span>):</span><br><span class="line">    a = K.square(</span><br><span class="line">        x[:, :img_nrows - <span class="number">1</span>, :img_ncols - <span class="number">1</span>, :] - x[:, <span class="number">1</span>:, :img_ncols - <span class="number">1</span>, :]) </span><br><span class="line">    b = K.square(</span><br><span class="line">        x[:, :img_nrows - <span class="number">1</span>, :img_ncols - <span class="number">1</span>, :] - x[:, :img_nrows - <span class="number">1</span>, <span class="number">1</span>:, :]) </span><br><span class="line">    <span class="keyword">return</span> K.<span class="built_in">sum</span>(K.<span class="built_in">pow</span>(a + b, <span class="number">1.25</span>))</span><br><span class="line">tv_loss = total_variation_weight * total_variation_loss(combination_image) </span><br><span class="line"><span class="number">16</span></span><br><span class="line"></span><br><span class="line">loss = content_loss + style_loss + tv_loss  <span class="comment"># 总损失</span></span><br><span class="line"><span class="comment"># 图像和同一图像之间的平方差将向下移动一个像素。图像和同一图像之间的平方差向右移动了一个像素。总方差损失由一个加权参数缩放。总体损失是内容、风格和总方差损失的总和。</span></span><br><span class="line"></span><br><span class="line"> </span><br></pre></td></tr></tbody></table></figure>

<h5 id="Running-the-Neural-Style-Transfer"><a href="#Running-the-Neural-Style-Transfer" class="headerlink" title="Running the Neural Style Transfer"></a>Running the Neural Style Transfer</h5><p>该过程以基图像作为起始合并图像进行初始化。</p>
<p>在每次迭代时，我们将当前合并的图像(变平)传递给scipy中的优化函数fmin_l_bfgs_b优化包，根据L- BFGS-B算法执行一个梯度下降步骤。</p>
<p>在这里，evaluator是一个对象，包含计算前面描述的总体损失以及相对于输入图像的损失梯度的方法。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> fmin_l_bfgs_b</span><br><span class="line">iterations = <span class="number">1000</span></span><br><span class="line">x = preprocess_image(base_image_path) </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iterations):</span><br><span class="line">    x, min_val, info = fmin_l_bfgs_b( </span><br><span class="line">        evaluator.loss </span><br><span class="line">        , x.flatten()</span><br><span class="line">        , fprime=evaluator.grads </span><br><span class="line">        , maxfun=<span class="number">20</span></span><br><span class="line">        )</span><br></pre></td></tr></tbody></table></figure>

<h5 id="Analysis-of-the-Neural-Style-Transfer-Model"><a href="#Analysis-of-the-Neural-Style-Transfer-Model" class="headerlink" title="Analysis of the Neural Style Transfer Model"></a>Analysis of the Neural Style Transfer Model</h5><p>图5-18显示了学习过程中三个不同阶段的神经风格迁移过程输出，参数如下:</p>
<ul>
<li>content_weight: 1</li>
<li>style_weight: 100</li>
<li>total_variation_weight: 20</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230423175534545.png" alt="image-20230423175534545"></p>
<p>我们可以看到，随着每个训练步骤的进行，该算法在风格上越来越接近风格图像，并失去了基础图像的细节，同时保留了整体内容结构。有许多方法可以试验这种架构。你可以尝试改变损失函数或用于确定内容相似度的层中的权重参数，看看这是如何影响组合输出图像和训练速度的。您还可以尝试衰减风格损失函数中赋予每个层的权重，以使模型偏向于迁移更精细或更粗糙的风格特征。</p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>在本章中，我们探索了两种不同的生成新艺术品的方法:CycleGAN和神经风格迁移。</p>
<p>CycleGAN方法允许我们训练一个模型来学习艺术家的一般风格，并将其转移到照片上，以生成看起来就像艺术家画了照片中的场景一样的输出。该模型还免费为我们提供了反向过程，将绘画转换为逼真的照片。至关重要的是，来自每个域的配对图像不需要CycleGAN工作，使其成为一种极其强大和灵活的技术。</p>
<p>神经风格迁移技术允许我们将单个图像的风格迁移到基图像上，使用巧妙选择的损失函数，惩罚模型偏离基础图像的内容和风格图像的艺术风格太远，同时保持输出的平滑程度。这项技术已经被许多知名的应用程序商业化，可以将用户的照片与一组特定风格的绘画融合在一起。在下一章中，我们将从基于图像的生成建模转移到一个新的挑战领域:基于文本的生成建模。</p>
<h2 id="Chapter-6-Write"><a href="#Chapter-6-Write" class="headerlink" title="Chapter 6. Write"></a>Chapter 6. Write</h2><p>在本章中，我们将探讨在文本数据上构建生成模型的方法。文本数据和图像数据之间有几个关键的区别，这意味着许多适用于图像数据的方法并不那么容易适用于文本数据。特别是:</p>
<ul>
<li><p>文本数据由离散的块(字符或单词)组成，而图像中的像素是连续光谱中的点。我们可以很容易地让绿色像素更蓝色，但如何让单词cat更像单词dog就不是很明显了。这意味着我们可以轻松地将反向传播应用于图像数据，因为我们可以计算损失函数相对于单个像素的梯度，以确定像素颜色应改变的方向，以最小化损失。对于离散的文本数据，我们不能像通常那样应用反向传播，因此我们需要找到一种解决这个问题的方法。</p>
</li>
<li><p>文本数据有时间维度但没有空间维度，而图像数据有两个空间维度但没有时间维度。单词的顺序在文本数据中非常重要，单词倒过来是没有意义的，而图像通常可以翻转而不影响内容。此外，模型需要捕获单词之间的长期顺序依赖关系:例如，问题的答案或延续代词的上下文。对于图像数据，可以同时处理所有像素。</p>
</li>
<li><p>文本数据对单个单位(单词或字符)的微小变化非常敏感。图像数据通常对单个像素单位的变化不太敏感——即使改变了一些像素，一幅房子的图像仍然可以被识别为一所房子。然而，对于文本数据，即使改变几个单词，也可能彻底改变文章的含义，或使其变得毫无意义。这使得训练一个模型来生成连贯的文本变得非常困难，因为每个单词对文章的整体含义都至关重要。</p>
</li>
<li><p>文本数据具有基于规则的语法结构，而图像数据不遵循关于如何分配像素值的规则。例如，在任何内容中写“The cat sat on The having”都没有语法意义。还有一些语义规则很难建模;说“I am in the beach”是没有意义的，尽管从语法上讲，这句话没有任何错误。</p>
</li>
</ul>
<p> 文本建模已经取得了良好的进展，但上述问题的解决方案仍然是正在进行的研究领域。我们将从最常用和已建立的用于生成序列数据(如文本)的模型之一开始，递归神经网络(RNN)，特别是长短期记忆(LSTM)层。本章还将探索一些在问题-答案对生成领域取得有希望成果的新技术。</p>
<h3 id="Long-Short-Term-Memory-Networks"><a href="#Long-Short-Term-Memory-Networks" class="headerlink" title="Long Short-Term Memory Networks"></a>Long Short-Term Memory Networks</h3><p>LSTM网络是一种特殊类型的循环神经网络(RNN)。rnn包含一个循环层(或单元格)，它能够通过使自己在特定时间步的输出成为下一个时间步输入的一部分来处理顺序数据，以便过去的信息可以影响当前时间步的预测。我们说LSTM网络是指具有LSTM递归层的神经网络。</p>
<p>当首次引入rnn时，递归层非常简单，仅由tanh算子组成，该算子确保时间步之间传递的信息在-1和1之间缩放。然而，这被证明会受到梯度消失问题的影响，并且不能很好地扩展到长序列数据。</p>
<p>1997年，Sepp Hochreiter和Jürgen Schmidhuber在一篇论文中首次介绍了LSTM单元。在这篇论文中，作者描述了lstm如何没有普通rnn所经历的梯度消失问题，并且可以在数百个时间步长的序列上进行训练。从那时起，LSTM架构已经被调整和改进，像门控循环单元(gru)这样的变体现在被广泛使用并作为Keras中的层。让我们首先看看如何使用Keras构建一个非常简单的LSTM网络，它可以生成伊索寓言风格的文本。</p>
<h3 id="Your-First-LSTM-Network"><a href="#Your-First-LSTM-Network" class="headerlink" title="Your First LSTM Network"></a>Your First LSTM Network</h3><p>11339 aesop</p>
<p>数据集：<a target="_blank" rel="noopener" href="http://www.gutenberg.org/cache/epub/11339/pg11339.txt">http://www.gutenberg.org/cache/epub/11339/pg11339.txt</a></p>
<h4 id="Tokenization"><a href="#Tokenization" class="headerlink" title="Tokenization"></a>Tokenization</h4><p>第一步是对文本进行清理和标记化。分词(Tokenization)是将文本分割为单个单元(如单词或字符)的过程。</p>
<p>如何标记文本将取决于您试图使用文本生成模型实现的目标。同时使用单词标记和字符标记各有利弊，您的选择将影响您在建模和模型输出之前需要如何清理文本。</p>
<ul>
<li>如果使用单词标记:</li>
</ul>
<p>所有文本都可以转换为小写，以确保句子开头大写的单词与出现在句子中间的相同单词的标记化方式相同。</p>
<p>但在某些情况下，这可能不是我们想要的;例如，一些专有名词，如名称或地点，可以保持大写，以便独立标记它们。</p>
<p>文本词汇表(训练集中不同的单词的集合)可能非常大，其中一些单词出现得非常稀疏，或者可能只出现一次。将稀疏单词替换为未知单词的标记可能是明智的，而不是将它们作为单独的标记包括在内，以减少神经网络需要学习的权重数量。</p>
<p>单词可以被词根化，这意味着它们被简化为最简单的形式，这样一个动词的不同时态保持在一起。例如，browse(浏览)、browsing(浏览)、browses(浏览)和browsing(浏览)都可以用眉毛来表示。您需要将标点符号标记化，或者完全删除它。使用单词标记化意味着模型将永远无法预测训练词汇表之外的单词。</p>
<ul>
<li>如果你使用字符标记:</li>
</ul>
<p>模型可能会在训练词汇表之外生成形成新单词的字符序列——这在某些上下文中可能是可取的，但在另一些上下文中则不是。大写字母可以转换为对应的小写字母，也可以保留为单独的标记。使用字符分词时，词汇表通常要小得多。这有利于模型训练速度，因为最终输出层需要学习的权重更少。</p>
<p>在本例中，我们将使用小写单词分词，而不使用单词词干提取。我们还将标记标点符号，例如，我们希望模型能够预测何时应该结束句子或开始/结束演讲标记。最后，我们将用一个新的故事角色块(||||||||||||||||||||)替换故事之间的多个换行符。这样，当我们使用模型生成文本时，我们可以用这个字符块填充模型，这样模型就知道要从头开始一个新的故事。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.text <span class="keyword">import</span> Tokenizer</span><br><span class="line">filename = <span class="string">"./data/aesop/data.txt"</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(filename, encoding=<span class="string">'utf-8-sig'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    text = f.read()</span><br><span class="line">seq_length = <span class="number">20</span></span><br><span class="line">start_story = <span class="string">'| '</span> * seq_length</span><br><span class="line"></span><br><span class="line"><span class="comment"># CLEANUP</span></span><br><span class="line">text = text.lower()</span><br><span class="line">text = start_story + text</span><br><span class="line">text = text.replace(<span class="string">'\n\n\n\n\n'</span>, start_story)</span><br><span class="line">text = text.replace(<span class="string">'\n'</span>, <span class="string">' '</span>)</span><br><span class="line">text = re.sub(<span class="string">'  +'</span>, <span class="string">'. '</span>, text).strip()</span><br><span class="line">text = text.replace(<span class="string">'..'</span>, <span class="string">'.'</span>)</span><br><span class="line">text = re.sub(<span class="string">'([!"#$%&amp;()*+,-./:;&lt;=&gt;?@[\]^_`{|}~])'</span>, <span class="string">r' \1 '</span>, text)</span><br><span class="line">text = re.sub(<span class="string">'\s{2,}'</span>, <span class="string">' '</span>, text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TOKENIZATION</span></span><br><span class="line">tokenizer = Tokenizer(char_level = <span class="literal">False</span>, filters = <span class="string">''</span>)</span><br><span class="line">tokenizer.fit_on_texts([text])</span><br><span class="line">total_words = <span class="built_in">len</span>(tokenizer.word_index) + <span class="number">1</span></span><br><span class="line">token_list = tokenizer.texts_to_sequences([text])[<span class="number">0</span>]</span><br></pre></td></tr></tbody></table></figure>

<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230424144605303.png" alt="image-20230424144605303"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230424144629316.png" alt="image-20230424144629316"></p>
<h4 id="Building-the-Dataset"><a href="#Building-the-Dataset" class="headerlink" title="Building the Dataset"></a>Building the Dataset</h4><p>我们的LSTM网络将被训练为预测序列中的下一个单词，给定此点之前的单词序列。例如，我们可以给模型输入贪心的猫和和的标记，并期望模型输出一个合适的下一个单词(例如，dog，而不是in)。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_sequences</span>(<span class="params">token_list, step</span>):</span><br><span class="line">    X = []</span><br><span class="line">    y = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(token_list) - seq_length, step):</span><br><span class="line">        X.append(token_list[i: i + seq_length])</span><br><span class="line">        y.append(token_list[i + seq_length])</span><br><span class="line">    y = np_utils.to_categorical(y, num_classes = total_words)</span><br><span class="line">    num_seq = <span class="built_in">len</span>(X)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'Number of sequences:'</span>, num_seq, <span class="string">"\n"</span>)</span><br><span class="line">    <span class="keyword">return</span> X, y, num_seq</span><br><span class="line">step = <span class="number">1</span></span><br><span class="line">seq_length = <span class="number">20</span></span><br><span class="line">X, y, num_seq = generate_sequences(token_list, step)</span><br><span class="line">X = np.array(X)</span><br><span class="line">y = np.array(y)</span><br></pre></td></tr></tbody></table></figure>

<h4 id="The-LSTM-Architecture"><a href="#The-LSTM-Architecture" class="headerlink" title="The LSTM Architecture"></a>The LSTM Architecture</h4><p>整个模型的架构如图6-3所示。模型的输入是一个整数标记序列，输出是词汇表中每个单词在下一个序列中出现的概率。为了详细理解它是如何工作的，我们需要引入两种新的层类型，嵌入和LSTM。</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230424153701138.png" alt="image-20230424153701138"></p>
<h4 id="The-Embedding-Layer"><a href="#The-Embedding-Layer" class="headerlink" title="The Embedding Layer"></a>The Embedding Layer</h4><p>嵌入层本质上是一个查找表，它将每个标记转换为长度为embedding_size的向量(图6-4)。因此，这一层学习到的权重数量等于词汇表的大小乘以embedding_size。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230424154117205.png" alt="image-20230424154117205"></p>
<p>输入层将形状为[batch_size, seq_length]的整数序列张量传递到嵌入层，嵌入层输出形状为[batch_size, seq_length, embedding_size]的张量。然后将其传递给LSTM层(图6-5)。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230424154234610.png" alt="image-20230424154234610"></p>
<p>我们将每个整数标记嵌入到一个连续向量中，因为它使模型能够学习每个单词的表示，并能够通过反向传播进行更新。我们也可以对每个输入标记进行独热编码，但首选使用嵌入层，因为它使嵌入本身可训练，从而使模型在决定如何嵌入每个标记以提高模型性能方面具有更大的灵活性。</p>
<h4 id="The-LSTM-Layer"><a href="#The-LSTM-Layer" class="headerlink" title="The LSTM Layer"></a>The LSTM Layer</h4><p>要理解LSTM层，我们必须首先了解一般的循环层是如何工作的。</p>
<p>循环层具有能够处理顺序输入数据$$[x_1,…,x_n]$$的特殊属性。它由一个单元格组成，当序列x的每个元素通过它时，它更新其隐藏状态$$h_t$$，每次一个时间步。隐藏状态是一个长度等于单元格中单元数量的向量，它可以被认为是单元格当前对序列的理解。在时间步t，单元格使用隐藏状态$$h_{t-1}$$的前一个值和当前时间步$$x_t$$的数据来生成更新的隐藏状态向量$$h_t$$。这个循环过程一直持续到序列的末尾。</p>
<p>一旦序列完成，该层输出单元的最终隐藏状态$$h_n$$，然后传递到网络的下一层。这个过程如图6-6所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230424155154532.png" alt="image-20230424155154532"></p>
<p>为了更详细地解释这一点，让我们展开这个过程，以便我们可以确切地看到单个序列是如何穿过该层的(图6-7)。</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230424155759670.png" alt="image-20230424155759670"></p>
<p>在这里，我们通过在每个时间步长绘制单元的副本来表示循环过程，并显示隐藏状态在流过单元时如何不断更新。我们可以清楚地看到前一个隐藏状态是如何与当前序列数据点(即当前嵌入的词向量)混合以产生下一个隐藏状态的。在输入序列中的每个单词被处理后，该层的输出是单元的最终隐藏状态。重要的是要记住，此图中的所有单元格共享相同的权重(因为它们实际上是同一个单元格)。这个图和图6-6没有什么区别。这只是绘制循环层机制的另一种方式。</p>
<h4 id="The-LSTM-Cell"><a href="#The-LSTM-Cell" class="headerlink" title="The LSTM Cell"></a>The LSTM Cell</h4><p>现在我们已经看到了通用循环层的工作原理，让我们看看单个LSTM单元的内部。</p>
<p>单元的工作是输出一个新的隐藏状态$$h_t$$，给定它之前的隐藏状态$$h_{t-1}$$和当前的词嵌入$$x_t$$。总结一下，$$h_t$$的长度等于LSTM中的单元数。这是一个在定义层时设置的参数，与序列的长度无关。确保你没有混淆术语cell和unit。LSTM层中有一个单元，由它包含的单元数量定义，就像我们之前故事中的囚犯单元包含许多囚犯一样。我们经常将循环层绘制为展开的单元链，因为它有助于可视化隐藏状态在每个时间步是如何更新的。</p>
<p>一个LSTM细胞维护一个细胞状态$$C_t$$，这可以被认为是细胞关于序列当前状态的内部信念。这与隐藏状态$$h_t$$不同，$$h_t$$在最后一个时间步长之后最终由单元输出。单元状态的长度与隐藏状态相同(单元中的单元数)。</p>
<p>让我们更仔细地看一下单个单元格，以及隐藏状态是如何更新的(如图6-8所示)。</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230424162101787.png" alt="image-20230424162101787"></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, LSTM, Input, Embedding, Dropout</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> RMSprop</span><br><span class="line"></span><br><span class="line">n_units = <span class="number">256</span></span><br><span class="line">embedding_size = <span class="number">100</span></span><br><span class="line">text_in = Input(shape = (<span class="literal">None</span>,))</span><br><span class="line">x = Embedding(total_words, embedding_size)(text_in)</span><br><span class="line">x = LSTM(n_units)(x)</span><br><span class="line"></span><br><span class="line">x = Dropout(<span class="number">0.2</span>)(x)</span><br><span class="line">text_out = Dense(total_words, activation = <span class="string">'softmax'</span>)(x)</span><br><span class="line">model = Model(text_in, text_out)</span><br><span class="line">opti = RMSprop(lr = <span class="number">0.001</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=opti)</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">model.fit(X, y, epochs=epochs, batch_size=batch_size, shuffle = <span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="Generating-New-Text"><a href="#Generating-New-Text" class="headerlink" title="Generating New Text"></a>Generating New Text</h3><p>现在我们已经编译和训练了LSTM网络，我们可以通过应用以下过程开始使用它来生成长字符串文本:</p>
<ol>
<li><p>向网络提供现有的单词序列，并要求它预测接下来的单词。</p>
</li>
<li><p>将这个单词添加到现有序列中并重复。网络将输出我们可以从中采样的每个单词的概率集合。因此，我们可以使文本的生成是随机的，而不是确定性的。此外，我们可以在采样过程中引入一个温度参数，以表明我们希望该过程具有多大的确定性。</p>
</li>
</ol>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sample_with_temp</span>(<span class="params">preds, temperature=<span class="number">1.0</span></span>): </span><br><span class="line">    <span class="comment"># helper function to sample an index from a probability array</span></span><br><span class="line">    preds = np.asarray(preds).astype(<span class="string">'float64'</span>)</span><br><span class="line">    preds = np.log(preds) / temperature</span><br><span class="line">    exp_preds = np.exp(preds)</span><br><span class="line">    preds = exp_preds / np.<span class="built_in">sum</span>(exp_preds)</span><br><span class="line">    probs = np.random.multinomial(<span class="number">1</span>, preds, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> np.argmax(probs)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_text</span>(<span class="params">seed_text, next_words, model, max_sequence_len, temp</span>):</span><br><span class="line">    output_text = seed_text</span><br><span class="line"></span><br><span class="line">    seed_text = start_story + seed_text </span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(next_words):</span><br><span class="line">        token_list = tokenizer.texts_to_sequences([seed_text])[<span class="number">0</span>] </span><br><span class="line">        token_list = token_list[-max_sequence_len:] </span><br><span class="line">        token_list = np.reshape(token_list, (<span class="number">1</span>, max_sequence_len))</span><br><span class="line">        probs = model.predict(token_list, verbose=<span class="number">0</span>)[<span class="number">0</span>] </span><br><span class="line">        y_class = sample_with_temp(probs, temperature = temp) </span><br><span class="line">        output_word = tokenizer.index_word[y_class] <span class="keyword">if</span> y_class &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="string">''</span></span><br><span class="line">        <span class="keyword">if</span> output_word == <span class="string">"|"</span>: </span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        seed_text += output_word + <span class="string">' '</span> </span><br><span class="line">        output_text += output_word + <span class="string">' '</span></span><br><span class="line">    <span class="keyword">return</span> output_text</span><br></pre></td></tr></tbody></table></figure>

<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230424171249034.png" alt="image-20230424171249034"></p>
<p>关于这两篇文章有几点需要注意。首先，两者在风格上类似于原始训练集中的寓言。它们都以熟悉的故事中人物的陈述开始，通常带有言语标记的文本更像对话，使用人称代词，由所说的单词的出现而准备。</p>
<p>其次，与温度= 1.0时生成的文本相比，在温度= 0.2时生成的文本不那么冒险，但在选择单词方面更连贯，因为较低的温度值导致更确定性的采样。最后，很明显，这两种方法都不能很好地跨越多个句子，因为LSTM网络无法掌握它生成的单词的语义。为了生成语义合理的可能性更大的段落，我们可以构建一个人工辅助的文本生成器，其中模型输出概率最高的10个单词，然后最终由人类从这个列表中选择下一个单词。这类似于手机上的预测文本，你可以从已经输入的单词中选择几个单词。为了证明这一点，图6-10显示了具有最高概率的前10个单词遵循各种序列(不是来自训练集)。</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230424171433039.png" alt="image-20230424171433039"></p>
<p>该模型能够在一系列上下文中为下一个最有可能的单词生成合适的分布。例如，即使模型从未被告知词性，如名词、动词、形容词和介词，但它通常能够将单词分成这些类别，并以语法正确的方式使用它们。它还可以猜测，以鹰为故事开头的文章更可能是an，而不是a。图6-10中的标点符号示例表明，模型对输入序列的细微变化也很敏感。在第一段中(狮子说，)，模型以98%的可能性猜测语音标记紧随其后，因此从句在口语对话之前。然而，如果我们输入下一个单词作为and，它能够理解，现在不太可能有词性标记，因为从句更有可能取代对话，句子更有可能继续作为描述性散文。</p>
<h3 id="RNN-Extensions"><a href="#RNN-Extensions" class="headerlink" title="RNN Extensions"></a>RNN Extensions</h3><p>上一节中的网络是一个简单的例子，展示了如何训练LSTM网络以学习如何生成给定样式的文本。在本节中，我们将探讨对这一思想的几种扩展。</p>
<h4 id="Stacked-Recurrent-Networks"><a href="#Stacked-Recurrent-Networks" class="headerlink" title="Stacked Recurrent Networks"></a>Stacked Recurrent Networks</h4><p>我们刚刚看到的网络包含单个LSTM层，但我们也可以训练具有堆叠LSTM层的网络，这样就可以从文本中学习到更深的特征。</p>
<p>为了实现这一点，我们将第一个LSTM层中的return_sequences参数设置为True。这使得层从每个时间步输出隐藏状态，而不仅仅是最后一个时间步。然后，第二个LSTM层可以使用第一层的隐藏状态作为其输入数据。如图6-11所示，整体模型架构如图6-12所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230425091603960.png" alt="image-20230425091603960"></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">text_in = Input(shape = (<span class="literal">None</span>,))</span><br><span class="line">embedding = Embedding(total_words, embedding_size)</span><br><span class="line">x = embedding(text_in)</span><br><span class="line">x = LSTM(n_units, return_sequences = <span class="literal">True</span>)(x)</span><br><span class="line">x = LSTM(n_units)(x)</span><br><span class="line">x = Dropout(<span class="number">0.2</span>)(x)</span><br><span class="line">text_out = Dense(total_words, activation = <span class="string">'softmax'</span>)(x)</span><br><span class="line">model = Model(text_in, text_out)</span><br></pre></td></tr></tbody></table></figure>

<h4 id="Gated-Recurrent-Units"><a href="#Gated-Recurrent-Units" class="headerlink" title="Gated Recurrent Units"></a>Gated Recurrent Units</h4><p>另一种常用的RNN层是门控循环单元(GRU)。与LSTM单元的关键区别如下:</p>
<ol>
<li><p>遗忘门和输入门被重置门和更新门取代。</p>
</li>
<li><p>没有单元状态或输出门，只有从单元输出的隐藏状态。隐藏状态分4步更新，如图6-13所示。</p>
</li>
</ol>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230425092121683.png" alt="image-20230425092121683"></p>
<ul>
<li>前一个时间步的隐藏状态$$h_{t-1}$$和当前的词嵌入状态$$x_t$$被连接起来，用于创建重置门。该门是一个密集层，具有权重矩阵$$W_r$$和sigmoid激活函数。结果向量$$r_t$$的长度等于单元格中单元的数量，并存储在0到1之间的值，这些值决定了前一个隐藏状态$$h_{t-1}$$的多少信息应该被带入计算单元格的新信念。</li>
<li>重置门应用于隐藏状态$$h_{t-1}$$，并与当前的词嵌入$$x_t$$连接。然后，该向量被馈送到具有权重矩阵W和tanh激活函数的密集层，以生成一个向量$$\tilde{h}_t$$，该向量存储细胞的新信念。它的长度等于单元格中的单元数，并存储在-1到1之间的值。</li>
<li>前一个时间步的隐藏状态$$h_{t-1}$$和当前词嵌入$$x_t$$的连接也用于创建更新门。该门是一个密集层，具有权重矩阵$$W_z$$和sigmoid激活。所得向量z的长度等于单元格中单元的数目，存储在0和1之间的值，用于确定有多少新的深信$$\tilde{h}<em>t$$融合到当前的隐藏状态$$h</em>{t-1}$$中。</li>
<li>细胞$$\tilde{h}<em>t$$的新信念与当前隐藏状态$$h</em>{t-1}$$按更新门$$z_t$$确定的比例混合，以产生更新后的隐藏状态$$h_t$$，该状态由细胞输出。</li>
</ul>
<h4 id="Bidirectional-Cells"><a href="#Bidirectional-Cells" class="headerlink" title="Bidirectional Cells"></a>Bidirectional Cells</h4><p>对于在推理时模型可以获得整个文本的预测问题，没有理由只向前处理序列——它可以向后处理。双向层通过存储两组隐藏状态来利用这一点:一组是在通常的正向处理序列时产生的，另一组是在向后处理序列时产生的。这样，该层可以从之前的和给定时间步后的信息中学习。</p>
<p>在Keras中，这是作为循环层的包装实现的，如下所示:</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">layer = Bidirectional(GRU(100))</span><br></pre></td></tr></tbody></table></figure>

<p>结果层中的隐藏状态是长度等于被包装单元中单元数量的两倍的向量(连接前向和后向隐藏状态)。因此，在这个例子中，层的隐藏状态是长度为200的向量。</p>
<h4 id="Encoder–Decoder-Models"><a href="#Encoder–Decoder-Models" class="headerlink" title="Encoder–Decoder Models"></a>Encoder–Decoder Models</h4><p>到目前为止，我们已经研究了使用LSTM网络来生成现有文本序列的延续。我们已经看到单个LSTM层如何顺序处理数据，以更新表示该层当前对序列的理解的隐藏状态。通过将最终的隐藏状态连接到密集层，网络可以输出下一个单词的概率分布。</p>
<p>对于某些任务，目标不是预测现有序列中的单个下一个单词;相反，我们希望预测一个完全不同的单词序列，该序列在某种程度上与输入序列相关。这种类型的任务的一些例子有:</p>
<ul>
<li>语言翻译网络：接收源语言的文本字符串，目标是输出翻译成目标语言的文本。</li>
<li>问题生成网络：接收一段文本，目标是生成一个可以针对文本提出的可行问题。</li>
<li>文本摘要网络：接收一段很长的文本，目标是对该文本进行简短的摘要。</li>
</ul>
<p>对于这类问题，我们可以使用一种称为编码器-解码器的网络。我们已经在图像生成的背景下看到了一种编码器-解码器网络:变分自动编码器。对于顺序数据，编码器-解码器过程如下所示。</p>
<ul>
<li>编码器RNN将原始输入序列汇总为单个向量。</li>
<li>这个向量用于初始化RNN解码器。</li>
<li>解码器RNN在每个时间步的隐藏状态连接到一个密集层，该层输出单词词汇表的概率分布。</li>
</ul>
<p>这样，解码器可以生成一个新的文本序列，它已经用编码器产生的输入数据的表示进行了初始化。这个过程如图6-14所示，以英语和德语之间的翻译为例。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230425101945430.png" alt="image-20230425101945430"></p>
<p>编码器的最终隐藏状态可以被认为是整个输入文档的表示。然后，解码器将这种表示转换为顺序输出，例如将文本翻译成另一种语言，或与文档有关的问题。</p>
<p>在训练过程中，将解码器在每个时间步产生的输出分布与真实的下一个单词进行比较，以计算损失。在训练过程中，解码器不需要从这些分布中采样来生成单词，因为后续的单元被提供的是下一个单词的真实值，而不是从前一个输出分布中采样的单词。这种训练编码器-解码器网络的方式被称为教师强迫。我们可以想象，网络是一个学生，有时会做出错误的分布预测，但无论网络在每个时间步长输出什么，老师都提供正确的响应作为网络尝试下一个单词的输入。</p>
<h3 id="A-Question-and-Answer-Generator"><a href="#A-Question-and-Answer-Generator" class="headerlink" title="A Question and Answer Generator"></a>A Question and Answer Generator</h3><p>现在，我们将把所有内容放在一起，构建一个可以从文本块中生成问题和答案对的模型。这个项目的灵感来自qgen-workshop的TensorFlow代码库和Tong Wang, Xingdi Yuan和Adam Trischler提出的模型。该模型由两部分组成:</p>
<ul>
<li><p>RNN从文本块中识别候选答案</p>
</li>
<li><p>编码器-解码器网络，在RNN突出显示的候选答案之一的情况下，生成合适的问题。</p>
</li>
</ul>
<p>例如，考虑以下关于足球比赛的文本段落的开头:</p>
<figure class="highlight tex"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">The winning goal was scored by 23-year-old striker Joe Bloggs during the match</span><br><span class="line">between Arsenal and Barcelona .</span><br><span class="line">Arsenal recently signed the striker for 50 million pounds . The next match is in</span><br><span class="line">two weeks time, on July 31st 2005 . "</span><br></pre></td></tr></tbody></table></figure>

<p>我们希望我们的第一个网络能够识别潜在的答案，例如:</p>
<figure class="highlight tex"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">"Joe Bloggs"</span><br><span class="line">"Arsenal"</span><br><span class="line">"Barcelona"</span><br><span class="line">"50 million pounds"</span><br><span class="line">"July 31st 2005"</span><br></pre></td></tr></tbody></table></figure>

<p>我们的第二个网络应该能够在给定每个答案的情况下生成一个问题，例如:</p>
<figure class="highlight tex"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">"Who scored the winning goal?"</span><br><span class="line">"Who won the match?"</span><br><span class="line">"Who were Arsenal playing?"</span><br><span class="line">"How much did the striker cost?"</span><br><span class="line">"When is the next match?"</span><br></pre></td></tr></tbody></table></figure>

<p>让我们首先看一下我们将更详细地使用的数据集。</p>
<h4 id="A-Question-Answer-Dataset"><a href="#A-Question-Answer-Dataset" class="headerlink" title="A Question-Answer Dataset"></a>A Question-Answer Dataset</h4><p>我们将使用Maluuba NewsQA数据集，你可以按照GitHub上的说明下载它。得到的train.csv、test.csv和dev.csv文件应该放在图书仓库的./data/qa/文件夹中。这些文件都具有相同的列结构，如下所示:</p>
<ul>
<li><p>story_id：故事的唯一标识符</p>
</li>
<li><p>story_text：(例如，“制胜球是由23岁的前锋乔·布洛格斯在比赛中打进的……”)。</p>
</li>
<li><p>question：(例如，“前锋花了多少钱?”)。</p>
</li>
<li><p>answer_token_ranges表示答案在故事文本中的标记位置(例如，24:27)。如果答案在文章中出现多次，可能会有多个区间(用逗号分隔)。</p>
</li>
</ul>
<p>这些原始数据经过处理并标记化，以便能够将其用作我们模型的输入。经过这种转换后，训练集中的每个观察值由以下五个特征组成:</p>
<ul>
<li>document_tokens分词后的故事文本(例如，[1,4633,7,66,11，…])，用零剪裁/填充，长度为max_document_length(一个参数)。</li>
<li>question_input_tokens分词后的问题(例如，[2,39,1,52，…])，填充0，长度为max_question_length(另一个参数)。</li>
<li>question_output_tokens分词后的问题，偏移一个时间步长(例如，[39,1,52,1866，…]，用零填充，长度为max_question_length。</li>
<li>answer_mask二进制掩码矩阵，形状为[max_answer_length, max_document_length]。如果问题的答案的第i个单词位于文档的第j个单词，则矩阵的[i, j]值为1，否则为0。</li>
<li>answer_labels长度为max_document_length的二进制向量(例如[0,1,1,0，…])。如果文档中的第i个单词被认为是答案的一部分，则向量的第i个元素为1，否则为0。</li>
</ul>
<p>现在让我们看一下能够从给定的文本块中生成问题-答案对的模型架构。</p>
<h4 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h4><p>图6-15展示了我们将要构建的整体模型架构。如果这看起来很吓人，不要担心!它只是由我们已经见过的元素构建而成，我们将在本节中一步一步地介绍这个架构。</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230425145311499.png" alt="image-20230425145311499"></p>
<p>让我们首先看一下构建图顶部模型部分的Keras代码，它预测文档中的每个单词是否是答案的一部分。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Embedding, GRU, Bidirectional, Dense, Lambda</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model, load_model</span><br><span class="line"><span class="keyword">import</span> keras.backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> qgen.embedding <span class="keyword">import</span> glove</span><br><span class="line"><span class="comment">#### PARAMETERS ####</span></span><br><span class="line">VOCAB_SIZE = glove.shape[<span class="number">0</span>] <span class="comment"># 9984</span></span><br><span class="line">EMBEDDING_DIMENS = glove.shape[<span class="number">1</span>] <span class="comment"># 100</span></span><br><span class="line">GRU_UNITS = <span class="number">100</span></span><br><span class="line">DOC_SIZE = <span class="literal">None</span></span><br><span class="line">ANSWER_SIZE = <span class="literal">None</span></span><br><span class="line">Q_SIZE = <span class="literal">None</span></span><br><span class="line">document_tokens = Input(shape=(DOC_SIZE,), name=<span class="string">"document_tokens"</span>) </span><br><span class="line">embedding = Embedding(input_dim = VOCAB_SIZE, output_dim = EMBEDDING_DIMENS</span><br><span class="line"> 	, weights=[glove], mask_zero = <span class="literal">True</span>, name = <span class="string">'embedding'</span>) </span><br><span class="line">document_emb = embedding(document_tokens)</span><br><span class="line">answer_outputs = Bidirectional(GRU(GRU_UNITS, return_sequences=<span class="literal">True</span>)</span><br><span class="line"> 	, name = <span class="string">'answer_outputs'</span>)(document__emb) </span><br><span class="line">answer_tags = Dense(<span class="number">2</span>, activation = <span class="string">'softmax'</span></span><br><span class="line">    , name = <span class="string">'answer_tags'</span>)(answer_outputs)</span><br></pre></td></tr></tbody></table></figure>

<p>注意：</p>
<ul>
<li>文档令牌作为模型的输入。这里，我们使用变量DOC_SIZE来描述输入的大小，但该变量实际上被设置为None。这是因为模型的架构不依赖于输入序列的长度——层中的单元格数量将自适应等于输入序列的长度，因此我们不需要显式指定它。</li>
<li>嵌入层用GloVe词向量初始化(在下面的侧边栏中解释)。</li>
<li>循环层是一个双向的GRU，它在每个时间步返回隐藏状态。</li>
<li>输出密集层在每个时间步与隐藏状态连接，只有两个单元，具有softmax激活，表示每个单词是答案一部分(1)或不是答案一部分(0)的概率。</li>
</ul>
<h5 id="GLOVE-WORD-VECTORS"><a href="#GLOVE-WORD-VECTORS" class="headerlink" title="GLOVE WORD VECTORS"></a>GLOVE WORD VECTORS</h5><p>嵌入层是用一组预训练的词嵌入初始化的，而不是我们之前看到的随机向量。这些词向量是Stanford GloVe(“全局向量”)项目的一部分，该项目使用无监督学习来获取大量单词的代表向量。</p>
<p>这些向量具有许多有益的特性，如连接词之间的向量相似度。例如，单词man和woman的嵌入向量与单词king和queen之间的向量大致相同。这就好像单词的性别被编码到单词向量存在的潜在空间中。用GloVe初始化嵌入层通常比从头开始训练要好，因为捕获单词表示的大量艰苦工作已经通过GloVe训练过程实现了。然后，您的算法可以调整单词嵌入以适应您的机器学习问题的特定上下文。</p>
<p>为了在这个项目中使用GloVe词向量，请从GloVe项目网站下载文件GloVe . 6b .100d.txt(60亿个单词，每个单词的嵌入长度为100)，然后从图书库中运行以下Python脚本，修剪该文件，使其只包含训练语料库中存在的单词:</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python ./utils/write.py</span><br></pre></td></tr></tbody></table></figure>



<p>模型的第二部分是编码器-解码器网络，它接受给定的答案，并尝试制定匹配的问题(图6-15的底部部分)。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">encoder_input_mask = Input(shape=(ANSWER_SIZE, DOC_SIZE)</span><br><span class="line"> 	, name=<span class="string">"encoder_input_mask"</span>)</span><br><span class="line">encoder_inputs = Lambda(<span class="keyword">lambda</span> x: K.batch_dot(x[<span class="number">0</span>], x[<span class="number">1</span>])</span><br><span class="line"> 	, name=<span class="string">"encoder_inputs"</span>)([encoder_input_mask, answer_outputs])</span><br><span class="line">encoder_cell = GRU(<span class="number">2</span> * GRU_UNITS, name = <span class="string">'encoder_cell'</span>)(encoder_inputs) </span><br><span class="line">decoder_inputs = Input(shape=(Q_SIZE,), name=<span class="string">"decoder_inputs"</span>) </span><br><span class="line">decoder_emb = embedding(decoder_inputs) </span><br><span class="line">decoder_emb.trainable = <span class="literal">False</span></span><br><span class="line">decoder_cell = GRU(<span class="number">2</span> * GRU_UNITS, return_sequences = <span class="literal">True</span>, name = <span class="string">'decoder_cell'</span>)</span><br><span class="line">decoder_states = decoder_cell(decoder_emb, initial_state = [encoder_cell]) </span><br><span class="line">decoder_projection = Dense(VOCAB_SIZE, name = <span class="string">'decoder_projection'</span></span><br><span class="line"> 	, activation = <span class="string">'softmax'</span>, use_bias = <span class="literal">False</span>)</span><br><span class="line">decoder_outputs = decoder_projection(decoder_states) </span><br><span class="line">total_model = Model([document_tokens, decoder_inputs, encoder_input_mask]</span><br><span class="line"> 	, [answer_tags, decoder_outputs])</span><br><span class="line">answer_model = Model(document_tokens, [answer_tags])</span><br><span class="line">decoder_initial_state_model = Model([document_tokens, encoder_input_mask]</span><br><span class="line"> 	, [encoder_cell])</span><br></pre></td></tr></tbody></table></figure>

<p>注意：</p>
<ul>
<li>答案掩码作为输入传递给模型，这允许我们将隐藏状态从单个答案范围传递到编码器-解码器。这是通过Lambda层实现的。</li>
<li>编码器是一个GRU层，它将给定答案范围的隐藏状态作为输入数据。</li>
<li>解码器的输入数据是与给定答案范围匹配的问题。</li>
<li>问题词标记通过与答案识别模型相同的嵌入层传递。</li>
<li>解码器是一个GRU层，使用编码器的最终隐藏状态进行初始化。</li>
<li>解码器的隐藏状态通过一个密集层传递，以生成序列中下一个单词的整个词汇表的分布。</li>
</ul>
<p>这就完成了我们的问题-答案对生成网络。为了训练网络，我们分批传递文档文本、问题文本和答案掩码作为输入数据，并最小化答案位置预测和问题词生成的交叉熵损失，平均加权。</p>
<h4 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h4><p>为了在一个它从未见过的输入文档序列上测试模型，我们需要运行以下过程:</p>
<ul>
<li>将文档字符串提供给答案生成器，以生成文档中答案的示例位置。</li>
<li>选择这些答案块中的一个，将其传递到编码器-解码器问题生成器(即，创建适当的答案掩码)。</li>
<li>将文档和答案掩码提供给编码器，以生成解码器的初始状态。</li>
<li>用这个初始状态初始化解码器，并将&lt;START&gt;Token来生成问题的第一个单词。继续这个过程，一个一个输入生成的单词，直到&lt;END&gt;Token由模型预测。</li>
</ul>
<p>如前所述，在训练过程中，模型使用教师强迫将基本真实单词(而不是预测的下一个单词)输入到解码器单元。然而，在推理过程中，模型必须自己生成一个问题，因此我们希望能够将预测的单词反馈给解码器单元，同时保留其隐藏状态。</p>
<p>实现这一目标的一种方法是定义一个额外的Keras模型(question_model)，该模型接受当前单词标记和当前解码器隐藏状态作为输入，并输出预测的下一个单词分布和更新的解码器隐藏状态。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">decoder_inputs_dynamic = Input(shape=(<span class="number">1</span>,), name=<span class="string">"decoder_inputs_dynamic"</span>)</span><br><span class="line">decoder_emb_dynamic = embedding(decoder_inputs_dynamic)</span><br><span class="line">decoder_init_state_dynamic = Input(shape=(<span class="number">2</span> * GRU_UNITS,)</span><br><span class="line"> 	, name = <span class="string">'decoder_init_state_dynamic'</span>)</span><br><span class="line">decoder_states_dynamic = decoder_cell(decoder_emb_dynamic</span><br><span class="line"> 	, initial_state = [decoder_init_state_dynamic])</span><br><span class="line">decoder_outputs_dynamic = decoder_projection(decoder_states_dynamic)</span><br><span class="line"></span><br><span class="line">question_model = Model([decoder_inputs_dynamic, decoder_init_state_dynamic]</span><br><span class="line">    , [decoder_outputs_dynamic, decoder_states_dynamic])</span><br></pre></td></tr></tbody></table></figure>

<p>然后，我们可以在循环中使用该模型，逐字生成输出问题。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">test_data_gen = test_data()</span><br><span class="line">batch = <span class="built_in">next</span>(test_data_gen)</span><br><span class="line">answer_preds = answer_model.predict(batch[<span class="string">"document_tokens"</span>])</span><br><span class="line">idx = <span class="number">0</span></span><br><span class="line">start_answer = <span class="number">37</span></span><br><span class="line">end_answer = <span class="number">39</span></span><br><span class="line">answers = [[<span class="number">0</span>] * <span class="built_in">len</span>(answer_preds[idx])]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(start_answer, end_answer + <span class="number">1</span>):</span><br><span class="line">    answers[idx][i] = <span class="number">1</span></span><br><span class="line">answer_batch = expand_answers(batch, answers)</span><br><span class="line">next_decoder_init_state = decoder_initial_state_model.predict(</span><br><span class="line"> 	[answer_batch[<span class="string">'document_tokens'</span>][[idx]], answer_batch[<span class="string">'answer_masks'</span>][[idx]]])</span><br><span class="line">word_tokens = [START_TOKEN]</span><br><span class="line">questions = [look_up_token(START_TOKEN)]</span><br><span class="line">ended = <span class="literal">False</span></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">not</span> ended:</span><br><span class="line">    word_preds, next_decoder_init_state = question_model.predict(</span><br><span class="line"> 															[word_tokens, next_decoder_init_state])</span><br><span class="line">    next_decoder_init_state = np.squeeze(next_decoder_init_state, axis = <span class="number">1</span>)</span><br><span class="line">    word_tokens = np.argmax(word_preds, <span class="number">2</span>)[<span class="number">0</span>]</span><br><span class="line">    questions.append(look_up_token(word_tokens[<span class="number">0</span>]))</span><br><span class="line">    <span class="keyword">if</span> word_tokens[<span class="number">0</span>] == END_TOKEN:</span><br><span class="line">        ended = <span class="literal">True</span></span><br><span class="line">questions = <span class="string">' '</span>.join(questions)</span><br></pre></td></tr></tbody></table></figure>

<h4 id="Model-Results"><a href="#Model-Results" class="headerlink" title="Model Results"></a>Model Results</h4><p>模型的示例结果如图6-16所示。根据模型，右边的图表显示了文档中每个单词构成答案一部分的概率。然后将这些答案短语提供给问题生成器，该模型的输出显示在图的左侧(“预测问题”)。</p>
<p>首先，请注意答案生成器如何能够准确识别文档中哪些单词最有可能包含在答案中。这已经相当令人印象深刻了，因为它以前从未见过此文本，也可能从未见过文档中包含在答案中的某些单词，例如Bloggs。它能够从上下文中理解这很可能是一个人的姓氏，因此很可能构成答案的一部分。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230425154320301.png" alt="image-20230425154320301"></p>
<p>编码器从每个可能的答案中提取上下文，以便解码器能够生成合适的问题。值得注意的是，编码器能够捕捉到第一个答案中提到的人，23岁的前锋乔·布洛格斯，可能有一个与他的进球能力相关的匹配问题，并能够将此上下文传递给解码器，从而生成问题“谁得分了&lt;UNK&gt;?，而不是，例如，“谁是总统?”解码器已经用标签&lt;UNK&gt;完成了这个问题，但不是因为它不知道接下来要做什么——它预测接下来的单词很可能来自核心词汇表之外。</p>
<p>我们不应该惊讶于模型诉诸于使用标签&lt;UNK&gt;在这种情况下，由于原始语料库中的许多小众词将被标记为这种方式。我们可以看到，在每种情况下，解码器都根据答案的类型选择了正确的问题“类型”——谁、多少钱、什么时候问。但是仍然有一些问题，例如问他损失了多少钱?而不是为这名前锋支付了多少钱?这是可以理解的，因为解码器只有最终的编码器状态，而不能引用原始文档以获取额外的信息。</p>
<p>有一些对编码器-解码器网络的扩展，可以提高模型的准确性和生成能力。其中使用最广泛的两种是pointer networks和attention mechanisms。pointer networks使模型能够”指向”输入文本中的特定单词，以包括在生成的问题中，而不仅仅依赖于已知词汇表中的单词。这有助于解决&lt;UNK&gt;前面提到的问题。我们将在下一章中详细探讨注意力机制。</p>
<h3 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h3><p>在本章中，我们看到了如何应用循环神经网络来生成模仿特定写作风格的文本序列，以及如何从给定的文档中生成合理的问题-答案对。</p>
<p>我们探索了两种不同类型的循环层，长短期记忆(long short-term memory, lstm)和GRU，并了解了这些单元如何堆叠或双向形成更复杂的网络架构。本章介绍的编码器-解码器架构是一个重要的生成工具，因为它允许序列数据被压缩为单个向量，然后可以解码为另一个序列。这适用于除问答对生成外的一系列问题，如翻译和文本摘要。</p>
<p>在这两种情况下，我们都看到了如何将非结构化的文本数据转换为可与循环神经网络层一起使用的结构化格式的重要性。很好地理解张量的形状在数据流经网络时如何变化，也是构建成功网络的关键，而递归层中需要特别注意顺序数据的时间维度，因为转换过程增加了额外的复杂性。在下一章中，我们将看到有多少关于rnn的相同思想可以应用于另一种类型的序列数据:音乐。</p>
<h2 id="Chapter-7-Compose"><a href="#Chapter-7-Compose" class="headerlink" title="Chapter 7. Compose"></a>Chapter 7. Compose</h2><p>除了视觉艺术和创意写作，音乐创作是我们认为人类独有的另一种核心创意行为。</p>
<p>要让机器创作出令我们愉悦的音乐，它必须克服前一章中涉及文本时遇到的许多相同的技术挑战。特别是，我们的模型必须能够学习并重新创建音乐的顺序结构，还必须能够从后续音符的离散可能性中进行选择。</p>
<p>然而，音乐生成提出了文本生成所不需要的额外挑战，即音高和节奏。音乐通常是复音的——也就是说，有几个音符流同时在不同的乐器上演奏，它们结合起来创造出不和谐(冲突)或和谐(和谐)的和谐。文本生成只需要我们处理单个文本流，而不是音乐中存在的并行和弦流。</p>
<p>此外，文本生成可以一次处理一个单词。我们必须仔细考虑这是否是处理音乐数据的合适方法，因为听音乐的大部分兴趣是在整个乐团不同节奏之间的相互作用。例如，吉他手可能会演奏一连串更快的音符，而钢琴家则演奏较长的持续和弦。因此，按音符生成音乐是复杂的，因为我们通常不希望所有的乐器同时变换音符。</p>
<p>本章将从简化问题开始，专注于单声道音乐的音乐生成。我们将看到，前一章中关于文本生成的许多RNN技术也可以用于音乐生成，因为这两个任务有许多共同的主题。本章还将介绍注意力机制，它允许我们构建rnn，能够选择关注之前的音符，从而预测接下来会出现哪些音符。最后，我们将解决复调音乐生成的任务，并探索如何部署基于GANs的架构来为多个声音创建音乐。</p>
<h3 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h3><p>任何处理音乐生成任务的人都必须首先对音乐理论有一个基本的了解。在本节中，我们将介绍阅读音乐所需的基本符号以及如何将其表示为数字，以便将音乐转换为训练生成模型所需的输入数据。</p>
<p>我们将学习notebook 07_01_notation_compose。本书存储库中的Ipynb。另一个入门使用Python生成音乐的优秀资源是Sigurður Skúli的博客文章和附带的GitHub存储库。</p>
<p>我们将使用的原始数据集是一组J.S.巴赫的大提琴组曲的MIDI文件。你可以使用任何你想使用的数据集，但如果你想使用这个数据集，你可以在notebook中找到下载MIDI文件的说明。</p>
<p>要查看和收听模型生成的音乐，您需要一些可以生成乐谱的软件。MuseScore是一个很好的工具，可以免费下载。</p>
<h3 id="Musical-Notation"><a href="#Musical-Notation" class="headerlink" title="Musical Notation"></a>Musical Notation</h3><p>我们将使用Python库music21将MIDI文件加载到Python中进行处理。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> music21 <span class="keyword">import</span> converter</span><br><span class="line">dataset_name = <span class="string">'cello'</span></span><br><span class="line">filename = <span class="string">'cs1-2all'</span></span><br><span class="line">file = <span class="string">"./data/{}/{}.mid"</span>.<span class="built_in">format</span>(dataset_name, filename)</span><br><span class="line"></span><br><span class="line">original_score = converter.parse(file).chordify()</span><br></pre></td></tr></tbody></table></figure>

<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230425163224780.png" alt="image-20230425163224780"></p>
<p>我们使用chordify方法将所有同时演奏的音符压缩为一个声部中的和弦，而不是将它们拆分为多个声部。由于这首曲子是由一种乐器(大提琴)演奏的，所以我们这样做是合理的，尽管有时我们可能希望将这些部分分开来产生本质上是复调的音乐。这带来了更多的挑战，我们将在本章后面讨论。</p>
<p>代码逻辑：循环遍历乐谱，并将乐曲中每个音符(和休止符)的音高和时值提取到两个列表中。和弦中的单个音符被一个点分隔开，这样整个和弦就可以被存储为一个单独的弦。每个音符名称后面的数字表示音符所在的八度，因为音符名称(A到G)重复，所以需要这个八度来唯一标识音符的音高。例如，G2是低于G3的一个倍频程。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">notes = []</span><br><span class="line">durations = []</span><br><span class="line"><span class="keyword">for</span> element <span class="keyword">in</span> original_score.flat:</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(element, chord.Chord):</span><br><span class="line">        notes.append(<span class="string">'.'</span>.join(n.nameWithOctave <span class="keyword">for</span> n <span class="keyword">in</span> element.pitches))</span><br><span class="line">        durations.append(element.duration.quarterLength)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(element, note.Note):</span><br><span class="line">        <span class="keyword">if</span> element.isRest:</span><br><span class="line">            notes.append(<span class="built_in">str</span>(element.name))</span><br><span class="line">            durations.append(element.duration.quarterLength)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            notes.append(<span class="built_in">str</span>(element.nameWithOctave))</span><br><span class="line">            durations.append(element.duration.quarterLength)</span><br></pre></td></tr></tbody></table></figure>

<p>这个过程的输出如表7-1所示。</p>
<p>结果数据集现在看起来更像我们之前处理过的文本数据。单词就是音高，我们应该尝试建立一个模型，在给定之前的音高序列的情况下，预测下一个音高。同样的想法也可以应用于持续时间列表。Keras使我们能够灵活地构建一个可以同时处理音高和持续时间预测的模型。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230425164913502.png" alt="image-20230425164913502"></p>
<h3 id="Your-First-Music-Generating-RNN"><a href="#Your-First-Music-Generating-RNN" class="headerlink" title="Your First Music-Generating RNN"></a>Your First Music-Generating RNN</h3><p>为了创建用于训练模型的数据集，我们首先需要给每个基音和持续时间一个整数值(图7-2)，就像我们之前对文本语料库中的每个单词所做的那样。这些值是什么并不重要，因为我们将使用嵌入层将整数查找值转换为向量。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230425165851992.png" alt="image-20230425165851992"></p>
<p>然后，我们通过将数据分割为32个音符的小块，并具有序列中下一个音符的响应变量(独热编码)，用于音高和持续时间，来创建训练集。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="C:\Users\cheris\AppData\Roaming\Typora\typora-user-images\image-20230425170106412.png" alt="image-20230425170106412"></p>
<p>我们将要构建的模型是一个带有注意力机制的堆叠LSTM网络。在上一章中，我们看到了如何通过将前一层的隐藏状态作为输入传递到下一层LSTM层来堆叠LSTM层。以这种方式堆叠层可以让模型自由地从数据中学习更复杂的特征。在本节中，我们将介绍注意力机制，它现在是最复杂的序列生成模型的组成部分。它最终产生了transformer，一种完全基于注意力的模型，甚至不需要递归或卷积层。第9章会详细介绍transformer的架构。</p>
<p>现在，让我们专注于将注意力合并到堆叠的LSTM网络中，以尝试在给定之前的音符序列的情况下预测下一个音符。</p>
<h4 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h4><p>注意力机制最初应用于文本翻译问题，特别是将英语句子翻译成法语。</p>
<p>在上一章中，我们看到了编码器-解码器网络如何解决这种问题，首先将输入序列通过编码器来生成上下文向量，然后将这个向量通过解码器网络来输出翻译后的文本。这种方法的一个问题是上下文向量可能成为瓶颈。来自源句子开头的信息在到达上下文向量时可能会被稀释，特别是对于长句子。因此，这种类型的编码器-解码器网络有时很难保留所有所需的信息，以使解码器准确地翻译源。</p>
<p>例如，假设我们希望模型将以下句子翻译成德语:I score a penalty in the football match against England。</p>
<p>显然，如果将单词scores替换为missed，整个句子的意思就会发生变化。然而，编码器的最终隐藏状态可能无法充分保留这些信息，因为得分的单词出现在句子的早期。</p>
<p>这个句子的正确翻译是:Ich habe im Fußballspiel gegen England einen Elfmeter erzielt。</p>
<p>如果我们看一下正确的德语翻译，我们可以看到得分(erzielt)这个词实际上出现在句子的末尾!因此，这个模型不仅要保留这样一个事实，即在编码器中判罚得分而不是失分，而且还要一直保留在解码器中。</p>
<p>同样的原理也适用于音乐。要了解某一特定的音乐段落可能会出现哪些音符或音符序列，使用序列中较早的信息可能是至关重要的，而不仅仅是最新的信息。以巴赫第一大提琴组曲的前奏曲为例(图7-4)。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230425172436776.png" alt="image-20230425172436776"></p>
<p>你认为下一个音符是什么?即使你没有受过音乐训练，你也可以猜出来。如果你说的是G(与曲子的第一个音符相同)，那么你是正确的。你怎么知道的?你可能已经看到每一小节和半小节都以相同的音符开始，并利用这些信息来指导你的决定。我们希望我们的模型能够执行同样的技巧——特别是，我们希望它不仅关心网络现在的隐藏状态，而且还要特别注意网络在8个音符前的隐藏状态，当前一个低G被记录时。</p>
<p>为了解决这一问题，提出了注意机制。而不是只使用编码器RNN的最终隐藏状态作为上下文向量，注意机制允许模型创建上下文向量作为编码器RNN在每个前一个时间步的隐藏状态的加权和。注意机制是将之前的编码器隐藏状态和当前的解码器隐藏状态转换为生成上下文向量的加权总和的一组层。</p>
<p>如果这听起来令人困惑，不要担心!首先，我们将看到如何在一个简单的循环层之后应用注意力机制(即，解决预测巴赫大提琴组曲第一的下一个音符的问题)，然后我们将看到它如何扩展到编码器-解码器网络，我们想要预测后续音符的整个序列，而不仅仅是一个。</p>
<h4 id="Building-an-Attention-Mechanism-in-Keras"><a href="#Building-an-Attention-Mechanism-in-Keras" class="headerlink" title="Building an Attention Mechanism in Keras"></a>Building an Attention Mechanism in Keras</h4><p>首先，让我们提醒自己如何使用标准循环层来预测给定前一个音符序列的下一个音符。图7-5显示了输入序列$$(x_1，…，x_n)$$如何一步一步地馈送到层，不断更新层的隐藏状态。输入序列可以是注释嵌入，也可以是来自前一个循环层的隐藏状态序列。循环层的输出是最终隐藏状态，一个与单元数相同长度的向量。然后可以将其馈送到具有softmax输出的Dense层，以预测序列中下一个音符的分布。</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230426091711036.png" alt="image-20230426091711036"></p>
<p>图7-6显示了相同的网络，但这一次将注意力机制应用于循环层的隐藏状态。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230426091851247.png" alt="image-20230426091851247"></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">notes_in = Input(shape = (<span class="literal">None</span>,)) <span class="comment"># 音符</span></span><br><span class="line">durations_in = Input(shape = (<span class="literal">None</span>,)) <span class="comment"># 持续时间</span></span><br><span class="line">x1 = Embedding(n_notes, embed_size)(notes_in)  </span><br><span class="line">x2 = Embedding(n_durations, embed_size)(durations_in)</span><br><span class="line">x = Concatenate()([x1,x2]) <span class="comment"># 拼接</span></span><br><span class="line">x = LSTM(rnn_units, return_sequences=<span class="literal">True</span>)(x) </span><br><span class="line">x = LSTM(rnn_units, return_sequences=<span class="literal">True</span>)(x)</span><br><span class="line">e = Dense(<span class="number">1</span>, activation=<span class="string">'tanh'</span>)(x) </span><br><span class="line">e = Reshape([-<span class="number">1</span>])(e)</span><br><span class="line">alpha = Activation(<span class="string">'softmax'</span>)(e) </span><br><span class="line">c = Permute([<span class="number">2</span>, <span class="number">1</span>])(RepeatVector(rnn_units)(alpha)) </span><br><span class="line">c = Multiply()([x, c])</span><br><span class="line">c = Lambda(<span class="keyword">lambda</span> xin: K.<span class="built_in">sum</span>(xin, axis=<span class="number">1</span>), output_shape=(rnn_units,))(c)</span><br><span class="line">notes_out = Dense(n_notes, activation = <span class="string">'softmax'</span>, name = <span class="string">'pitch'</span>)(c) </span><br><span class="line">durations_out = Dense(n_durations, activation = <span class="string">'softmax'</span>, name = <span class="string">'duration'</span>)(c)</span><br><span class="line">model = Model([notes_in, durations_in], [notes_out, durations_out]) </span><br><span class="line">att_model = Model([notes_in, durations_in], alpha) </span><br><span class="line">opti = RMSprop(lr = <span class="number">0.001</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(loss=[<span class="string">'categorical_crossentropy'</span>, <span class="string">'categorical_crossentropy'</span>],</span><br><span class="line">              optimizer=opti)</span><br></pre></td></tr></tbody></table></figure>

<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230426101938940.png" alt="image-20230426101938940"></p>
<h4 id="Analysis-of-the-RNN-with-Attention"><a href="#Analysis-of-the-RNN-with-Attention" class="headerlink" title="Analysis of the RNN with Attention"></a>Analysis of the RNN with Attention</h4><p>我们将从从头开始生成一些音乐开始，通过仅用&lt; start &gt;令牌(即，我们告诉模型假设它是从片段的开头开始的)。然后我们可以使用我们在第6章中用于生成文本序列的相同迭代技术来生成一个音乐段落，如下所示:</p>
<ul>
<li>给定当前序列(音符名称和音符持续时间)，该模型预测下一个音符名称和持续时间的两个分布。</li>
<li>我们从这两个分布中采样，使用温度参数来控制采样过程中的变化程度。</li>
<li>所选音符被存储，其名称和持续时间被附加到相应的序列中。</li>
<li>如果序列的长度现在大于模型被训练的序列长度，我们从序列的开始处删除一个元素。</li>
<li>这个过程用新的序列重复，依此类推，我们希望生成多少音符就生成多少音符。</li>
</ul>
<p>图7-8显示了该模型在训练过程的各个阶段从头生成的音乐示例。我们在本节中的大部分分析将集中在音调预测上，而不是节奏上，因为巴赫的大提琴组曲的和声复杂性更难捕捉，因此更值得研究。但是，您也可以将相同的分析应用于模型的节奏预测，这可能与您可以用于训练该模型的其他风格的音乐(例如鼓音轨)特别相关。</p>
<p>关于图7-8中生成的通道，有几点需要注意。首先，看看随着训练的进行，音乐是如何变得越来越复杂的。首先，该模型通过坚持使用同一组音符和节奏来确保安全。到第10阶段，这个模型已经开始产生小的音符，到第20阶段，它开始产生有趣的节奏，并牢固地建立在一个固定的键(降e大调)上。</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230426103252726.png" alt="image-20230426103252726"></p>
<p>每个时间步的预测分布作为热图。图7-8中epoch 20为例，热图如图7-9所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230426103338445.png" alt="image-20230426103338445"></p>
<p>这里值得注意的有趣的一点是，模型已经清楚地了解了哪些音符属于特定的键，因为在不属于该键的音符的分布中存在空白。例如，有一个灰色的间隙沿行注释54(对应于Gb/f#)。这个音在降e大调的乐曲中是极不可能出现的。在生成过程的早期(图的左边)，关键还没有牢固地建立起来，因此在如何选择下一个音符方面有更多的不确定性。随着作品的发展，模型固定在一个键上，某些音符几乎肯定不会出现。</p>
<p>值得注意的是，这个模型并没有在一开始就明确地决定将音乐设置在某个键上，而是在它的过程中不断地创造它，试图选择最适合它之前选择的音符。值得指出的是，该模型已经学习了巴赫的典型风格，即在大提琴上下降到一个低音来结束一个乐句，然后再弹回来开始下一个乐句。看看在音符20左右，乐句以低e调结束，这在巴赫大提琴组曲中很常见，然后在下一个乐句开始时回到更高、更铿锵的乐器音域，这正是模型所预测的。在低E- flat(音高39)和下一个音符之间有一个很大的灰色间隙，这个音符预计在音高50左右，而不是继续在乐器的深处隆隆作响。</p>
<p>最后，我们应该检查我们的注意力机制是否像预期的那样工作。图7-10所示为网络在生成序列中各点计算出的alpha向量元素值。横轴表示生成的音符序列;纵轴显示了当沿水平轴(即alpha向量)预测每个音符时，网络的注意力集中在哪里。正方形越暗，对序列中与此点对应的隐藏状态的关注就越大。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230426103628561.png" alt="image-20230426103628561"></p>
<p>我们可以看到，对于作品的第二个音符(B-3 =降b)，网络选择将几乎所有的注意力放在作品的第一个音符也是B-3的事实上。这是有道理的;如果你知道第一个音符是降B，你可能会用这个信息来决定下一个音符。</p>
<p>当我们在接下来的几个音符中移动时，神经网络将它的注意力大致均匀地分散在之前的音符上——然而，它很少把注意力放在超过六个音符之前的音符上。再说一次，这是有道理的;在前六个隐藏状态中可能包含足够的信息来理解这个短语应该如何继续。也有网络选择忽略附近某个音符的例子，因为它不会在理解短语时添加任何额外的信息。</p>
<p>例如，看一下图表中心的白色方框，注意中间有一条方框，它切断了回顾前面四到六个音符的通常模式。为什么网络在决定如何继续这个短语时愿意选择忽略这个注释呢?如果你看一下它对应的是哪个音符，你会发现它是三个E-3音符中的第一个。模型选择忽略这一点，因为在此之前的音符也是降e，低一个八度(E-2)。此时网络的隐藏状态将为模型提供足够的信息来理解降e是这段话中的一个重要音符，因此模型不需要注意后续的更高的降e，因为它没有添加任何额外的信息。</p>
<p>更多的证据表明，该模型已经开始理解八度的概念，可以在下方和右侧的绿色框中看到。在这里，模型选择忽略低G (G2)，因为在此之前的音符也是G (G3)，高一个八度。记住，我们并没有告诉模型，哪些音符是通过八度相关联的——它只是通过研究巴赫的音乐，自己解决了这个问题，这很了不起。</p>
<h4 id="Attention-in-Encoder–Decoder-Networks"><a href="#Attention-in-Encoder–Decoder-Networks" class="headerlink" title="Attention in Encoder–Decoder Networks"></a>Attention in Encoder–Decoder Networks</h4><p>注意机制是一个强大的工具，可以帮助网络决定循环层的哪些先前状态对于预测序列的延续是重要的。到目前为止，我们已经看到了提前一个音符的预测。然而，我们也可能希望将注意力构建到编码器-解码器网络中，在那里我们通过使用RNN解码器来预测未来音符的序列，而不是一次构建一个音符序列。</p>
<p>回顾一下，图7-11显示了一个标准的编码器-解码器模型在没有注意的情况下的样子——我们在第6章中介绍的那种。图7-12显示了相同的网络，但在编码器和解码器之间增加了注意机制。</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230426151200476.png" alt="image-20230426151200476"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230426151244452.png" alt="image-20230426151244452"></p>
<p>注意机制的工作方式与我们之前看到的完全相同，只是有一点改变:解码器的隐藏状态也被纳入到机制中，这样模型不仅可以通过之前的编码器隐藏状态，还可以从当前的解码器隐藏状态决定将注意力集中在哪里。图7-13显示了编码器-解码器框架中注意模块的内部工作原理。</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230426151449798.png" alt="image-20230426151449798"></p>
<p>虽然在编码器-解码器网络中存在许多注意机制的副本，但它们都共享相同的权值，因此在需要学习的参数数量上没有额外的开销。唯一的变化是，现在，解码器隐藏状态被滚动到注意力计算中(图中的红线)。这稍微改变了方程，加入了一个额外的索引(i)来指定解码器的步长。</p>
<p>还要注意在图7-11中我们如何使用编码器的最终状态来初始化解码器的隐藏状态。在有注意的编码器-解码器中，我们使用循环层的内置标准初始化器来初始化解码器。上下文向量c与传入数据y连接起来，形成一个扩展的数据向量，进入解码器的每个单元。因此，我们将上下文向量视为输入解码器的附加数据。</p>
<h3 id="Generating-Polyphonic-Music"><a href="#Generating-Polyphonic-Music" class="headerlink" title="Generating Polyphonic Music"></a>Generating Polyphonic Music</h3><p>我们在本节中探索的带有注意机制框架的RNN在单线(单音)音乐中效果很好，但它能适应多线(复音)音乐吗?</p>
<p>RNN框架当然足够灵活，可以通过循环机制同时生成多条音乐线。但就目前而言，我们目前的数据集并没有很好地为此设置，因为我们将和弦存储为单个实体，而不是由多个单独的音符组成的部分。例如，我们当前的RNN无法知道C大调和弦(C、E和G)实际上与a小调和弦(a、C和E)非常接近——只有一个音符需要改变，即G变为a。</p>
<p>相反，它将两者视为两个不同的元素，可以独立预测。理想情况下，我们希望设计一个网络，可以接受多个渠道的音乐作为单独的流，并学习这些流应该如何相互作用，以产生好听的音乐，而不是不和谐的噪音。</p>
<p>这听起来是不是有点像生成图像?对于图像生成，我们有三个通道(红、绿、蓝)，我们希望网络学习如何组合这些通道来生成漂亮的图像，而不是随机的像素化噪声。事实上，正如我们将在下一节看到的，我们可以将音乐生成直接视为图像生成问题。这意味着我们可以将同样的基于卷积的技术应用于音乐图像生成问题，而不是使用循环网络，特别是gan。在我们探索这个新建筑之前，我们只有足够的时间去参观音乐厅，那里的演出即将开始。</p>
<h3 id="The-Musical-Organ"><a href="#The-Musical-Organ" class="headerlink" title="The Musical Organ"></a>The Musical Organ</h3><p>指挥家在指挥台上敲了两下指挥棒。演出就要开始了。在他面前坐着一支管弦乐队。然而，这支乐团并不打算演奏贝多芬的交响乐或柴可夫斯基的序曲。这个管弦乐队在演出期间现场创作原创音乐，完全由一组演奏者向舞台中央的一个巨大的管风琴(简称MuseGAN)发出指令，它将这些指令转化为美妙的音乐，为观众带来愉悦。管弦乐队可以通过训练来演奏特定风格的音乐，而且没有两次演出是相同的。</p>
<p>管弦乐队的128名演奏者被分成4个平均的小组，每组32名演奏者。每个部分都向MuseGAN提供指示，并在管弦乐队中负有明确的责任。</p>
<p>风格组负责制作演出的整体音乐风格。在许多方面，它是所有部分中最简单的工作，因为每个演奏者只需要在音乐会开始时生成一个指令，然后在整个演出过程中不断地向博物馆提供信息。凹槽部分有类似的工作，但每个播放器产生几个指令:一个为每个不同的音乐轨道，由MuseGAN输出。</p>
<p>例如，在一场音乐会中，每个groove部分的成员制作了五个指令，分别对应人声、钢琴、弦乐、贝斯和鼓的音轨。因此，他们的工作是为每一个独立的器乐声音提供槽，然后在整个演出中保持不变。风格和凹槽部分在整个作品中没有改变它们的指示。表演的动态元素是由最后两个部分提供的，这确保了音乐随着每个小节的变化而不断变化。小节(或小节)是一个小的音乐单位，包含固定的、少量的节拍。例如，如果你能跟着一段音乐数1、2、1、2，那么每小节就有两拍，你可能在听进行曲。如果你能数1、2、3、1、2、3，那么每小节有三拍，你可能正在听华尔兹。</p>
<p>和弦部分的演奏者在每小节开始时改变他们的指示。这样做的效果是给每个小节一个独特的音乐特征，例如，通过一个和弦的变化。和弦部分的演奏者每小节只产生一个指令，然后应用于每个器乐轨道。旋律部分的乐手是最累人的工作，因为他们在整首曲子的每个小节开始时对每个器乐音轨给出不同的指示。这些玩家对音乐有最精细的控制，因此这可以被认为是提供旋律兴趣的部分。这就完成了对管弦乐队的描述。</p>
<p>我们可以将各部分的职责总结如表7-2所示。</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230426154336388.png" alt="image-20230426154336388"></p>
<p>根据当前的128条指令(每个播放器一条指令)，由MuseGAN生成下一个音乐小节。训练MuseGAN做到这一点并不容易。最初，乐器只会产生可怕的噪音，因为它无法理解如何解释指令来产生与真正的音乐没有区别的小节。</p>
<p>这就是指挥员的作用。当音乐与真实音乐明显不同时，指挥会告诉MuseGAN，然后MuseGAN调整其内部线路，以便下次更有可能骗过指挥。指挥和博物馆使用的过程与我们在第4章看到的完全相同，Di和Gene一起工作，不断改进Gene拍摄的动物照片。MuseGAN的演奏者在世界各地巡回演出，在有足够的现有音乐来训练MuseGAN的地方举办任何风格的音乐会。在下一节中，我们将看到如何使用Keras构建MuseGAN，以学习如何生成逼真的复调音乐。</p>
<h2 id="Chapter-8-Play"><a href="#Chapter-8-Play" class="headerlink" title="Chapter 8. Play"></a>Chapter 8. Play</h2><p>2018年3月，David Ha和Jürgen Schmidhuber发表了他们的“世界模型”论文。这篇论文展示了如何训练一个模型，该模型可以通过在自身生成的幻觉梦境中进行实验，而不是在环境本身中学习如何执行特定任务。当与强化学习等其他机器学习技术一起应用时，这是一个很好的例子，说明了生成式建模如何用于解决实际问题。</p>
<p>该架构的一个关键组件是生成模型，在给定当前状态和动作的情况下，该模型可以构建下一个可能状态的概率分布。在通过随机运动建立了对环境基本物理的理解之后，该模型能够完全在其自身对环境的内部表示中从头开始训练自己的新任务。这种方法在两项测试中都获得了世界上最好的分数。</p>
<p>在本章中，我们将详细探索该模型，并展示如何创建这种惊人的尖端技术的自己版本。基于原始论文，我们将构建一个强化学习算法，学习如何以尽可能快的速度在赛道上驾驶汽车。虽然我们将使用2D计算机模拟作为我们的环境，但同样的技术也可以应用于现实世界的场景，在真实环境中测试策略是昂贵的或不可实现的。</p>
<p>然而，在我们开始构建模型之前，我们需要仔细了解一下强化学习的概念和OpenAI Gym平台。</p>
<h3 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h3><p>强化学习可以定义如下:</p>
<ul>
<li>强化学习(RL)是机器学习的一个领域，旨在训练智能体在给定环境中针对特定目标进行最佳表现。</li>
</ul>
<p>判别建模和生成建模的目标都是最小化观测数据集上的损失函数，而强化学习的目标是最大化智能体在给定环境中的长期奖励。它通常被描述为机器学习的三大分支之一，与监督学习(使用标记数据进行预测)和无监督学习(从无标记数据中学习结构)并列。</p>
<p>让我们首先介绍一些与强化学习相关的关键术语:</p>
<ul>
<li><p>Environment：智能体运行的世界。它定义了一组规则，根据智能体的先前动作和当前游戏状态，控制游戏状态更新过程和奖励分配。例如，如果我们正在教强化学习算法下国际象棋，环境将由控制给定行动(例如，移动e4)如何影响下一个游戏状态(棋盘上棋子的新位置)的规则组成，还将指定如何评估给定位置是否被将死，并在获胜棋手获胜后分配1的奖励。</p>
</li>
<li><p>Agent：在环境中执行操作的智能体。</p>
</li>
<li><p>Game state：代表代理可能的特定情况的数据遭遇(也称为状态)，例如，一个特定的棋盘配置附带的游戏信息，如哪个玩家会采取下一步行动。</p>
</li>
<li><p>Action：一个智能体可以采取的可行行动。</p>
</li>
<li><p>Reward：奖励</p>
</li>
<li><p>Episode：在环境中运行一次智能体;这也被称为rollout。</p>
</li>
<li><p>Timestep：对于离散事件环境，所有状态、动作和奖励都被标下标，以显示它们在时间步t的值。</p>
</li>
</ul>
<p>这些定义之间的关系如图8-1所示。</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230426165238864.png" alt="image-20230426165238864"></p>
<p>首先用当前游戏状态s初始化环境。在时间步t，智能体接收当前游戏状态s，并使用它来决定下一个最佳动作a，然后执行该动作。给定这个动作，环境然后计算下一个状态s并奖励r，并将这些传递回智能体，以便再次开始循环。循环继续，直到满足事件的结束标准(例如，经过给定数量的时间步长或代理赢/输)。</p>
<p>我们如何设计一个智能体来最大化给定环境中的奖励总和?我们可以构建一个智能体，它包含一套如何响应任何给定游戏状态的规则。然而，随着环境变得更加复杂，这很快就变得不可行，并且永远不允许我们在特定任务中构建具有超人能力的智能体，因为我们正在硬编码规则。强化学习包括创建一个智能体，该智能体可以通过反复游戏在复杂环境中学习最优策略——本章将使用它来构建我们的智能体。</p>
<p>现在我将介绍OpenAI Gym，赛车环境之家，我们将使用它来模拟汽车在赛道上行驶。</p>
<h3 id="OpenAI-Gym"><a href="#OpenAI-Gym" class="headerlink" title="OpenAI Gym"></a>OpenAI Gym</h3><p>OpenAI Gym是一个用于开发强化学习算法的工具包，可以作为Python库使用。</p>
<p>该库中包含几个经典的强化学习环境，如CartPole和Pong，以及提出更复杂挑战的环境，如训练智能体在不平坦的地形上行走或赢得Atari游戏。所有的环境都提供了step方法，你可以通过它提交给定的操作;环境将返回下一个状态和奖励。通过使用智能体选择的操作反复调用step方法，您可以在环境中播放一个片段。</p>
<p>除了每个环境的抽象机制外，OpenAI Gym还提供了允许您观看智能体在给定环境中执行的图形。这对于调试和查找智能体可以改进的地方很有用。我们将利用OpenAI Gym中的赛车环境。让我们看看如何为这个环境定义游戏状态、动作、奖励和情节:</p>
<ul>
<li>Game state<ul>
<li>A 64 × 64–pixel RGB image depicting an overhead view of the track and car.</li>
</ul>
</li>
<li>Action<ul>
<li>A set of three values: the steering direction (–1 to 1), acceleration (0 to 1), and braking (0 to 1). The agent must set all three values at each timestep.</li>
</ul>
</li>
<li>Reward<ul>
<li>A negative penalty of –0.1 for each timestep taken and a positive reward of 1000/N if a new track tile is visited, where N is the total number of tiles that make up the track.</li>
</ul>
</li>
<li>Episode<ul>
<li>The episode ends when either the car completes the track, drives off the edge of the environment, or 3,000 timesteps have elapsed.</li>
</ul>
</li>
</ul>
<p>图8-2以图形形式展示了这些概念。请注意，汽车从它的视角看不到轨道，但我们应该想象一个漂浮在轨道上方的智能体从鸟瞰图控制汽车。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230426171429540.png" alt="image-20230426171429540"></p>
<h3 id="World-Model-Architecture"><a href="#World-Model-Architecture" class="headerlink" title="World Model Architecture"></a>World Model Architecture</h3><p>在探索构建每个组件所需的详细步骤之前，我们现在将介绍我们将用于构建通过强化学习进行学习的代理的整个架构的高级概述。该解决方案由三个不同的部分组成，分别进行训练，如图8-3所示。</p>
<ul>
<li>V<ul>
<li>A variational autoencoder.</li>
</ul>
</li>
<li>M<ul>
<li>A recurrent neural network with a mixture density network (MDN-RNN).</li>
</ul>
</li>
<li>C<ul>
<li>A controller.</li>
</ul>
</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230426172302701.png" alt="image-20230426172302701"></p>
<h4 id="The-Variational-Autoencoder"><a href="#The-Variational-Autoencoder" class="headerlink" title="The Variational Autoencoder"></a>The Variational Autoencoder</h4><p>当你在开车时做决定时，你不会主动分析视图中的每个像素，而是将视觉信息压缩为少量的潜在实体，例如道路的直线度、即将到来的转弯以及你相对于道路的位置，以通知你的下一步行动。</p>
<p> 我们在第3章中看到，VAE如何通过最小化重构误差和KL散度，将一个高维输入图像压缩为一个近似服从标准多元正态分布的潜在随机变量。这确保了潜空间是连续的，并且我们能够轻松地从中采样以生成有意义的新观测。</p>
<p>在赛车示例中，VAE将64 × 64 × 3 (RGB)输入图像压缩为32维正态分布随机变量，参数为mu和log_var。这里，log_var是分布方差的对数。我们可以从这个分布中采样，以产生表示当前状态的潜在向量z。这被传递到网络的下一部分，MDN-RNN。</p>
<h4 id="The-MDN-RNN"><a href="#The-MDN-RNN" class="headerlink" title="The MDN-RNN"></a>The MDN-RNN</h4><p>当你开车时，随后的每一个观察对你来说都不是一个完全的惊喜。如果当前的观察显示前方道路左转，而你把方向盘转到左边，你期望下一次观察显示你仍然与道路保持一致。</p>
<p>如果你没有这种能力，你的驾驶可能会在整个道路上蜿蜒，因为你无法看到稍微偏离中心会在下一个时间步变得更糟，除非你现在就做些什么。</p>
<p>这种前瞻性思维是MDN-RNN的工作，这是一个试图根据前一个潜在状态和前一个动作预测下一个潜在状态分布的网络。具体来说，MDN-RNN是一个具有256个隐藏单元的LSTM层，后面是一个混合密度网络(MDN)输出层，它允许下一个潜在状态实际上可以从几个正态分布中的任何一个中提取。“世界模型”论文的作者之一David Ha将同样的技术应用于手写生成任务，如图8-4所示，以描述下一个笔尖可能落在任何一个明显的红色区域的事实。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://gitee.com/chjjj666/mkdown-images/raw/master/imgs/image-20230427110040387.png" alt="image-20230427110040387"></p>
<p>在赛车的例子中，我们允许从五个正态分布中的任意一个提取下一个观察到的潜在状态的每个元素。</p>
<h4 id="The-Controller"><a href="#The-Controller" class="headerlink" title="The Controller"></a>The Controller</h4><p>到目前为止，我们还没有提到任何关于选择动作的内容。这个责任在于控制者。</p>
<p>控制器是一个密集连接的神经网络，其中输入是z(从VAE编码的分布中采样的当前潜状态)和RNN的隐藏状态的连接。三个输出神经元对应于三个动作(转弯、加速、刹车)，并被缩放到适当的范围内下降。我们需要使用强化学习来训练控制器，因为没有训练数据集可以告诉我们某个动作是好的，另一个动作是坏的。</p>
<p>相反，智能体将需要通过反复实验自己发现这一点。正如我们将在本章后面看到的，“世界模型”论文的关键是它演示了如何在智能体自己的环境生成模型中进行这种强化学习，而不是在OpenAI Gym环境中。换句话说，它发生在智能体对环境行为的幻觉版本中，而不是真实的东西。</p>
<p>为了理解这三个组件的不同角色以及它们如何一起工作，我们可以想象它们之间的对话:</p>
<ul>
<li><p>VAE(查看最新的64 × 64 × 3观察):这看起来像一条笔直的道路，有一个轻微的左转弯接近，汽车面向道路的方向(z)。</p>
</li>
<li><p>RNN:根据描述(z)和控制器在最后一个时间步(动作)选择加速的事实，我将更新我的隐藏状态，以便预测下一个观察仍然是直线道路，但在视图中有更多的左转。</p>
</li>
<li><p>控制器:基于VAE (z)的描述和RNN (h)的当前隐藏状态，我的神经网络输出[0.34,0.8,0]作为下一个动作。</p>
</li>
</ul>
<p>然后，控制器的动作被传递给环境，环境返回一个更新后的观测值，然后循环再次开始。</p>
<p>有关该模型的更多信息，也有一个优秀的在线交互式解释。</p>
<h4 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h4></article><div class="post-copyright"><div class="copyright-cc-box"><i class="anzhiyufont anzhiyu-icon-copyright"></i></div><div class="post-copyright__author_box"><a class="post-copyright__author_img" href="/" title="头像"><img class="post-copyright__author_img_back" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" title="头像" alt="头像"><img class="post-copyright__author_img_front" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" title="头像" alt="头像"></a><div class="post-copyright__author_name">陈汉江</div><div class="post-copyright__author_desc">AI创造非凡人生</div></div><div class="post-copyright__post__info"><a class="post-copyright__original" title="该文章为原创文章，注意版权协议" href="http://promoteai.com/2023/05/12/sheng-cheng-shi-ai-xiang-jie/">原创</a><a class="post-copyright-title"><span onclick="rm.copyPageUrl('http://promoteai.com/2023/05/12/sheng-cheng-shi-ai-xiang-jie/')">生成式AI详解</span></a></div><div class="post-tools" id="post-tools"><div class="post-tools-left"><div class="rewardLeftButton"><div class="post-reward" onclick="anzhiyu.addRewardMask()"><div class="reward-button button--animated" title="赞赏作者"><i class="anzhiyufont anzhiyu-icon-hand-heart-fill"></i>打赏作者</div><div class="reward-main"><div class="reward-all"><span class="reward-title">感谢你赐予我前进的力量</span><ul class="reward-group"><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul><a class="reward-main-btn" href="/about/#about-reward" target="_blank"><div class="reward-text">赞赏者名单</div><div class="reward-dec">因为你们的支持让我意识到写文章的价值🙏</div></a></div></div></div><div id="quit-box" onclick="anzhiyu.removeRewardMask()" style="display: none"></div></div><div class="shareRight"><div class="share-link mobile"><div class="share-qrcode"><div class="share-button" title="使用手机访问这篇文章"><i class="anzhiyufont anzhiyu-icon-qrcode"></i></div><div class="share-main"><div class="share-main-all"><div id="qrcode" title="http://promoteai.com/2023/05/12/sheng-cheng-shi-ai-xiang-jie/"></div><div class="reward-dec">使用手机访问这篇文章</div></div></div></div></div><div class="share-link weibo"><a class="share-button" target="_blank" href="https://service.weibo.com/share/share.php?title=undefined&amp;url=http://promoteai.com/2023/05/12/sheng-cheng-shi-ai-xiang-jie/&amp;pic=undefined" rel="external nofollow noreferrer noopener"><i class="anzhiyufont anzhiyu-icon-weibo"></i></a></div><script>function copyCurrentPageUrl() {
  var currentPageUrl = window.location.href;
  var input = document.createElement("input");
  input.setAttribute("value", currentPageUrl);
  document.body.appendChild(input);
  input.select();
  input.setSelectionRange(0, 99999);
  document.execCommand("copy");
  document.body.removeChild(input);
}</script><div class="share-link copyurl"><div class="share-button" id="post-share-url" title="复制链接" onclick="copyCurrentPageUrl()"><i class="anzhiyufont anzhiyu-icon-link"></i></div></div></div></div></div><div class="post-copyright__notice"><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://promoteai.com" target="_blank">promoteAI</a>！</span></div></div><div class="post-tools-right"><div class="tag_share"><div class="post-meta__box"><div class="post-meta__box__tag-list"></div></div></div><div class="post_share"><div class="social-share" data-image="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"/><script src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer="defer"></script></div></div><nav class="pagination-post" id="pagination"></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-content"><div class="author-info__sayhi" id="author-info__sayhi" onclick="anzhiyu.changeSayHelloText()"></div><div class="author-info-avatar"><img class="avatar-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-status"><img class="g-status" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/08/24/64e6ce9c507bb.png" alt="status"/></div></div><div class="author-info__description">这是属于陈汉江的个人博客</div></div></div><div class="card-widget anzhiyu-right-widget" id="card-wechat" onclick="null"><div id="flip-wrapper"><div id="flip-content"><div class="face" style="background: url(https://bu.dusays.com/2023/01/13/63c02edf44033.png) center center / 100% no-repeat"></div><div class="back face" style="background: url(https://bu.dusays.com/2023/05/13/645fa415e8694.png) center center / 100% no-repeat"></div></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bars"></i><span>文章目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Chapter-5-Paint"><span class="toc-number">1.</span> <span class="toc-text">Chapter 5. Paint</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#CycleGAN"><span class="toc-number">1.1.</span> <span class="toc-text">CycleGAN</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Data"><span class="toc-number">1.1.1.</span> <span class="toc-text">Data</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Overview"><span class="toc-number">1.1.2.</span> <span class="toc-text">Overview</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#The-Generators-U-Net"><span class="toc-number">1.1.3.</span> <span class="toc-text">The Generators (U-Net)</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#CONCATENATE-LAYER"><span class="toc-number">1.1.3.1.</span> <span class="toc-text">CONCATENATE LAYER</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#INSTANCE-NORMALIZATION-LAYER"><span class="toc-number">1.1.3.2.</span> <span class="toc-text">INSTANCE NORMALIZATION LAYER</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#core-code"><span class="toc-number">1.1.3.3.</span> <span class="toc-text">core code</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#The-Discriminators"><span class="toc-number">1.1.4.</span> <span class="toc-text">The Discriminators</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#core-code-1"><span class="toc-number">1.1.4.1.</span> <span class="toc-text">core code</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Compiling-the-CycleGAN"><span class="toc-number">1.1.5.</span> <span class="toc-text">Compiling the CycleGAN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Training-the-CycleGAN"><span class="toc-number">1.1.6.</span> <span class="toc-text">Training the CycleGAN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Analysis-of-the-CycleGAN"><span class="toc-number">1.1.7.</span> <span class="toc-text">Analysis of the CycleGAN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Creating-a-CycleGAN-to-Paint-Like-Monet"><span class="toc-number">1.1.8.</span> <span class="toc-text">Creating a CycleGAN to Paint Like Monet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#The-Generators-ResNet"><span class="toc-number">1.1.9.</span> <span class="toc-text">The Generators (ResNet)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Analysis-of-the-CycleGAN-1"><span class="toc-number">1.1.10.</span> <span class="toc-text">Analysis of the CycleGAN</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Neural-Style-Transfer"><span class="toc-number">1.2.</span> <span class="toc-text">Neural Style Transfer</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Content-Loss"><span class="toc-number">1.2.0.1.</span> <span class="toc-text">Content Loss</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Style-Loss"><span class="toc-number">1.2.0.2.</span> <span class="toc-text">Style Loss</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Total-Variance-Loss"><span class="toc-number">1.2.0.3.</span> <span class="toc-text">Total Variance Loss</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Running-the-Neural-Style-Transfer"><span class="toc-number">1.2.0.4.</span> <span class="toc-text">Running the Neural Style Transfer</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Analysis-of-the-Neural-Style-Transfer-Model"><span class="toc-number">1.2.0.5.</span> <span class="toc-text">Analysis of the Neural Style Transfer Model</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Summary"><span class="toc-number">1.3.</span> <span class="toc-text">Summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Chapter-6-Write"><span class="toc-number">2.</span> <span class="toc-text">Chapter 6. Write</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Long-Short-Term-Memory-Networks"><span class="toc-number">2.1.</span> <span class="toc-text">Long Short-Term Memory Networks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Your-First-LSTM-Network"><span class="toc-number">2.2.</span> <span class="toc-text">Your First LSTM Network</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Tokenization"><span class="toc-number">2.2.1.</span> <span class="toc-text">Tokenization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Building-the-Dataset"><span class="toc-number">2.2.2.</span> <span class="toc-text">Building the Dataset</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#The-LSTM-Architecture"><span class="toc-number">2.2.3.</span> <span class="toc-text">The LSTM Architecture</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#The-Embedding-Layer"><span class="toc-number">2.2.4.</span> <span class="toc-text">The Embedding Layer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#The-LSTM-Layer"><span class="toc-number">2.2.5.</span> <span class="toc-text">The LSTM Layer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#The-LSTM-Cell"><span class="toc-number">2.2.6.</span> <span class="toc-text">The LSTM Cell</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Generating-New-Text"><span class="toc-number">2.3.</span> <span class="toc-text">Generating New Text</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RNN-Extensions"><span class="toc-number">2.4.</span> <span class="toc-text">RNN Extensions</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Stacked-Recurrent-Networks"><span class="toc-number">2.4.1.</span> <span class="toc-text">Stacked Recurrent Networks</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Gated-Recurrent-Units"><span class="toc-number">2.4.2.</span> <span class="toc-text">Gated Recurrent Units</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Bidirectional-Cells"><span class="toc-number">2.4.3.</span> <span class="toc-text">Bidirectional Cells</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Encoder%E2%80%93Decoder-Models"><span class="toc-number">2.4.4.</span> <span class="toc-text">Encoder–Decoder Models</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#A-Question-and-Answer-Generator"><span class="toc-number">2.5.</span> <span class="toc-text">A Question and Answer Generator</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#A-Question-Answer-Dataset"><span class="toc-number">2.5.1.</span> <span class="toc-text">A Question-Answer Dataset</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Model-Architecture"><span class="toc-number">2.5.2.</span> <span class="toc-text">Model Architecture</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#GLOVE-WORD-VECTORS"><span class="toc-number">2.5.2.1.</span> <span class="toc-text">GLOVE WORD VECTORS</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Inference"><span class="toc-number">2.5.3.</span> <span class="toc-text">Inference</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Model-Results"><span class="toc-number">2.5.4.</span> <span class="toc-text">Model Results</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Summary-1"><span class="toc-number">2.6.</span> <span class="toc-text">Summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Chapter-7-Compose"><span class="toc-number">3.</span> <span class="toc-text">Chapter 7. Compose</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Preliminaries"><span class="toc-number">3.1.</span> <span class="toc-text">Preliminaries</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Musical-Notation"><span class="toc-number">3.2.</span> <span class="toc-text">Musical Notation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Your-First-Music-Generating-RNN"><span class="toc-number">3.3.</span> <span class="toc-text">Your First Music-Generating RNN</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Attention"><span class="toc-number">3.3.1.</span> <span class="toc-text">Attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Building-an-Attention-Mechanism-in-Keras"><span class="toc-number">3.3.2.</span> <span class="toc-text">Building an Attention Mechanism in Keras</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Analysis-of-the-RNN-with-Attention"><span class="toc-number">3.3.3.</span> <span class="toc-text">Analysis of the RNN with Attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Attention-in-Encoder%E2%80%93Decoder-Networks"><span class="toc-number">3.3.4.</span> <span class="toc-text">Attention in Encoder–Decoder Networks</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Generating-Polyphonic-Music"><span class="toc-number">3.4.</span> <span class="toc-text">Generating Polyphonic Music</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#The-Musical-Organ"><span class="toc-number">3.5.</span> <span class="toc-text">The Musical Organ</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Chapter-8-Play"><span class="toc-number">4.</span> <span class="toc-text">Chapter 8. Play</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Reinforcement-Learning"><span class="toc-number">4.1.</span> <span class="toc-text">Reinforcement Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#OpenAI-Gym"><span class="toc-number">4.2.</span> <span class="toc-text">OpenAI Gym</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#World-Model-Architecture"><span class="toc-number">4.3.</span> <span class="toc-text">World Model Architecture</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#The-Variational-Autoencoder"><span class="toc-number">4.3.1.</span> <span class="toc-text">The Variational Autoencoder</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#The-MDN-RNN"><span class="toc-number">4.3.2.</span> <span class="toc-text">The MDN-RNN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#The-Controller"><span class="toc-number">4.3.3.</span> <span class="toc-text">The Controller</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Setup"><span class="toc-number">4.3.4.</span> <span class="toc-text">Setup</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-history"></i><span>最近发布</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/05/12/sheng-cheng-shi-ai-xiang-jie/" title="生成式AI详解">生成式AI详解</a><time datetime="2023-05-12T04:30:27.816Z" title="发表于 2023-05-12 12:30:27">2023-05-12</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"></div><div id="footer-bar"><div class="footer-bar-links"><div class="footer-bar-left"><div id="footer-bar-tips"><div class="copyright">&copy;2020 - 2025 By <a class="footer-bar-link" href="/" title="陈汉江" target="_blank">陈汉江</a></div></div><div id="footer-type-tips"></div></div><div class="footer-bar-right"><a class="footer-bar-link" target="_blank" rel="noopener" href="https://github.com/anzhiyu-c/hexo-theme-anzhiyu" title="主题">主题</a></div></div></div></footer></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="sidebar-site-data site-data is-center"><a href="/archives/" title="archive"><div class="headline">文章</div><div class="length-num">1</div></a><a href="/tags/" title="tag"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/" title="category"><div class="headline">分类</div><div class="length-num">1</div></a></div><span class="sidebar-menu-item-title">功能</span><div class="sidebar-menu-item"><a class="darkmode_switchbutton menu-child" href="javascript:void(0);" title="显示模式"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span>显示模式</span></a></div><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">项目</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://gitee.com/promoteAI/mkdown-images/tree/master/imgs" title="图床"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://image.anheyu.com/favicon.ico" alt="图床"/><span class="back-menu-item-text">图床</span></a></div></div><div class="back-menu-list-group"><div class="back-menu-list-title">项目</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://image.anheyu.com/" title="安知鱼图床"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://image.anheyu.com/favicon.ico" alt="安知鱼图床"/><span class="back-menu-item-text">安知鱼图床</span></a></div></div></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 文章</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><i class="anzhiyufont anzhiyu-icon-box-archive faa-tada" style="font-size: 0.9em;"></i><span> 隧道</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><i class="anzhiyufont anzhiyu-icon-shapes faa-tada" style="font-size: 0.9em;"></i><span> 分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags faa-tada" style="font-size: 0.9em;"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 友链</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/link/"><i class="anzhiyufont anzhiyu-icon-link faa-tada" style="font-size: 0.9em;"></i><span> 友人帐</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/fcircle/"><i class="anzhiyufont anzhiyu-icon-artstation faa-tada" style="font-size: 0.9em;"></i><span> 朋友圈</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/comments/"><i class="anzhiyufont anzhiyu-icon-envelope faa-tada" style="font-size: 0.9em;"></i><span> 留言板</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 我的</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/music/"><i class="anzhiyufont anzhiyu-icon-music faa-tada" style="font-size: 0.9em;"></i><span> 音乐馆</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/bangumis/"><i class="anzhiyufont anzhiyu-icon-bilibili faa-tada" style="font-size: 0.9em;"></i><span> 追番页</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/album/"><i class="anzhiyufont anzhiyu-icon-images faa-tada" style="font-size: 0.9em;"></i><span> 相册集</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/air-conditioner/"><i class="anzhiyufont anzhiyu-icon-fan faa-tada" style="font-size: 0.9em;"></i><span> 小空调</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 关于</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/about/"><i class="anzhiyufont anzhiyu-icon-paper-plane faa-tada" style="font-size: 0.9em;"></i><span> 关于本人</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/essay/"><i class="anzhiyufont anzhiyu-icon-lightbulb faa-tada" style="font-size: 0.9em;"></i><span> 闲言碎语</span></a></li><li><a class="site-page child faa-parent animated-hover" href="javascript:toRandomPost()"><i class="anzhiyufont anzhiyu-icon-shoe-prints1 faa-tada" style="font-size: 0.9em;"></i><span> 随便逛逛</span></a></li></ul></div></div><span class="sidebar-menu-item-title">标签</span></div></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="anzhiyufont anzhiyu-icon-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="anzhiyufont anzhiyu-icon-gear"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="anzhiyufont anzhiyu-icon-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></button></div></div><div id="nav-music"><a id="nav-music-hoverTips" onclick="anzhiyu.musicToggle()" accesskey="m">播放音乐</a><div id="console-music-bg"></div><meting-js id="8152976493" server="netease" type="playlist" mutex="true" preload="none" theme="var(--anzhiyu-main)" data-lrctype="0" order="random" volume="0.7"></meting-js></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="anzhiyufont anzhiyu-icon-xmark"></i></button></nav><div class="is-center" id="loading-database"><i class="anzhiyufont anzhiyu-icon-spinner anzhiyu-pulse-icon"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="anzhiyufont anzhiyu-icon-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="anzhiyufont anzhiyu-icon-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right" style="font-size: 1rem;"></i></div><div class="rightMenu-item" id="menu-top"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></div></div><div class="rightMenu-group rightMenu-line rightMenuPlugin"><div class="rightMenu-item" id="menu-copytext"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制选中文本</span></div><div class="rightMenu-item" id="menu-pastetext"><i class="anzhiyufont anzhiyu-icon-paste"></i><span>粘贴文本</span></div><a class="rightMenu-item" id="menu-commenttext"><i class="anzhiyufont anzhiyu-icon-comment-medical"></i><span>引用到评论</span></a><div class="rightMenu-item" id="menu-newwindow"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开</span></div><div class="rightMenu-item" id="menu-copylink"><i class="anzhiyufont anzhiyu-icon-link"></i><span>复制链接地址</span></div><div class="rightMenu-item" id="menu-copyimg"><i class="anzhiyufont anzhiyu-icon-images"></i><span>复制此图片</span></div><div class="rightMenu-item" id="menu-downloadimg"><i class="anzhiyufont anzhiyu-icon-download"></i><span>下载此图片</span></div><div class="rightMenu-item" id="menu-newwindowimg"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开图片</span></div><div class="rightMenu-item" id="menu-search"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>站内搜索</span></div><div class="rightMenu-item" id="menu-searchBaidu"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>百度搜索</span></div><div class="rightMenu-item" id="menu-music-toggle"><i class="anzhiyufont anzhiyu-icon-play"></i><span>播放音乐</span></div><div class="rightMenu-item" id="menu-music-back"><i class="anzhiyufont anzhiyu-icon-backward"></i><span>切换到上一首</span></div><div class="rightMenu-item" id="menu-music-forward"><i class="anzhiyufont anzhiyu-icon-forward"></i><span>切换到下一首</span></div><div class="rightMenu-item" id="menu-music-playlist" onclick="window.open(&quot;https://y.qq.com/n/ryqq/playlist/8802438608&quot;, &quot;_blank&quot;);" style="display: none;"><i class="anzhiyufont anzhiyu-icon-radio"></i><span>查看所有歌曲</span></div><div class="rightMenu-item" id="menu-music-copyMusicName"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制歌名</span></div></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" id="menu-randomPost"><i class="anzhiyufont anzhiyu-icon-shuffle"></i><span>随便逛逛</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="anzhiyufont anzhiyu-icon-cube"></i><span>博客分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item" id="menu-copy" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制地址</span></a><a class="rightMenu-item" id="menu-commentBarrage" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-message"></i><span class="menu-commentBarrage-text">关闭热评</span></a><a class="rightMenu-item" id="menu-darkmode" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span class="menu-darkmode-text">深色模式</span></a><a class="rightMenu-item" id="menu-translate" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-language"></i><span>轉為繁體</span></a></div></div><div id="rightmenu-mask"></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.umd.js"></script><script src="https://cdn.cbd.int/instant.page@5.2.0/instantpage.js" type="module"></script><script src="https://cdn.cbd.int/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script src="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.js"></script><canvas id="universe"></canvas><script async src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/dark/dark.js"></script><script>// 消除控制台打印
var HoldLog = console.log;
console.log = function () {};
let now1 = new Date();
queueMicrotask(() => {
  const Log = function () {
    HoldLog.apply(console, arguments);
  }; //在恢复前输出日志
  const grt = new Date("04/01/2021 00:00:00"); //此处修改你的建站时间或者网站上线时间
  now1.setTime(now1.getTime() + 250);
  const days = (now1 - grt) / 1000 / 60 / 60 / 24;
  const dnum = Math.floor(days);
  const ascll = [
    `欢迎使用安知鱼!`,
    `生活明朗, 万物可爱`,
    `
        
       █████╗ ███╗   ██╗███████╗██╗  ██╗██╗██╗   ██╗██╗   ██╗
      ██╔══██╗████╗  ██║╚══███╔╝██║  ██║██║╚██╗ ██╔╝██║   ██║
      ███████║██╔██╗ ██║  ███╔╝ ███████║██║ ╚████╔╝ ██║   ██║
      ██╔══██║██║╚██╗██║ ███╔╝  ██╔══██║██║  ╚██╔╝  ██║   ██║
      ██║  ██║██║ ╚████║███████╗██║  ██║██║   ██║   ╚██████╔╝
      ╚═╝  ╚═╝╚═╝  ╚═══╝╚══════╝╚═╝  ╚═╝╚═╝   ╚═╝    ╚═════╝
        
        `,
    "已上线",
    dnum,
    "天",
    "©2020 By 安知鱼 V1.6.14",
  ];
  const ascll2 = [`NCC2-036`, `调用前置摄像头拍照成功，识别为【小笨蛋】.`, `Photo captured: `, `🤪`];

  setTimeout(
    Log.bind(
      console,
      `\n%c${ascll[0]} %c ${ascll[1]} %c ${ascll[2]} %c${ascll[3]}%c ${ascll[4]}%c ${ascll[5]}\n\n%c ${ascll[6]}\n`,
      "color:#425AEF",
      "",
      "color:#425AEF",
      "color:#425AEF",
      "",
      "color:#425AEF",
      ""
    )
  );
  setTimeout(
    Log.bind(
      console,
      `%c ${ascll2[0]} %c ${ascll2[1]} %c \n${ascll2[2]} %c\n${ascll2[3]}\n`,
      "color:white; background-color:#4fd953",
      "",
      "",
      'background:url("https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/tinggge.gif") no-repeat;font-size:450%'
    )
  );

  setTimeout(Log.bind(console, "%c WELCOME %c 你好，小笨蛋.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(
      console,
      "%c ⚡ Powered by 安知鱼 %c 你正在访问 陈汉江 的博客.",
      "color:white; background-color:#f0ad4e",
      ""
    )
  );

  setTimeout(Log.bind(console, "%c W23-12 %c 你已打开控制台.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(console, "%c S013-782 %c 你现在正处于监控中.", "color:white; background-color:#d9534f", "")
  );
});</script><script async src="/anzhiyu/random.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.cbd.int/mathjax@3.2.2/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.cbd.int/katex@0.16.0/dist/katex.min.css"><script src="https://cdn.cbd.int/katex@0.16.0/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    anzhiyu.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><input type="hidden" name="page-type" id="page-type" value="post"></div><script>var visitorMail = "";
</script><script async data-pjax src="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/waterfall/waterfall.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/qrcodejs/1.0.0/qrcode.min.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.9/icon/ali_iconfont_css.css"><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/aplayer/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.cbd.int/anzhiyu-blog-static@1.0.1/js/APlayer.min.js"></script><script src="https://cdn.cbd.int/hexo-anzhiyu-music@1.0.1/assets/js/Meting2.min.js"></script><script src="https://cdn.cbd.int/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]
var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {
  // removeEventListener scroll 
  anzhiyu.removeGlobalFnEvent('pjax')
  anzhiyu.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script charset="UTF-8" src="https://cdn.cbd.int/anzhiyu-theme-static@1.1.5/accesskey/accesskey.js"></script></div><div id="popup-window"><div class="popup-window-title">通知</div><div class="popup-window-divider"></div><div class="popup-window-content"><div class="popup-tip">你好呀</div><div class="popup-link"><i class="anzhiyufont anzhiyu-icon-arrow-circle-right"></i></div></div></div></body></html>